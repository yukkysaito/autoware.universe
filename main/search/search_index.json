{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"autoware.universe # For Autoware's general documentation, see Autoware Documentation . For detailed documents of Autoware Universe components, see Autoware Universe Documentation .","title":"autoware.universe"},{"location":"#autowareuniverse","text":"For Autoware's general documentation, see Autoware Documentation . For detailed documents of Autoware Universe components, see Autoware Universe Documentation .","title":"autoware.universe"},{"location":"CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct # Our Pledge # We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards # Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities # Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope # This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement # Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at conduct@autoware.org. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines # Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction # Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning # Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban # Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban # Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution # This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at conduct@autoware.org. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"CODE_OF_CONDUCT/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"CODE_OF_CONDUCT/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Attribution"},{"location":"CONTRIBUTING/","text":"Contributing # See https://autowarefoundation.github.io/autoware-documentation/main/contributing/ .","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"See https://autowarefoundation.github.io/autoware-documentation/main/contributing/ .","title":"Contributing"},{"location":"DISCLAIMER/","text":"DISCLAIMER \u201cAutoware\u201d will be provided by The Autoware Foundation under the Apache License 2.0. This \u201cDISCLAIMER\u201d will be applied to all users of Autoware (a \u201cUser\u201d or \u201cUsers\u201d) with the Apache License 2.0 and Users shall hereby approve and acknowledge all the contents specified in this disclaimer below and will be deemed to consent to this disclaimer without any objection upon utilizing or downloading Autoware. Disclaimer and Waiver of Warranties AUTOWARE FOUNDATION MAKES NO REPRESENTATION OR WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, WITH RESPECT TO PROVIDING AUTOWARE (the \u201cService\u201d) including but not limited to any representation or warranty (i) of fitness or suitability for a particular purpose contemplated by the Users, (ii) of the expected functions, commercial value, accuracy, or usefulness of the Service, (iii) that the use by the Users of the Service complies with the laws and regulations applicable to the Users or any internal rules established by industrial organizations, (iv) that the Service will be free of interruption or defects, (v) of the non-infringement of any third party's right and (vi) the accuracy of the content of the Services and the software itself. The Autoware Foundation shall not be liable for any damage incurred by the User that are attributable to the Autoware Foundation for any reasons whatsoever. UNDER NO CIRCUMSTANCES SHALL THE AUTOWARE FOUNDATION BE LIABLE FOR INCIDENTAL, INDIRECT, SPECIAL OR FUTURE DAMAGES OR LOSS OF PROFITS. A User shall be entirely responsible for the content posted by the User and its use of any content of the Service or the Website. If the User is held responsible in a civil action such as a claim for damages or even in a criminal case, the Autoware Foundation and member companies, governments and academic & non-profit organizations and their directors, officers, employees and agents (collectively, the \u201cIndemnified Parties\u201d) shall be completely discharged from any rights or assertions the User may have against the Indemnified Parties, or from any legal action, litigation or similar procedures. Indemnity A User shall indemnify and hold the Indemnified Parties harmless from any of their damages, losses, liabilities, costs or expenses (including attorneys' fees or criminal compensation), or any claims or demands made against the Indemnified Parties by any third party, due to or arising out of, or in connection with utilizing Autoware (including the representations and warranties), the violation of applicable Product Liability Law of each country (including criminal case) or violation of any applicable laws by the Users, or the content posted by the User or its use of any content of the Service or the Website.","title":"DISCLAIMER"},{"location":"common/autoware_ad_api_msgs/","text":"autoware_ad_api_msgs # ResponseStatus # This message is a response status commonly used in the service type API. Each API can define its own status codes. The status codes are primarily used to indicate the error cause, such as invalid parameter and timeout. If the API succeeds, set success to true, code to zero, and message to the empty string. Alternatively, codes and messages can be used for warnings or additional information. If the API fails, set success to false, code to the related status code, and message to the information. The status code zero is reserved for success. The status code 50000 or over are also reserved for typical cases. Name Code Description SUCCESS 0 This API has completed successfully. DEPRECATED 50000 This API is deprecated. InterfaceVersion # Considering the product life cycle, there may be multiple vehicles using different versions of the AD API due to changes in requirements or functional improvements. For example, one vehicle uses v1 for stability and another vehicle uses v2 to enable more advanced functionality. In that situation, the AD API users such as developers of a web service have to switch the application behavior based on the version that each vehicle uses. The version of AD API follows Semantic Versioning in order to provide an intuitive understanding of the changes between versions.","title":"autoware_ad_api_msgs"},{"location":"common/autoware_ad_api_msgs/#autoware_ad_api_msgs","text":"","title":"autoware_ad_api_msgs"},{"location":"common/autoware_ad_api_msgs/#responsestatus","text":"This message is a response status commonly used in the service type API. Each API can define its own status codes. The status codes are primarily used to indicate the error cause, such as invalid parameter and timeout. If the API succeeds, set success to true, code to zero, and message to the empty string. Alternatively, codes and messages can be used for warnings or additional information. If the API fails, set success to false, code to the related status code, and message to the information. The status code zero is reserved for success. The status code 50000 or over are also reserved for typical cases. Name Code Description SUCCESS 0 This API has completed successfully. DEPRECATED 50000 This API is deprecated.","title":"ResponseStatus"},{"location":"common/autoware_ad_api_msgs/#interfaceversion","text":"Considering the product life cycle, there may be multiple vehicles using different versions of the AD API due to changes in requirements or functional improvements. For example, one vehicle uses v1 for stability and another vehicle uses v2 to enable more advanced functionality. In that situation, the AD API users such as developers of a web service have to switch the application behavior based on the version that each vehicle uses. The version of AD API follows Semantic Versioning in order to provide an intuitive understanding of the changes between versions.","title":"InterfaceVersion"},{"location":"common/autoware_auto_common/design/comparisons/","text":"Comparisons # The float_comparisons.hpp library is a simple set of functions for performing approximate numerical comparisons. There are separate functions for performing comparisons using absolute bounds and relative bounds. Absolute comparison checks are prefixed with abs_ and relative checks are prefixed with rel_ . The bool_comparisons.hpp library additionally contains an XOR operator. The intent of the library is to improve readability of code and reduce likelihood of typographical errors when using numerical and boolean comparisons. Target use cases # The approximate comparisons are intended to be used to check whether two numbers lie within some absolute or relative interval. The exclusive_or function will test whether two values cast to different boolean values. Assumptions # The approximate comparisons all take an epsilon parameter. The value of this parameter must be >= 0. The library is only intended to be used with floating point types. A static assertion will be thrown if the library is used with a non-floating point type. Example Usage # #include \"common/bool_comparisons.hpp\" #include \"common/float_comparisons.hpp\" #include <iostream> // using-directive is just for illustration; don't do this in practice using namespace autoware :: common :: helper_functions :: comparisons ; static constexpr auto epsilon = 0.2 ; static constexpr auto relative_epsilon = 0.01 ; std :: cout << exclusive_or ( true , false ) << \" \\n \" ; // Prints: true std :: cout << rel_eq ( 1.0 , 1.1 , relative_epsilon )) << \" \\n \" ; // Prints: false std :: cout << approx_eq ( 10000.0 , 10010.0 , epsilon , relative_epsilon )) << \" \\n \" ; // Prints: true std :: cout << abs_eq ( 4.0 , 4.2 , epsilon ) << \" \\n \" ; // Prints: true std :: cout << abs_ne ( 4.0 , 4.2 , epsilon ) << \" \\n \" ; // Prints: false std :: cout << abs_eq_zero ( 0.2 , epsilon ) << \" \\n \" ; // Prints: false std :: cout << abs_lt ( 4.0 , 4.25 , epsilon ) << \" \\n \" ; // Prints: true std :: cout << abs_lte ( 1.0 , 1.2 , epsilon ) << \" \\n \" ; // Prints: true std :: cout << abs_gt ( 1.25 , 1.0 , epsilon ) << \" \\n \" ; // Prints: true std :: cout << abs_gte ( 0.75 , 1.0 , epsilon ) << \" \\n \" ; // Prints: false","title":"Comparisons"},{"location":"common/autoware_auto_common/design/comparisons/#comparisons","text":"The float_comparisons.hpp library is a simple set of functions for performing approximate numerical comparisons. There are separate functions for performing comparisons using absolute bounds and relative bounds. Absolute comparison checks are prefixed with abs_ and relative checks are prefixed with rel_ . The bool_comparisons.hpp library additionally contains an XOR operator. The intent of the library is to improve readability of code and reduce likelihood of typographical errors when using numerical and boolean comparisons.","title":"Comparisons"},{"location":"common/autoware_auto_common/design/comparisons/#target-use-cases","text":"The approximate comparisons are intended to be used to check whether two numbers lie within some absolute or relative interval. The exclusive_or function will test whether two values cast to different boolean values.","title":"Target use cases"},{"location":"common/autoware_auto_common/design/comparisons/#assumptions","text":"The approximate comparisons all take an epsilon parameter. The value of this parameter must be >= 0. The library is only intended to be used with floating point types. A static assertion will be thrown if the library is used with a non-floating point type.","title":"Assumptions"},{"location":"common/autoware_auto_common/design/comparisons/#example-usage","text":"#include \"common/bool_comparisons.hpp\" #include \"common/float_comparisons.hpp\" #include <iostream> // using-directive is just for illustration; don't do this in practice using namespace autoware :: common :: helper_functions :: comparisons ; static constexpr auto epsilon = 0.2 ; static constexpr auto relative_epsilon = 0.01 ; std :: cout << exclusive_or ( true , false ) << \" \\n \" ; // Prints: true std :: cout << rel_eq ( 1.0 , 1.1 , relative_epsilon )) << \" \\n \" ; // Prints: false std :: cout << approx_eq ( 10000.0 , 10010.0 , epsilon , relative_epsilon )) << \" \\n \" ; // Prints: true std :: cout << abs_eq ( 4.0 , 4.2 , epsilon ) << \" \\n \" ; // Prints: true std :: cout << abs_ne ( 4.0 , 4.2 , epsilon ) << \" \\n \" ; // Prints: false std :: cout << abs_eq_zero ( 0.2 , epsilon ) << \" \\n \" ; // Prints: false std :: cout << abs_lt ( 4.0 , 4.25 , epsilon ) << \" \\n \" ; // Prints: true std :: cout << abs_lte ( 1.0 , 1.2 , epsilon ) << \" \\n \" ; // Prints: true std :: cout << abs_gt ( 1.25 , 1.0 , epsilon ) << \" \\n \" ; // Prints: true std :: cout << abs_gte ( 0.75 , 1.0 , epsilon ) << \" \\n \" ; // Prints: false","title":"Example Usage"},{"location":"common/autoware_auto_geometry/design/interval/","text":"Interval # The interval is a standard 1D real-valued interval. The class implements a representation and operations on the interval type and guarantees interval validity on construction. Basic operations and accessors are implemented, as well as other common operations. See 'Example Usage' below. Target use cases # Range or containment checks. The interval class simplifies code that involves checking membership of a value to a range, or intersecting two ranges. It also provides consistent behavior and consistent handling of edge cases. Properties # empty : An empty interval is equivalent to an empty set. It contains no elements. It is a valid interval, but because it is empty, the notion of measure (length) is undefined; the measure of an empty interval is not zero. The implementation represents the measure of an empty interval with NaN . zero measure : An interval with zero measure is an interval whose bounds are exactly equal. The measure is zero because the interval contains only a single point, and points have zero measure. However, because it does contain a single element, the interval is not empty. valid : A valid interval is either empty or has min/max bounds such that (min <= max). On construction, interval objects are guaranteed to be valid. An attempt to construct an invalid interval results in a runtime_error exception being thrown. pseudo-immutable : Once constructed the only way to change the value of an interval is to overwrite it with a new one; an existing object cannot be modified. Conventions # All operations on interval objects are defined as static class methods on the interval class. This is a functional-style of programming that basically turns the class into a namespace that grants functions access to private member variables of the object they operate on. Assumptions # The interval is only intended for floating point types. This is enforced via static assertion. The constructor for non-empty intervals takes two arguments 'min' and 'max', and they must be ordered (i.e., min <= max). If this assumption is violated, an exception is emitted and construction fails. Example Usage # #include \"geometry/interval.hpp\" #include <iostream> // using-directive is just for illustration; don't do this in practice using namespace autoware :: common :: geometry ; // bounds for example interval constexpr auto MIN = 0.0 ; constexpr auto MAX = 1.0 ; // // Try to construct an invalid interval. This will give the following error: // 'Attempted to construct an invalid interval: {\"min\": 1.0, \"max\": 0.0}' // try { const auto i = Interval_d ( MAX , MIN ); } catch ( const std :: runtime_error & e ) { std :: cerr << e . what (); } // // Construct a double precision interval from 0 to 1 // const auto i = Interval_d ( MIN , MAX ); // // Test accessors and properties // std :: cout << Interval_d :: min ( i ) << \" \" << Interval_d :: max ( i ) << \" \\n \" ; // Prints: 0.0 1.0 std :: cout << Interval_d :: empty ( i ) << \" \" << Interval_d :: length ( i ) << \" \\n \" ; // Prints: false 1.0 std :: cout << Interval_d :: contains ( i , 0.3 ) << \" \\n \" ; // Prints: true std :: cout << Interval_d :: is_subset_eq ( Interval_d ( 0.2 , 0.4 ), i ) << \" \\n \" ; // Prints: true // // Test operations. // std :: cout << Interval_d :: intersect ( i , Interval ( -1.0 , 0.3 )) << \" \\n \" ; // Prints: {\"min\": 0.0, \"max\": 0.3} std :: cout << Interval_d :: project_to_interval ( i , 0.5 ) << \" \" << Interval_d :: project_to_interval ( i , -1.3 ) << \" \\n \" ; // Prints: 0.5 0.0 // // Distinguish empty/zero measure // const auto i_empty = Interval (); const auto i_zero_length = Interval ( 0.0 , 0.0 ); std :: cout << Interval_d :: empty ( i_empty ) << \" \" << Interval_d :: empty ( i_zero_length ) << \" \\n \" ; // Prints: true false std :: cout << Interval_d :: zero_measure ( i_empty ) << \" \" << Interval_d :: zero_measure ( i_zero_length ) << \" \\n \" ; // Prints: false false","title":"Interval"},{"location":"common/autoware_auto_geometry/design/interval/#interval","text":"The interval is a standard 1D real-valued interval. The class implements a representation and operations on the interval type and guarantees interval validity on construction. Basic operations and accessors are implemented, as well as other common operations. See 'Example Usage' below.","title":"Interval"},{"location":"common/autoware_auto_geometry/design/interval/#target-use-cases","text":"Range or containment checks. The interval class simplifies code that involves checking membership of a value to a range, or intersecting two ranges. It also provides consistent behavior and consistent handling of edge cases.","title":"Target use cases"},{"location":"common/autoware_auto_geometry/design/interval/#properties","text":"empty : An empty interval is equivalent to an empty set. It contains no elements. It is a valid interval, but because it is empty, the notion of measure (length) is undefined; the measure of an empty interval is not zero. The implementation represents the measure of an empty interval with NaN . zero measure : An interval with zero measure is an interval whose bounds are exactly equal. The measure is zero because the interval contains only a single point, and points have zero measure. However, because it does contain a single element, the interval is not empty. valid : A valid interval is either empty or has min/max bounds such that (min <= max). On construction, interval objects are guaranteed to be valid. An attempt to construct an invalid interval results in a runtime_error exception being thrown. pseudo-immutable : Once constructed the only way to change the value of an interval is to overwrite it with a new one; an existing object cannot be modified.","title":"Properties"},{"location":"common/autoware_auto_geometry/design/interval/#conventions","text":"All operations on interval objects are defined as static class methods on the interval class. This is a functional-style of programming that basically turns the class into a namespace that grants functions access to private member variables of the object they operate on.","title":"Conventions"},{"location":"common/autoware_auto_geometry/design/interval/#assumptions","text":"The interval is only intended for floating point types. This is enforced via static assertion. The constructor for non-empty intervals takes two arguments 'min' and 'max', and they must be ordered (i.e., min <= max). If this assumption is violated, an exception is emitted and construction fails.","title":"Assumptions"},{"location":"common/autoware_auto_geometry/design/interval/#example-usage","text":"#include \"geometry/interval.hpp\" #include <iostream> // using-directive is just for illustration; don't do this in practice using namespace autoware :: common :: geometry ; // bounds for example interval constexpr auto MIN = 0.0 ; constexpr auto MAX = 1.0 ; // // Try to construct an invalid interval. This will give the following error: // 'Attempted to construct an invalid interval: {\"min\": 1.0, \"max\": 0.0}' // try { const auto i = Interval_d ( MAX , MIN ); } catch ( const std :: runtime_error & e ) { std :: cerr << e . what (); } // // Construct a double precision interval from 0 to 1 // const auto i = Interval_d ( MIN , MAX ); // // Test accessors and properties // std :: cout << Interval_d :: min ( i ) << \" \" << Interval_d :: max ( i ) << \" \\n \" ; // Prints: 0.0 1.0 std :: cout << Interval_d :: empty ( i ) << \" \" << Interval_d :: length ( i ) << \" \\n \" ; // Prints: false 1.0 std :: cout << Interval_d :: contains ( i , 0.3 ) << \" \\n \" ; // Prints: true std :: cout << Interval_d :: is_subset_eq ( Interval_d ( 0.2 , 0.4 ), i ) << \" \\n \" ; // Prints: true // // Test operations. // std :: cout << Interval_d :: intersect ( i , Interval ( -1.0 , 0.3 )) << \" \\n \" ; // Prints: {\"min\": 0.0, \"max\": 0.3} std :: cout << Interval_d :: project_to_interval ( i , 0.5 ) << \" \" << Interval_d :: project_to_interval ( i , -1.3 ) << \" \\n \" ; // Prints: 0.5 0.0 // // Distinguish empty/zero measure // const auto i_empty = Interval (); const auto i_zero_length = Interval ( 0.0 , 0.0 ); std :: cout << Interval_d :: empty ( i_empty ) << \" \" << Interval_d :: empty ( i_zero_length ) << \" \\n \" ; // Prints: true false std :: cout << Interval_d :: zero_measure ( i_empty ) << \" \" << Interval_d :: zero_measure ( i_zero_length ) << \" \\n \" ; // Prints: false false","title":"Example Usage"},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/","text":"2D Convex Polygon Intersection # Two convex polygon's intersection can be visualized on the image below as the blue area: Purpose / Use cases # Computing the intersection between two polygons can be useful in many applications of scene understanding. It can be used to estimate collision detection, shape alignment, shape association and in any application that deals with the objects around the perceiving agent. Design # Livermore, Calif, 1977 Livermore, Calif, 1977 mention the following observations about convex polygon intersection: Intersection of two convex polygons is a convex polygon A vertex from a polygon that is contained in the other polygon is a vertex of the intersection shape. (Vertices A, C, D in the shape above) An edge from a polygon that is contained in the other polygon is an edge in the intersection shape. (edge C-D in the shape above) Edge intersections between two polygons are vertices in the intersection shape. (Vertices B, E in the shape above.) Inner-workings / Algorithms # With the observation mentioned above, the current algorithm operates in the following way: Compute and find the vertices from each polygon that is contained in the other polygon (Vertices A, C, D) Compute and find the intersection points between each polygon (Vertices B, E) Compute the convex hull shaped by these vertices by ordering them CCW. Inputs / Outputs / API # Inputs: Two iterables that contain vertices of the convex polygons ordered in the CCW direction. Outputs: A list of vertices of the intersection shape ordered in the CCW direction. Future Work # 1230: Applying efficient algorithms. # Related issues # 983: Integrate vision detections in object tracker #","title":"2D Convex Polygon Intersection"},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/#2d-convex-polygon-intersection","text":"Two convex polygon's intersection can be visualized on the image below as the blue area:","title":"2D Convex Polygon Intersection"},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/#purpose-use-cases","text":"Computing the intersection between two polygons can be useful in many applications of scene understanding. It can be used to estimate collision detection, shape alignment, shape association and in any application that deals with the objects around the perceiving agent.","title":"Purpose / Use cases"},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/#design","text":"Livermore, Calif, 1977 Livermore, Calif, 1977 mention the following observations about convex polygon intersection: Intersection of two convex polygons is a convex polygon A vertex from a polygon that is contained in the other polygon is a vertex of the intersection shape. (Vertices A, C, D in the shape above) An edge from a polygon that is contained in the other polygon is an edge in the intersection shape. (edge C-D in the shape above) Edge intersections between two polygons are vertices in the intersection shape. (Vertices B, E in the shape above.)","title":"Design"},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/#inner-workings-algorithms","text":"With the observation mentioned above, the current algorithm operates in the following way: Compute and find the vertices from each polygon that is contained in the other polygon (Vertices A, C, D) Compute and find the intersection points between each polygon (Vertices B, E) Compute the convex hull shaped by these vertices by ordering them CCW.","title":"Inner-workings / Algorithms"},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/#inputs-outputs-api","text":"Inputs: Two iterables that contain vertices of the convex polygons ordered in the CCW direction. Outputs: A list of vertices of the intersection shape ordered in the CCW direction.","title":"Inputs / Outputs / API"},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/#future-work","text":"","title":"Future Work"},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/#1230-applying-efficient-algorithms","text":"","title":"1230: Applying efficient algorithms."},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/#related-issues","text":"","title":"Related issues"},{"location":"common/autoware_auto_geometry/design/polygon_intersection_2d-design/#983-integrate-vision-detections-in-object-tracker","text":"","title":"983: Integrate vision detections in object tracker"},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/","text":"Spatial Hash # The spatial hash is a data structure designed for efficient fixed-radius near-neighbor queries in low dimensions. The fixed-radius near-neighbors problem is defined as follows: For point p, find all points p' s.t. d(p, p') < r Where in this case d(p, p') is euclidean distance, and r is the fixed radius. For n points with an average of k neighbors each, this data structure can perform m near-neighbor queries (to generate lists of near-neighbors for m different points) in O(mk) time. By contrast, using a k-d tree for successive nearest-neighbor queries results in a running time of O(m log n) . The spatial hash works as follows: Each point is assigned to a bin in the predefined bounding area defined by x_min/x_max and y_min/y_max This can be done by converting x and y position into x and y index respectively For example with the bin containing x_min and y_min as index (0, 0) The two (or more) indices can then be converted into a single index Once every point of interest has been inserted into the hash, near-neighbor queries can begin: The bin of the reference point is first computed For each point in each adjacent bin, perform an explicit distance computation between said point and the reference point. If the distance is below the given radius, said point is considered to be a near-neighbor Under the hood, an std::unordered_multimap is used, where the key is a bin/voxel index. The bin size was computed to be the same as the lookup distance. In addition, this data structure can support 2D or 3D queries. This is determined during configuration, and baked into the data structure via the configuration class. The purpose of this was to avoid if statements in tight loops. The configuration class specializations themselves use CRTP (Curiously Recurring Template Patterns) to do \"static polymorphism\", and avoid a dispatching call. Performance characterization # Time # Insertion is O(n) because lookup time for the underlying hashmap is O(n) for hashmaps. In practice, lookup time for hashmaps and thus insertion time should be O(1) . Removing a point is O(1) because the current API only supports removal via direct reference to a node. Finding k near-neighbors is worst case O(n) in the case of an adversarial example, but in practice O(k) . Space # The module consists of the following components: The internal hashmap is O(n + n + A * n) , where A is an arbitrary constant (load factor) The other components of the spatial hash are O(n + n) This results in O(n) space complexity. States # The spatial hash's state is dictated by the status of the underlying unordered_multimap. The data structure is wholly configured by a config class. The constructor of the class determines in the data structure accepts strictly 2D or strictly 3D queries. Inputs # The primary method of introducing data into the data structure is via the insert method. Outputs # The primary method of retrieving data from the data structure is via the near 2D configuration 2D configuration or near 3D configuration 3D configuration method. The whole data structure can also be traversed using standard constant iterators. Future Work # Performance tuning and optimization Related issues # 28: Port to autoware.Auto #","title":"Spatial Hash"},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#spatial-hash","text":"The spatial hash is a data structure designed for efficient fixed-radius near-neighbor queries in low dimensions. The fixed-radius near-neighbors problem is defined as follows: For point p, find all points p' s.t. d(p, p') < r Where in this case d(p, p') is euclidean distance, and r is the fixed radius. For n points with an average of k neighbors each, this data structure can perform m near-neighbor queries (to generate lists of near-neighbors for m different points) in O(mk) time. By contrast, using a k-d tree for successive nearest-neighbor queries results in a running time of O(m log n) . The spatial hash works as follows: Each point is assigned to a bin in the predefined bounding area defined by x_min/x_max and y_min/y_max This can be done by converting x and y position into x and y index respectively For example with the bin containing x_min and y_min as index (0, 0) The two (or more) indices can then be converted into a single index Once every point of interest has been inserted into the hash, near-neighbor queries can begin: The bin of the reference point is first computed For each point in each adjacent bin, perform an explicit distance computation between said point and the reference point. If the distance is below the given radius, said point is considered to be a near-neighbor Under the hood, an std::unordered_multimap is used, where the key is a bin/voxel index. The bin size was computed to be the same as the lookup distance. In addition, this data structure can support 2D or 3D queries. This is determined during configuration, and baked into the data structure via the configuration class. The purpose of this was to avoid if statements in tight loops. The configuration class specializations themselves use CRTP (Curiously Recurring Template Patterns) to do \"static polymorphism\", and avoid a dispatching call.","title":"Spatial Hash"},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#performance-characterization","text":"","title":"Performance characterization"},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#time","text":"Insertion is O(n) because lookup time for the underlying hashmap is O(n) for hashmaps. In practice, lookup time for hashmaps and thus insertion time should be O(1) . Removing a point is O(1) because the current API only supports removal via direct reference to a node. Finding k near-neighbors is worst case O(n) in the case of an adversarial example, but in practice O(k) .","title":"Time"},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#space","text":"The module consists of the following components: The internal hashmap is O(n + n + A * n) , where A is an arbitrary constant (load factor) The other components of the spatial hash are O(n + n) This results in O(n) space complexity.","title":"Space"},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#states","text":"The spatial hash's state is dictated by the status of the underlying unordered_multimap. The data structure is wholly configured by a config class. The constructor of the class determines in the data structure accepts strictly 2D or strictly 3D queries.","title":"States"},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#inputs","text":"The primary method of introducing data into the data structure is via the insert method.","title":"Inputs"},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#outputs","text":"The primary method of retrieving data from the data structure is via the near 2D configuration 2D configuration or near 3D configuration 3D configuration method. The whole data structure can also be traversed using standard constant iterators.","title":"Outputs"},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#future-work","text":"Performance tuning and optimization","title":"Future Work"},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#related-issues","text":"","title":"Related issues"},{"location":"common/autoware_auto_geometry/design/spatial-hash-design/#28-port-to-autowareauto","text":"","title":"28: Port to autoware.Auto"},{"location":"common/autoware_auto_perception_rviz_plugin/","text":"autoware_auto_perception_plugin # Purpose # It is an rviz plugin for visualizing the result from perception module. This package is based on the implementation of the rviz plugin developed by Autoware.Auto. See Autoware.Auto design documentation for the original design philosophy. [1] Input Types / Visualization Results # DetectedObjects # Input Types # Name Type Description autoware_auto_perception_msgs::msg::DetectedObjects detection result array Visualization Result # TrackedObjects # Input Types # Name Type Description autoware_auto_perception_msgs::msg::TrackedObjects tracking result array Visualization Result # Overwrite tracking results with detection results. PredictedObjects # Input Types # Name Type Description autoware_auto_perception_msgs::msg::PredictedObjects prediction result array Visualization Result # Overwrite prediction results with tracking results. References/External links # [1] https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/tree/master/src/tools/visualization/autoware_rviz_plugins Future extensions / Unimplemented parts #","title":"autoware_auto_perception_plugin"},{"location":"common/autoware_auto_perception_rviz_plugin/#autoware_auto_perception_plugin","text":"","title":"autoware_auto_perception_plugin"},{"location":"common/autoware_auto_perception_rviz_plugin/#purpose","text":"It is an rviz plugin for visualizing the result from perception module. This package is based on the implementation of the rviz plugin developed by Autoware.Auto. See Autoware.Auto design documentation for the original design philosophy. [1]","title":"Purpose"},{"location":"common/autoware_auto_perception_rviz_plugin/#input-types-visualization-results","text":"","title":"Input Types / Visualization Results"},{"location":"common/autoware_auto_perception_rviz_plugin/#detectedobjects","text":"","title":"DetectedObjects"},{"location":"common/autoware_auto_perception_rviz_plugin/#input-types","text":"Name Type Description autoware_auto_perception_msgs::msg::DetectedObjects detection result array","title":"Input Types"},{"location":"common/autoware_auto_perception_rviz_plugin/#visualization-result","text":"","title":"Visualization Result"},{"location":"common/autoware_auto_perception_rviz_plugin/#trackedobjects","text":"","title":"TrackedObjects"},{"location":"common/autoware_auto_perception_rviz_plugin/#input-types_1","text":"Name Type Description autoware_auto_perception_msgs::msg::TrackedObjects tracking result array","title":"Input Types"},{"location":"common/autoware_auto_perception_rviz_plugin/#visualization-result_1","text":"Overwrite tracking results with detection results.","title":"Visualization Result"},{"location":"common/autoware_auto_perception_rviz_plugin/#predictedobjects","text":"","title":"PredictedObjects"},{"location":"common/autoware_auto_perception_rviz_plugin/#input-types_2","text":"Name Type Description autoware_auto_perception_msgs::msg::PredictedObjects prediction result array","title":"Input Types"},{"location":"common/autoware_auto_perception_rviz_plugin/#visualization-result_2","text":"Overwrite prediction results with tracking results.","title":"Visualization Result"},{"location":"common/autoware_auto_perception_rviz_plugin/#referencesexternal-links","text":"[1] https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/tree/master/src/tools/visualization/autoware_rviz_plugins","title":"References/External links"},{"location":"common/autoware_auto_perception_rviz_plugin/#future-extensions-unimplemented-parts","text":"","title":"Future extensions / Unimplemented parts"},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/","text":"autoware_auto_tf2 # This is the design document for the autoware_auto_tf2 package. Purpose / Use cases # In general, users of ROS rely on tf (and its successor, tf2) for publishing and utilizing coordinate frame transforms. This is true even to the extent that the tf2 contains the packages tf2_geometry_msgs and tf2_sensor_msgs which allow for easy conversion to and from the message types defined in geometry_msgs and sensor_msgs , respectively. However, AutowareAuto contains some specialized message types which are not transformable between frames using the ROS2 library. The autoware_auto_tf2 package aims to provide developers with tools to transform applicable autoware_auto_msgs types. In addition to this, this package also provides transform tools for messages types in geometry_msgs missing in tf2_geometry_msgs . Design # While writing tf2_some_msgs or contributing to tf2_geometry_msgs , compatibility and design intent was ensured with the following files in the existing tf2 framework: tf2/convert.h tf2_ros/buffer_interface.h For example: void tf2 :: convert ( const A & a , B & b ) The method tf2::convert is dependent on the following: template < typename A , typename B > B tf2 :: toMsg ( const A & a ); template < typename A , typename B > void tf2 :: fromMsg ( const A & , B & b ); // New way to transform instead of using tf2::doTransform() directly tf2_ros :: BufferInterface :: transform (...) Which, in turn, is dependent on the following: void tf2::convert ( const A & a , B & b ) const std :: string & tf2 :: getFrameId ( const T & t ) const ros :: Time & tf2 :: getTimestamp ( const T & t ); Current Implementation of tf2_geometry_msgs # In both ROS1 and ROS2 stamped msgs like Vector3Stamped , QuaternionStamped have associated functions like: getTimestamp getFrameId doTransform toMsg fromMsg In ROS1, to support tf2::convert and need in doTransform of the stamped data, non-stamped underlying data like Vector3 , Point , have implementations of the following functions: toMsg fromMsg In ROS2, much of the doTransform method is not using toMsg and fromMsg as data types from tf2 are not used. Instead doTransform is done using KDL , thus functions relating to underlying data were not added; such as Vector3 , Point , or ported in this commit ros/geometry2/commit/6f2a82. The non-stamped data with toMsg and fromMsg are Quaternion , Transform . Pose has the modified toMsg and not used by PoseStamped . Plan for autoware_auto_perception_msgs::msg::BoundingBoxArray # The initial rough plan was to implement some of the common tf2 functions like toMsg , fromMsg , and doTransform , as needed for all the underlying data types in BoundingBoxArray . Examples of the data types include: BoundingBox , Quaternion32 , and Point32 . In addition, the implementation should be done such that upstream contributions could also be made to geometry_msgs . Assumptions / Known limits # Due to conflicts in a function signatures, the predefined template of convert.h / transform_functions.h is not followed and compatibility with tf2::convert(..) is broken and toMsg is written differently. // Old style geometry_msgs :: Vector3 toMsg ( const tf2 :: Vector3 & in ) geometry_msgs :: Point & toMsg ( const tf2 :: Vector3 & in ) // New style geometry_msgs :: Point & toMsg ( const tf2 :: Vector3 & in , geometry_msgs :: Point & out ) Inputs / Outputs / API # The library provides API doTransform for the following data-types that are either not available in tf2_geometry_msgs or the messages types are part of autoware_auto_msgs and are therefore custom and not inherently supported by any of the tf2 libraries. The following APIs are provided for the following data types: Point32 inline void doTransform ( const geometry_msgs :: msg :: Point32 & t_in , geometry_msgs :: msg :: Point32 & t_out , const geometry_msgs :: msg :: TransformStamped & transform ) Quaternion32 ( autoware_auto_msgs ) inline void doTransform ( const autoware_auto_geometry_msgs :: msg :: Quaternion32 & t_in , autoware_auto_geometry_msgs :: msg :: Quaternion32 & t_out , const geometry_msgs :: msg :: TransformStamped & transform ) BoundingBox ( autoware_auto_msgs ) inline void doTransform ( const BoundingBox & t_in , BoundingBox & t_out , const geometry_msgs :: msg :: TransformStamped & transform ) BoundingBoxArray inline void doTransform ( const BoundingBoxArray & t_in , BoundingBoxArray & t_out , const geometry_msgs :: msg :: TransformStamped & transform ) In addition, the following helper methods are also added: BoundingBoxArray inline tf2 :: TimePoint getTimestamp ( const BoundingBoxArray & t ) inline std :: string getFrameId ( const BoundingBoxArray & t ) Future extensions / Unimplemented parts # Challenges # tf2_geometry_msgs does not implement doTransform for any non-stamped data types, but it is possible with the same function template. It is needed when transforming sub-data, with main data that does have a stamp and can call doTransform on the sub-data with the same transform. Is this a useful upstream contribution? tf2_geometry_msgs does not have Point , Point32 , does not seem it needs one, also the implementation of non-standard toMsg would not help the convert. BoundingBox uses 32-bit float like Quaternion32 and Point32 to save space, as they are used repeatedly in BoundingBoxArray . While transforming is it better to convert to 64-bit Quaternion , Point , or PoseStamped , to re-use existing implementation of doTransform , or does it need to be implemented? It may not be simple to template.","title":"autoware_auto_tf2"},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/#autoware_auto_tf2","text":"This is the design document for the autoware_auto_tf2 package.","title":"autoware_auto_tf2"},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/#purpose-use-cases","text":"In general, users of ROS rely on tf (and its successor, tf2) for publishing and utilizing coordinate frame transforms. This is true even to the extent that the tf2 contains the packages tf2_geometry_msgs and tf2_sensor_msgs which allow for easy conversion to and from the message types defined in geometry_msgs and sensor_msgs , respectively. However, AutowareAuto contains some specialized message types which are not transformable between frames using the ROS2 library. The autoware_auto_tf2 package aims to provide developers with tools to transform applicable autoware_auto_msgs types. In addition to this, this package also provides transform tools for messages types in geometry_msgs missing in tf2_geometry_msgs .","title":"Purpose / Use cases"},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/#design","text":"While writing tf2_some_msgs or contributing to tf2_geometry_msgs , compatibility and design intent was ensured with the following files in the existing tf2 framework: tf2/convert.h tf2_ros/buffer_interface.h For example: void tf2 :: convert ( const A & a , B & b ) The method tf2::convert is dependent on the following: template < typename A , typename B > B tf2 :: toMsg ( const A & a ); template < typename A , typename B > void tf2 :: fromMsg ( const A & , B & b ); // New way to transform instead of using tf2::doTransform() directly tf2_ros :: BufferInterface :: transform (...) Which, in turn, is dependent on the following: void tf2::convert ( const A & a , B & b ) const std :: string & tf2 :: getFrameId ( const T & t ) const ros :: Time & tf2 :: getTimestamp ( const T & t );","title":"Design"},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/#current-implementation-of-tf2_geometry_msgs","text":"In both ROS1 and ROS2 stamped msgs like Vector3Stamped , QuaternionStamped have associated functions like: getTimestamp getFrameId doTransform toMsg fromMsg In ROS1, to support tf2::convert and need in doTransform of the stamped data, non-stamped underlying data like Vector3 , Point , have implementations of the following functions: toMsg fromMsg In ROS2, much of the doTransform method is not using toMsg and fromMsg as data types from tf2 are not used. Instead doTransform is done using KDL , thus functions relating to underlying data were not added; such as Vector3 , Point , or ported in this commit ros/geometry2/commit/6f2a82. The non-stamped data with toMsg and fromMsg are Quaternion , Transform . Pose has the modified toMsg and not used by PoseStamped .","title":"Current Implementation of tf2_geometry_msgs"},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/#plan-for-autoware_auto_perception_msgsmsgboundingboxarray","text":"The initial rough plan was to implement some of the common tf2 functions like toMsg , fromMsg , and doTransform , as needed for all the underlying data types in BoundingBoxArray . Examples of the data types include: BoundingBox , Quaternion32 , and Point32 . In addition, the implementation should be done such that upstream contributions could also be made to geometry_msgs .","title":"Plan for autoware_auto_perception_msgs::msg::BoundingBoxArray"},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/#assumptions-known-limits","text":"Due to conflicts in a function signatures, the predefined template of convert.h / transform_functions.h is not followed and compatibility with tf2::convert(..) is broken and toMsg is written differently. // Old style geometry_msgs :: Vector3 toMsg ( const tf2 :: Vector3 & in ) geometry_msgs :: Point & toMsg ( const tf2 :: Vector3 & in ) // New style geometry_msgs :: Point & toMsg ( const tf2 :: Vector3 & in , geometry_msgs :: Point & out )","title":"Assumptions / Known limits"},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/#inputs-outputs-api","text":"The library provides API doTransform for the following data-types that are either not available in tf2_geometry_msgs or the messages types are part of autoware_auto_msgs and are therefore custom and not inherently supported by any of the tf2 libraries. The following APIs are provided for the following data types: Point32 inline void doTransform ( const geometry_msgs :: msg :: Point32 & t_in , geometry_msgs :: msg :: Point32 & t_out , const geometry_msgs :: msg :: TransformStamped & transform ) Quaternion32 ( autoware_auto_msgs ) inline void doTransform ( const autoware_auto_geometry_msgs :: msg :: Quaternion32 & t_in , autoware_auto_geometry_msgs :: msg :: Quaternion32 & t_out , const geometry_msgs :: msg :: TransformStamped & transform ) BoundingBox ( autoware_auto_msgs ) inline void doTransform ( const BoundingBox & t_in , BoundingBox & t_out , const geometry_msgs :: msg :: TransformStamped & transform ) BoundingBoxArray inline void doTransform ( const BoundingBoxArray & t_in , BoundingBoxArray & t_out , const geometry_msgs :: msg :: TransformStamped & transform ) In addition, the following helper methods are also added: BoundingBoxArray inline tf2 :: TimePoint getTimestamp ( const BoundingBoxArray & t ) inline std :: string getFrameId ( const BoundingBoxArray & t )","title":"Inputs / Outputs / API"},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/#future-extensions-unimplemented-parts","text":"","title":"Future extensions / Unimplemented parts"},{"location":"common/autoware_auto_tf2/design/autoware-auto-tf2-design/#challenges","text":"tf2_geometry_msgs does not implement doTransform for any non-stamped data types, but it is possible with the same function template. It is needed when transforming sub-data, with main data that does have a stamp and can call doTransform on the sub-data with the same transform. Is this a useful upstream contribution? tf2_geometry_msgs does not have Point , Point32 , does not seem it needs one, also the implementation of non-standard toMsg would not help the convert. BoundingBox uses 32-bit float like Quaternion32 and Point32 to save space, as they are used repeatedly in BoundingBoxArray . While transforming is it better to convert to 64-bit Quaternion , Point , or PoseStamped , to re-use existing implementation of doTransform , or does it need to be implemented? It may not be simple to template.","title":"Challenges"},{"location":"common/autoware_testing/design/autoware_testing-design/","text":"autoware_testing # This is the design document for the autoware_testing package. Purpose / Use cases # The package aims to provide a unified way to add standard testing functionality to the package, currently supporting: Smoke testing ( add_smoke_test ): launch a node with default configuration and ensure that it starts up and does not crash. Design # Uses ros_testing (which is an extension of launch_testing ) and provides some parametrized, reusable standard tests to run. Assumptions / Known limits # Parametrization is limited to package, executable names, parameters filename and executable arguments. Test namespace is set as 'test'. Parameters file for the package is expected to be in param directory inside package. Inputs / Outputs / API # To add a smoke test to your package tests, add test dependency on autoware_testing to package.xml <test_depend> autoware_testing </test_depend> and add the following two lines to CMakeLists.txt in the IF (BUILD_TESTING) section: find_package ( autoware_testing REQUIRED ) add_smoke_test ( <package_name> <executable_name> [PARAM_FILENAME <param_filename>] [EXECUTABLE_ARGUMENTS <arguments>] ) Where <package_name> - [required] tested node package name. <executable_name> - [required] tested node executable name. <param_filename> - [optional] param filename. Default value is test.param.yaml . Required mostly in situation where there are multiple smoke tests in a package and each requires different parameters set <arguments> - [optional] arguments passed to executable. By default no arguments are passed. which adds <executable_name>_smoke_test test to suite. Example test result: build/<package_name>/test_results/<package_name>/<executable_name>_smoke_test.xunit.xml: 1 test, 0 errors, 0 failures, 0 skipped References / External links # https://en.wikipedia.org/wiki/Smoke_testing_(software) https://github.com/ros2/ros_testing https://github.com/ros2/launch/blob/master/launch_testing Future extensions / Unimplemented parts # Adding more types of standard tests. Related issues # Issue #700: add smoke test Issue #1224: Port other packages with smoke tests to use autoware_testing","title":"autoware_testing"},{"location":"common/autoware_testing/design/autoware_testing-design/#autoware_testing","text":"This is the design document for the autoware_testing package.","title":"autoware_testing"},{"location":"common/autoware_testing/design/autoware_testing-design/#purpose-use-cases","text":"The package aims to provide a unified way to add standard testing functionality to the package, currently supporting: Smoke testing ( add_smoke_test ): launch a node with default configuration and ensure that it starts up and does not crash.","title":"Purpose / Use cases"},{"location":"common/autoware_testing/design/autoware_testing-design/#design","text":"Uses ros_testing (which is an extension of launch_testing ) and provides some parametrized, reusable standard tests to run.","title":"Design"},{"location":"common/autoware_testing/design/autoware_testing-design/#assumptions-known-limits","text":"Parametrization is limited to package, executable names, parameters filename and executable arguments. Test namespace is set as 'test'. Parameters file for the package is expected to be in param directory inside package.","title":"Assumptions / Known limits"},{"location":"common/autoware_testing/design/autoware_testing-design/#inputs-outputs-api","text":"To add a smoke test to your package tests, add test dependency on autoware_testing to package.xml <test_depend> autoware_testing </test_depend> and add the following two lines to CMakeLists.txt in the IF (BUILD_TESTING) section: find_package ( autoware_testing REQUIRED ) add_smoke_test ( <package_name> <executable_name> [PARAM_FILENAME <param_filename>] [EXECUTABLE_ARGUMENTS <arguments>] ) Where <package_name> - [required] tested node package name. <executable_name> - [required] tested node executable name. <param_filename> - [optional] param filename. Default value is test.param.yaml . Required mostly in situation where there are multiple smoke tests in a package and each requires different parameters set <arguments> - [optional] arguments passed to executable. By default no arguments are passed. which adds <executable_name>_smoke_test test to suite. Example test result: build/<package_name>/test_results/<package_name>/<executable_name>_smoke_test.xunit.xml: 1 test, 0 errors, 0 failures, 0 skipped","title":"Inputs / Outputs / API"},{"location":"common/autoware_testing/design/autoware_testing-design/#references-external-links","text":"https://en.wikipedia.org/wiki/Smoke_testing_(software) https://github.com/ros2/ros_testing https://github.com/ros2/launch/blob/master/launch_testing","title":"References / External links"},{"location":"common/autoware_testing/design/autoware_testing-design/#future-extensions-unimplemented-parts","text":"Adding more types of standard tests.","title":"Future extensions / Unimplemented parts"},{"location":"common/autoware_testing/design/autoware_testing-design/#related-issues","text":"Issue #700: add smoke test Issue #1224: Port other packages with smoke tests to use autoware_testing","title":"Related issues"},{"location":"common/component_interface_utils/","text":"component_interface_utils # Features # This is a utility package that provides the following features: Logging for service and client Usage # This package requires interface information in this format. struct SampleService { using Service = sample_msgs :: srv :: ServiceType ; static constexpr char name [] = \"/sample/service\" ; }; Create a wrapper using the above definition as follows. component_interface_utils :: Service < SampleService >:: SharedPtr srv_ ; srv_ = component_interface_utils :: create_service < SampleService > ( node , ...); Design # This package provides the wrappers for the interface classes of rclcpp. The wrappers limit the usage of the original class to enforce the processing recommended by the component interface. Do not inherit the class of rclcpp, and forward or wrap the member function that is allowed to be used.","title":"component_interface_utils"},{"location":"common/component_interface_utils/#component_interface_utils","text":"","title":"component_interface_utils"},{"location":"common/component_interface_utils/#features","text":"This is a utility package that provides the following features: Logging for service and client","title":"Features"},{"location":"common/component_interface_utils/#usage","text":"This package requires interface information in this format. struct SampleService { using Service = sample_msgs :: srv :: ServiceType ; static constexpr char name [] = \"/sample/service\" ; }; Create a wrapper using the above definition as follows. component_interface_utils :: Service < SampleService >:: SharedPtr srv_ ; srv_ = component_interface_utils :: create_service < SampleService > ( node , ...);","title":"Usage"},{"location":"common/component_interface_utils/#design","text":"This package provides the wrappers for the interface classes of rclcpp. The wrappers limit the usage of the original class to enforce the processing recommended by the component interface. Do not inherit the class of rclcpp, and forward or wrap the member function that is allowed to be used.","title":"Design"},{"location":"common/fake_test_node/design/fake_test_node-design/","text":"Fake Test Node # What this package provides # When writing an integration test for a node in C++ using GTest, there is quite some boilerplate code that needs to be written to set up a fake node that would publish expected messages on an expected topic and subscribes to messages on some other topic. This is usually implemented as a custom GTest fixture. This package contains a library that introduces two utility classes that can be used in place of custom fixtures described above to write integration tests for a node: autoware::tools::testing::FakeTestNode - to use as a custom test fixture with TEST_F tests autoware::tools::testing::FakeTestNodeParametrized - to use a custom test fixture with the parametrized TEST_P tests (accepts a template parameter that gets forwarded to testing::TestWithParam<T> ) These fixtures take care of initializing and re-initializing rclcpp as well as of checking that all subscribers and publishers have a match, thus reducing the amount of boilerplate code that the user needs to write. How to use this library # After including the relevant header the user can use a typedef to use a custom fixture name and use the provided classes as fixtures in TEST_F and TEST_P tests directly. Example usage # Let's say there is a node NodeUnderTest that requires testing. It just subscribes to std_msgs::msg::Int32 messages and publishes a std_msgs::msg::Bool to indicate that the input is positive. To test such a node the following code can be used utilizing the autoware::tools::testing::FakeTestNode : using FakeNodeFixture = autoware :: tools :: testing :: FakeTestNode ; /// @test Test that we can use a non-parametrized test. TEST_F ( FakeNodeFixture , Test ) { Int32 msg {}; msg . data = 15 ; const auto node = std :: make_shared < NodeUnderTest > (); Bool :: SharedPtr last_received_msg {}; auto fake_odom_publisher = create_publisher < Int32 > ( \"/input_topic\" ); auto result_odom_subscription = create_subscription < Bool > ( \"/output_topic\" , * node , [ & last_received_msg ]( const Bool :: SharedPtr msg ) { last_received_msg = msg ;}); const auto dt { std :: chrono :: milliseconds { 100L L }}; const auto max_wait_time { std :: chrono :: seconds { 10L L }}; auto time_passed { std :: chrono :: milliseconds { 0L L }}; while ( ! last_received_msg ) { fake_odom_publisher -> publish ( msg ); rclcpp :: spin_some ( node ); rclcpp :: spin_some ( get_fake_node ()); std :: this_thread :: sleep_for ( dt ); time_passed += dt ; if ( time_passed > max_wait_time ) { FAIL () << \"Did not receive a message soon enough.\" ; } } EXPECT_TRUE ( last_received_msg -> data ); SUCCEED (); } Here only the TEST_F example is shown but a TEST_P usage is very similar with a little bit more boilerplate to set up all the parameter values, see test_fake_test_node.cpp for an example usage.","title":"Fake Test Node"},{"location":"common/fake_test_node/design/fake_test_node-design/#fake-test-node","text":"","title":"Fake Test Node"},{"location":"common/fake_test_node/design/fake_test_node-design/#what-this-package-provides","text":"When writing an integration test for a node in C++ using GTest, there is quite some boilerplate code that needs to be written to set up a fake node that would publish expected messages on an expected topic and subscribes to messages on some other topic. This is usually implemented as a custom GTest fixture. This package contains a library that introduces two utility classes that can be used in place of custom fixtures described above to write integration tests for a node: autoware::tools::testing::FakeTestNode - to use as a custom test fixture with TEST_F tests autoware::tools::testing::FakeTestNodeParametrized - to use a custom test fixture with the parametrized TEST_P tests (accepts a template parameter that gets forwarded to testing::TestWithParam<T> ) These fixtures take care of initializing and re-initializing rclcpp as well as of checking that all subscribers and publishers have a match, thus reducing the amount of boilerplate code that the user needs to write.","title":"What this package provides"},{"location":"common/fake_test_node/design/fake_test_node-design/#how-to-use-this-library","text":"After including the relevant header the user can use a typedef to use a custom fixture name and use the provided classes as fixtures in TEST_F and TEST_P tests directly.","title":"How to use this library"},{"location":"common/fake_test_node/design/fake_test_node-design/#example-usage","text":"Let's say there is a node NodeUnderTest that requires testing. It just subscribes to std_msgs::msg::Int32 messages and publishes a std_msgs::msg::Bool to indicate that the input is positive. To test such a node the following code can be used utilizing the autoware::tools::testing::FakeTestNode : using FakeNodeFixture = autoware :: tools :: testing :: FakeTestNode ; /// @test Test that we can use a non-parametrized test. TEST_F ( FakeNodeFixture , Test ) { Int32 msg {}; msg . data = 15 ; const auto node = std :: make_shared < NodeUnderTest > (); Bool :: SharedPtr last_received_msg {}; auto fake_odom_publisher = create_publisher < Int32 > ( \"/input_topic\" ); auto result_odom_subscription = create_subscription < Bool > ( \"/output_topic\" , * node , [ & last_received_msg ]( const Bool :: SharedPtr msg ) { last_received_msg = msg ;}); const auto dt { std :: chrono :: milliseconds { 100L L }}; const auto max_wait_time { std :: chrono :: seconds { 10L L }}; auto time_passed { std :: chrono :: milliseconds { 0L L }}; while ( ! last_received_msg ) { fake_odom_publisher -> publish ( msg ); rclcpp :: spin_some ( node ); rclcpp :: spin_some ( get_fake_node ()); std :: this_thread :: sleep_for ( dt ); time_passed += dt ; if ( time_passed > max_wait_time ) { FAIL () << \"Did not receive a message soon enough.\" ; } } EXPECT_TRUE ( last_received_msg -> data ); SUCCEED (); } Here only the TEST_F example is shown but a TEST_P usage is very similar with a little bit more boilerplate to set up all the parameter values, see test_fake_test_node.cpp for an example usage.","title":"Example usage"},{"location":"common/global_parameter_loader/Readme/","text":"Autoware Global Parameter Loader # This package is to set common ROS parameters to each node. Usage # Add the following lines to the launch file of the node in which you want to get global parameters. <!-- Global parameters --> <include file= \"$(find-pkg-share global_parameter_loader)/launch/global_params.launch.py\" > <arg name= \"vehicle_model\" value= \"$(var vehicle_model)\" /> </include> The vehicle model parameter is read from config/vehicle_info.param.yaml in vehicle_model _description package. Assumptions / Known limits # Currently only vehicle_info is loaded by this launcher.","title":"Autoware Global Parameter Loader"},{"location":"common/global_parameter_loader/Readme/#autoware-global-parameter-loader","text":"This package is to set common ROS parameters to each node.","title":"Autoware Global Parameter Loader"},{"location":"common/global_parameter_loader/Readme/#usage","text":"Add the following lines to the launch file of the node in which you want to get global parameters. <!-- Global parameters --> <include file= \"$(find-pkg-share global_parameter_loader)/launch/global_params.launch.py\" > <arg name= \"vehicle_model\" value= \"$(var vehicle_model)\" /> </include> The vehicle model parameter is read from config/vehicle_info.param.yaml in vehicle_model _description package.","title":"Usage"},{"location":"common/global_parameter_loader/Readme/#assumptions-known-limits","text":"Currently only vehicle_info is loaded by this launcher.","title":"Assumptions / Known limits"},{"location":"common/goal_distance_calculator/Readme/","text":"goal_distance_calculator # Purpose # This node publishes deviation of self-pose from goal pose. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description /planning/mission_planning/route autoware_auto_planning_msgs::msg::Route Used to get goal pose /tf tf2_msgs/TFMessage TF (self-pose) Output # Name Type Description deviation/lateral tier4_debug_msgs::msg::Float64Stamped publish lateral deviation of self-pose from goal pose[m] deviation/longitudinal tier4_debug_msgs::msg::Float64Stamped publish longitudinal deviation of self-pose from goal pose[m] deviation/yaw tier4_debug_msgs::msg::Float64Stamped publish yaw deviation of self-pose from goal pose[rad] deviation/yaw_deg tier4_debug_msgs::msg::Float64Stamped publish yaw deviation of self-pose from goal pose[deg] Parameters # Node Parameters # Name Type Default Value Explanation update_rate double 10.0 Timer callback period. [Hz] Core Parameters # Name Type Default Value Explanation oneshot bool true publish deviations just once or repeatedly Assumptions / Known limits # TBD.","title":"goal_distance_calculator"},{"location":"common/goal_distance_calculator/Readme/#goal_distance_calculator","text":"","title":"goal_distance_calculator"},{"location":"common/goal_distance_calculator/Readme/#purpose","text":"This node publishes deviation of self-pose from goal pose.","title":"Purpose"},{"location":"common/goal_distance_calculator/Readme/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"common/goal_distance_calculator/Readme/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"common/goal_distance_calculator/Readme/#input","text":"Name Type Description /planning/mission_planning/route autoware_auto_planning_msgs::msg::Route Used to get goal pose /tf tf2_msgs/TFMessage TF (self-pose)","title":"Input"},{"location":"common/goal_distance_calculator/Readme/#output","text":"Name Type Description deviation/lateral tier4_debug_msgs::msg::Float64Stamped publish lateral deviation of self-pose from goal pose[m] deviation/longitudinal tier4_debug_msgs::msg::Float64Stamped publish longitudinal deviation of self-pose from goal pose[m] deviation/yaw tier4_debug_msgs::msg::Float64Stamped publish yaw deviation of self-pose from goal pose[rad] deviation/yaw_deg tier4_debug_msgs::msg::Float64Stamped publish yaw deviation of self-pose from goal pose[deg]","title":"Output"},{"location":"common/goal_distance_calculator/Readme/#parameters","text":"","title":"Parameters"},{"location":"common/goal_distance_calculator/Readme/#node-parameters","text":"Name Type Default Value Explanation update_rate double 10.0 Timer callback period. [Hz]","title":"Node Parameters"},{"location":"common/goal_distance_calculator/Readme/#core-parameters","text":"Name Type Default Value Explanation oneshot bool true publish deviations just once or repeatedly","title":"Core Parameters"},{"location":"common/goal_distance_calculator/Readme/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"common/grid_map_utils/","text":"Grid Map Utils # Overview # This packages contains a re-implementation of the grid_map::PolygonIterator used to iterate over all cells of a grid map contained inside some polygon. Algorithm # This implementation uses the scan line algorithm , a common algorithm used to draw polygons on a rasterized image. The main idea of the algorithm adapted to a grid map is as follow: calculate intersections between rows of the grid map and the edges of the polygon edges; calculate for each row the column between each pair of intersections; the resulting (row, column) indexes are inside of the polygon. More details on the scan line algorithm can be found in the References. API # The grid_map_utils::PolygonIterator follows the same API as the original grid_map::PolygonIterator . Assumptions # The behavior of the grid_map_utils::PolygonIterator is only guaranteed to match the grid_map::PolygonIterator if edges of the polygon do not exactly cross any cell center. In such a case, whether the crossed cell is considered inside or outside of the polygon can vary due to floating precision error. Performances # Benchmarking code is implemented in test/benchmarking.cpp and is also used to validate that the grid_map_utils::PolygonIterator behaves exactly like the grid_map::PolygonIterator . The following figure shows a comparison of the runtime between the implementation of this package ( grid_map_utils ) and the original implementation ( grid_map ). The time measured includes the construction of the iterator and the iteration over all indexes and is shown using a logarithmic scale. Results were obtained varying the side size of a square grid map with 100 <= n <= 1000 (size= n means a grid of n x n cells), random polygons with a number of vertices 3 <= m <= 100 and with each parameter (n,m) repeated 10 times. Future improvements # There exists variations of the scan line algorithm for multiple polygons. These can be implemented if we want to iterate over the cells contained in at least one of multiple polygons. The current implementation imitate the behavior of the original grid_map::PolygonIterator where a cell is selected if its center position is inside the polygon. This behavior could be changed for example to only return all cells overlapped by the polygon. References # https://en.wikipedia.org/wiki/Scanline_rendering https://web.cs.ucdavis.edu/~ma/ECS175_S00/Notes/0411_b.pdf https://www.techfak.uni-bielefeld.de/ags/wbski/lehre/digiSA/WS0607/3DVRCG/Vorlesung/13.RT3DCGVR-vertex-2-fragment.pdf","title":"Grid Map Utils"},{"location":"common/grid_map_utils/#grid-map-utils","text":"","title":"Grid Map Utils"},{"location":"common/grid_map_utils/#overview","text":"This packages contains a re-implementation of the grid_map::PolygonIterator used to iterate over all cells of a grid map contained inside some polygon.","title":"Overview"},{"location":"common/grid_map_utils/#algorithm","text":"This implementation uses the scan line algorithm , a common algorithm used to draw polygons on a rasterized image. The main idea of the algorithm adapted to a grid map is as follow: calculate intersections between rows of the grid map and the edges of the polygon edges; calculate for each row the column between each pair of intersections; the resulting (row, column) indexes are inside of the polygon. More details on the scan line algorithm can be found in the References.","title":"Algorithm"},{"location":"common/grid_map_utils/#api","text":"The grid_map_utils::PolygonIterator follows the same API as the original grid_map::PolygonIterator .","title":"API"},{"location":"common/grid_map_utils/#assumptions","text":"The behavior of the grid_map_utils::PolygonIterator is only guaranteed to match the grid_map::PolygonIterator if edges of the polygon do not exactly cross any cell center. In such a case, whether the crossed cell is considered inside or outside of the polygon can vary due to floating precision error.","title":"Assumptions"},{"location":"common/grid_map_utils/#performances","text":"Benchmarking code is implemented in test/benchmarking.cpp and is also used to validate that the grid_map_utils::PolygonIterator behaves exactly like the grid_map::PolygonIterator . The following figure shows a comparison of the runtime between the implementation of this package ( grid_map_utils ) and the original implementation ( grid_map ). The time measured includes the construction of the iterator and the iteration over all indexes and is shown using a logarithmic scale. Results were obtained varying the side size of a square grid map with 100 <= n <= 1000 (size= n means a grid of n x n cells), random polygons with a number of vertices 3 <= m <= 100 and with each parameter (n,m) repeated 10 times.","title":"Performances"},{"location":"common/grid_map_utils/#future-improvements","text":"There exists variations of the scan line algorithm for multiple polygons. These can be implemented if we want to iterate over the cells contained in at least one of multiple polygons. The current implementation imitate the behavior of the original grid_map::PolygonIterator where a cell is selected if its center position is inside the polygon. This behavior could be changed for example to only return all cells overlapped by the polygon.","title":"Future improvements"},{"location":"common/grid_map_utils/#references","text":"https://en.wikipedia.org/wiki/Scanline_rendering https://web.cs.ucdavis.edu/~ma/ECS175_S00/Notes/0411_b.pdf https://www.techfak.uni-bielefeld.de/ags/wbski/lehre/digiSA/WS0607/3DVRCG/Vorlesung/13.RT3DCGVR-vertex-2-fragment.pdf","title":"References"},{"location":"common/interpolation/","text":"Interpolation package # This package supplies linear and spline interpolation functions. Linear Interpolation # lerp(src_val, dst_val, ratio) (for scalar interpolation) interpolates src_val and dst_val with ratio . This will be replaced with std::lerp(src_val, dst_val, ratio) in C++20 . lerp(base_keys, base_values, query_keys) (for vector interpolation) applies linear regression to each two continuous points whose x values are base_keys and whose y values are base_values . Then it calculates interpolated values on y-axis for query_keys on x-axis. Spline Interpolation # slerp(base_keys, base_values, query_keys) (for vector interpolation) applies spline regression to each two continuous points whose x values are base_keys and whose y values are base_values . Then it calculates interpolated values on y-axis for query_keys on x-axis. Evaluation of calculation cost # We evaluated calculation cost of spline interpolation for 100 points, and adopted the best one which is tridiagonal matrix algorithm. Methods except for tridiagonal matrix algorithm exists in spline_interpolation package, which has been removed from Autoware. Method Calculation time Tridiagonal Matrix Algorithm 0.007 [ms] Preconditioned Conjugate Gradient 0.024 [ms] Successive Over-Relaxation 0.074 [ms] Spline Interpolation Algorithm # Assuming that the size of base_keys ( x_i x_i ) and base_values ( y_i y_i ) are N + 1 N + 1 , we aim to calculate spline interpolation with the following equation to interpolate between y_i y_i and y_{i+1} y_{i+1} . Y_i(x) = a_i (x - x_i)^3 + b_i (x - x_i)^2 + c_i (x - x_i) + d_i \\ \\ \\ (i = 0, \\dots, N-1) Y_i(x) = a_i (x - x_i)^3 + b_i (x - x_i)^2 + c_i (x - x_i) + d_i \\ \\ \\ (i = 0, \\dots, N-1) Constraints on spline interpolation are as follows. The number of constraints is 4N 4N , which is equal to the number of variables of spline interpolation. \\begin{align} Y_i (x_i) & = y_i \\ \\ \\ (i = 0, \\dots, N-1) \\\\ Y_i (x_{i+1}) & = y_{i+1} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ Y'_i (x_{i+1}) & = Y'_{i+1} (x_{i+1}) \\ \\ \\ (i = 0, \\dots, N-2) \\\\ Y''_i (x_{i+1}) & = Y''_{i+1} (x_{i+1}) \\ \\ \\ (i = 0, \\dots, N-2) \\\\ Y''_0 (x_0) & = 0 \\\\ Y''_{N-1} (x_N) & = 0 \\end{align} \\begin{align} Y_i (x_i) & = y_i \\ \\ \\ (i = 0, \\dots, N-1) \\\\ Y_i (x_{i+1}) & = y_{i+1} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ Y'_i (x_{i+1}) & = Y'_{i+1} (x_{i+1}) \\ \\ \\ (i = 0, \\dots, N-2) \\\\ Y''_i (x_{i+1}) & = Y''_{i+1} (x_{i+1}) \\ \\ \\ (i = 0, \\dots, N-2) \\\\ Y''_0 (x_0) & = 0 \\\\ Y''_{N-1} (x_N) & = 0 \\end{align} According to this article , spline interpolation is formulated as the following linear equation. \\begin{align} \\begin{pmatrix} 2(h_0 + h_1) & h_1 \\\\ h_0 & 2 (h_1 + h_2) & h_2 & & O \\\\ & & & \\ddots \\\\ O & & & & h_{N-2} & 2 (h_{N-2} + h_{N-1}) \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_{N-1} \\end{pmatrix}= \\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ \\vdots \\\\ w_{N-1} \\end{pmatrix} \\end{align} \\begin{align} \\begin{pmatrix} 2(h_0 + h_1) & h_1 \\\\ h_0 & 2 (h_1 + h_2) & h_2 & & O \\\\ & & & \\ddots \\\\ O & & & & h_{N-2} & 2 (h_{N-2} + h_{N-1}) \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_{N-1} \\end{pmatrix}= \\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ \\vdots \\\\ w_{N-1} \\end{pmatrix} \\end{align} where \\begin{align} h_i & = x_{i+1} - x_i \\ \\ \\ (i = 0, \\dots, N-1) \\\\ w_i & = 6 \\left(\\frac{y_{i+1} - y_{i+1}}{h_i} - \\frac{y_i - y_{i-1}}{h_{i-1}}\\right) \\ \\ \\ (i = 1, \\dots, N-1) \\end{align} \\begin{align} h_i & = x_{i+1} - x_i \\ \\ \\ (i = 0, \\dots, N-1) \\\\ w_i & = 6 \\left(\\frac{y_{i+1} - y_{i+1}}{h_i} - \\frac{y_i - y_{i-1}}{h_{i-1}}\\right) \\ \\ \\ (i = 1, \\dots, N-1) \\end{align} The coefficient matrix of this linear equation is tridiagonal matrix. Therefore, it can be solve with tridiagonal matrix algorithm, which can solve linear equations without gradient descent methods. Solving this linear equation with tridiagonal matrix algorithm, we can calculate coefficients of spline interpolation as follows. \\begin{align} a_i & = \\frac{v_{i+1} - v_i}{6 (x_{i+1} - x_i)} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ b_i & = \\frac{v_i}{2} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ c_i & = \\frac{y_{i+1} - y_i}{x_{i+1} - x_i} - \\frac{1}{6}(x_{i+1} - x_i)(2 v_i + v_{i+1}) \\ \\ \\ (i = 0, \\dots, N-1) \\\\ d_i & = y_i \\ \\ \\ (i = 0, \\dots, N-1) \\end{align} \\begin{align} a_i & = \\frac{v_{i+1} - v_i}{6 (x_{i+1} - x_i)} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ b_i & = \\frac{v_i}{2} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ c_i & = \\frac{y_{i+1} - y_i}{x_{i+1} - x_i} - \\frac{1}{6}(x_{i+1} - x_i)(2 v_i + v_{i+1}) \\ \\ \\ (i = 0, \\dots, N-1) \\\\ d_i & = y_i \\ \\ \\ (i = 0, \\dots, N-1) \\end{align} Tridiagonal Matrix Algorithm # We solve tridiagonal linear equation according to this article where variables of linear equation are expressed as follows in the implementation. \\begin{align} \\begin{pmatrix} b_0 & c_0 & & \\\\ a_0 & b_1 & c_2 & O \\\\ & & \\ddots \\\\ O & & a_{N-2} & b_{N-1} \\end{pmatrix} x = \\begin{pmatrix} d_0 \\\\ d_2 \\\\ d_3 \\\\ \\vdots \\\\ d_{N-1} \\end{pmatrix} \\end{align} \\begin{align} \\begin{pmatrix} b_0 & c_0 & & \\\\ a_0 & b_1 & c_2 & O \\\\ & & \\ddots \\\\ O & & a_{N-2} & b_{N-1} \\end{pmatrix} x = \\begin{pmatrix} d_0 \\\\ d_2 \\\\ d_3 \\\\ \\vdots \\\\ d_{N-1} \\end{pmatrix} \\end{align}","title":"Interpolation package"},{"location":"common/interpolation/#interpolation-package","text":"This package supplies linear and spline interpolation functions.","title":"Interpolation package"},{"location":"common/interpolation/#linear-interpolation","text":"lerp(src_val, dst_val, ratio) (for scalar interpolation) interpolates src_val and dst_val with ratio . This will be replaced with std::lerp(src_val, dst_val, ratio) in C++20 . lerp(base_keys, base_values, query_keys) (for vector interpolation) applies linear regression to each two continuous points whose x values are base_keys and whose y values are base_values . Then it calculates interpolated values on y-axis for query_keys on x-axis.","title":"Linear Interpolation"},{"location":"common/interpolation/#spline-interpolation","text":"slerp(base_keys, base_values, query_keys) (for vector interpolation) applies spline regression to each two continuous points whose x values are base_keys and whose y values are base_values . Then it calculates interpolated values on y-axis for query_keys on x-axis.","title":"Spline Interpolation"},{"location":"common/interpolation/#evaluation-of-calculation-cost","text":"We evaluated calculation cost of spline interpolation for 100 points, and adopted the best one which is tridiagonal matrix algorithm. Methods except for tridiagonal matrix algorithm exists in spline_interpolation package, which has been removed from Autoware. Method Calculation time Tridiagonal Matrix Algorithm 0.007 [ms] Preconditioned Conjugate Gradient 0.024 [ms] Successive Over-Relaxation 0.074 [ms]","title":"Evaluation of calculation cost"},{"location":"common/interpolation/#spline-interpolation-algorithm","text":"Assuming that the size of base_keys ( x_i x_i ) and base_values ( y_i y_i ) are N + 1 N + 1 , we aim to calculate spline interpolation with the following equation to interpolate between y_i y_i and y_{i+1} y_{i+1} . Y_i(x) = a_i (x - x_i)^3 + b_i (x - x_i)^2 + c_i (x - x_i) + d_i \\ \\ \\ (i = 0, \\dots, N-1) Y_i(x) = a_i (x - x_i)^3 + b_i (x - x_i)^2 + c_i (x - x_i) + d_i \\ \\ \\ (i = 0, \\dots, N-1) Constraints on spline interpolation are as follows. The number of constraints is 4N 4N , which is equal to the number of variables of spline interpolation. \\begin{align} Y_i (x_i) & = y_i \\ \\ \\ (i = 0, \\dots, N-1) \\\\ Y_i (x_{i+1}) & = y_{i+1} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ Y'_i (x_{i+1}) & = Y'_{i+1} (x_{i+1}) \\ \\ \\ (i = 0, \\dots, N-2) \\\\ Y''_i (x_{i+1}) & = Y''_{i+1} (x_{i+1}) \\ \\ \\ (i = 0, \\dots, N-2) \\\\ Y''_0 (x_0) & = 0 \\\\ Y''_{N-1} (x_N) & = 0 \\end{align} \\begin{align} Y_i (x_i) & = y_i \\ \\ \\ (i = 0, \\dots, N-1) \\\\ Y_i (x_{i+1}) & = y_{i+1} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ Y'_i (x_{i+1}) & = Y'_{i+1} (x_{i+1}) \\ \\ \\ (i = 0, \\dots, N-2) \\\\ Y''_i (x_{i+1}) & = Y''_{i+1} (x_{i+1}) \\ \\ \\ (i = 0, \\dots, N-2) \\\\ Y''_0 (x_0) & = 0 \\\\ Y''_{N-1} (x_N) & = 0 \\end{align} According to this article , spline interpolation is formulated as the following linear equation. \\begin{align} \\begin{pmatrix} 2(h_0 + h_1) & h_1 \\\\ h_0 & 2 (h_1 + h_2) & h_2 & & O \\\\ & & & \\ddots \\\\ O & & & & h_{N-2} & 2 (h_{N-2} + h_{N-1}) \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_{N-1} \\end{pmatrix}= \\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ \\vdots \\\\ w_{N-1} \\end{pmatrix} \\end{align} \\begin{align} \\begin{pmatrix} 2(h_0 + h_1) & h_1 \\\\ h_0 & 2 (h_1 + h_2) & h_2 & & O \\\\ & & & \\ddots \\\\ O & & & & h_{N-2} & 2 (h_{N-2} + h_{N-1}) \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_{N-1} \\end{pmatrix}= \\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ \\vdots \\\\ w_{N-1} \\end{pmatrix} \\end{align} where \\begin{align} h_i & = x_{i+1} - x_i \\ \\ \\ (i = 0, \\dots, N-1) \\\\ w_i & = 6 \\left(\\frac{y_{i+1} - y_{i+1}}{h_i} - \\frac{y_i - y_{i-1}}{h_{i-1}}\\right) \\ \\ \\ (i = 1, \\dots, N-1) \\end{align} \\begin{align} h_i & = x_{i+1} - x_i \\ \\ \\ (i = 0, \\dots, N-1) \\\\ w_i & = 6 \\left(\\frac{y_{i+1} - y_{i+1}}{h_i} - \\frac{y_i - y_{i-1}}{h_{i-1}}\\right) \\ \\ \\ (i = 1, \\dots, N-1) \\end{align} The coefficient matrix of this linear equation is tridiagonal matrix. Therefore, it can be solve with tridiagonal matrix algorithm, which can solve linear equations without gradient descent methods. Solving this linear equation with tridiagonal matrix algorithm, we can calculate coefficients of spline interpolation as follows. \\begin{align} a_i & = \\frac{v_{i+1} - v_i}{6 (x_{i+1} - x_i)} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ b_i & = \\frac{v_i}{2} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ c_i & = \\frac{y_{i+1} - y_i}{x_{i+1} - x_i} - \\frac{1}{6}(x_{i+1} - x_i)(2 v_i + v_{i+1}) \\ \\ \\ (i = 0, \\dots, N-1) \\\\ d_i & = y_i \\ \\ \\ (i = 0, \\dots, N-1) \\end{align} \\begin{align} a_i & = \\frac{v_{i+1} - v_i}{6 (x_{i+1} - x_i)} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ b_i & = \\frac{v_i}{2} \\ \\ \\ (i = 0, \\dots, N-1) \\\\ c_i & = \\frac{y_{i+1} - y_i}{x_{i+1} - x_i} - \\frac{1}{6}(x_{i+1} - x_i)(2 v_i + v_{i+1}) \\ \\ \\ (i = 0, \\dots, N-1) \\\\ d_i & = y_i \\ \\ \\ (i = 0, \\dots, N-1) \\end{align}","title":"Spline Interpolation Algorithm"},{"location":"common/interpolation/#tridiagonal-matrix-algorithm","text":"We solve tridiagonal linear equation according to this article where variables of linear equation are expressed as follows in the implementation. \\begin{align} \\begin{pmatrix} b_0 & c_0 & & \\\\ a_0 & b_1 & c_2 & O \\\\ & & \\ddots \\\\ O & & a_{N-2} & b_{N-1} \\end{pmatrix} x = \\begin{pmatrix} d_0 \\\\ d_2 \\\\ d_3 \\\\ \\vdots \\\\ d_{N-1} \\end{pmatrix} \\end{align} \\begin{align} \\begin{pmatrix} b_0 & c_0 & & \\\\ a_0 & b_1 & c_2 & O \\\\ & & \\ddots \\\\ O & & a_{N-2} & b_{N-1} \\end{pmatrix} x = \\begin{pmatrix} d_0 \\\\ d_2 \\\\ d_3 \\\\ \\vdots \\\\ d_{N-1} \\end{pmatrix} \\end{align}","title":"Tridiagonal Matrix Algorithm"},{"location":"common/kalman_filter/","text":"kalman_filter # Purpose # This common package contains the kalman filter with time delay and the calculation of the kalman filter. Assumptions / Known limits # TBD.","title":"kalman_filter"},{"location":"common/kalman_filter/#kalman_filter","text":"","title":"kalman_filter"},{"location":"common/kalman_filter/#purpose","text":"This common package contains the kalman filter with time delay and the calculation of the kalman filter.","title":"Purpose"},{"location":"common/kalman_filter/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"common/neural_networks_provider/design/neural_networks-design/","text":"Macro Syntax Error # Line 1 in Markdown file: Missing end of comment tag # Neural Networks Provider design {#neural-networks-provider-design}","title":"Neural Networks Provider design {#neural-networks-provider-design}"},{"location":"common/neural_networks_provider/design/neural_networks-design/#macro-syntax-error","text":"Line 1 in Markdown file: Missing end of comment tag # Neural Networks Provider design {#neural-networks-provider-design}","title":"Macro Syntax Error"},{"location":"common/osqp_interface/design/osqp_interface-design/","text":"Interface for the OSQP library # This is the design document for the osqp_interface package. Purpose / Use cases # This packages provides a C++ interface for the OSQP library . Design # The class OSQPInterface takes a problem formulation as Eigen matrices and vectors, converts these objects into C-style Compressed-Column-Sparse matrices and dynamic arrays, loads the data into the OSQP workspace dataholder, and runs the optimizer. Inputs / Outputs / API # The interface can be used in several ways: Initialize the interface WITHOUT data. Load the problem formulation at the optimization call. osqp_interface = OSQPInterface (); osqp_interface . optimize ( P , A , q , l , u ); Initialize the interface WITH data. osqp_interface = OSQPInterface ( P , A , q , l , u ); osqp_interface . optimize (); WARM START OPTIMIZATION by modifying the problem formulation between optimization runs. osqp_interface = OSQPInterface ( P , A , q , l , u ); osqp_interface . optimize (); osqp . initializeProblem ( P_new , A_new , q_new , l_new , u_new ); osqp_interface . optimize (); The optimization results are returned as a vector by the optimization function. std :: tuple < std :: vector < double > , std :: vector < double >> result = osqp_interface . optimize (); std :: vector < double > param = std :: get < 0 > ( result ); double x_0 = param [ 0 ]; double x_1 = param [ 1 ]; References / External links # OSQP library: https://osqp.org/ Related issues # This package was introduced as a dependency of the MPC-based lateral controller: https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1057","title":"Interface for the OSQP library"},{"location":"common/osqp_interface/design/osqp_interface-design/#interface-for-the-osqp-library","text":"This is the design document for the osqp_interface package.","title":"Interface for the OSQP library"},{"location":"common/osqp_interface/design/osqp_interface-design/#purpose-use-cases","text":"This packages provides a C++ interface for the OSQP library .","title":"Purpose / Use cases"},{"location":"common/osqp_interface/design/osqp_interface-design/#design","text":"The class OSQPInterface takes a problem formulation as Eigen matrices and vectors, converts these objects into C-style Compressed-Column-Sparse matrices and dynamic arrays, loads the data into the OSQP workspace dataholder, and runs the optimizer.","title":"Design"},{"location":"common/osqp_interface/design/osqp_interface-design/#inputs-outputs-api","text":"The interface can be used in several ways: Initialize the interface WITHOUT data. Load the problem formulation at the optimization call. osqp_interface = OSQPInterface (); osqp_interface . optimize ( P , A , q , l , u ); Initialize the interface WITH data. osqp_interface = OSQPInterface ( P , A , q , l , u ); osqp_interface . optimize (); WARM START OPTIMIZATION by modifying the problem formulation between optimization runs. osqp_interface = OSQPInterface ( P , A , q , l , u ); osqp_interface . optimize (); osqp . initializeProblem ( P_new , A_new , q_new , l_new , u_new ); osqp_interface . optimize (); The optimization results are returned as a vector by the optimization function. std :: tuple < std :: vector < double > , std :: vector < double >> result = osqp_interface . optimize (); std :: vector < double > param = std :: get < 0 > ( result ); double x_0 = param [ 0 ]; double x_1 = param [ 1 ];","title":"Inputs / Outputs / API"},{"location":"common/osqp_interface/design/osqp_interface-design/#references-external-links","text":"OSQP library: https://osqp.org/","title":"References / External links"},{"location":"common/osqp_interface/design/osqp_interface-design/#related-issues","text":"This package was introduced as a dependency of the MPC-based lateral controller: https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1057","title":"Related issues"},{"location":"common/path_distance_calculator/Readme/","text":"path_distance_calculator # Purpose # This node publishes a distance from the closest path point from the self-position to the end point of the path. Note that the distance means the arc-length along the path, not the Euclidean distance between the two points. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description /planning/scenario_planning/lane_driving/behavior_planning/path autoware_auto_planning_msgs::msg::Path Reference path /tf tf2_msgs/TFMessage TF (self-pose) Output # Name Type Description ~/distance tier4_debug_msgs::msg::Float64Stamped Publish a distance from the closest path point from the self-position to the end point of the path[m] Parameters # Node Parameters # None. Core Parameters # None. Assumptions / Known limits # TBD.","title":"path_distance_calculator"},{"location":"common/path_distance_calculator/Readme/#path_distance_calculator","text":"","title":"path_distance_calculator"},{"location":"common/path_distance_calculator/Readme/#purpose","text":"This node publishes a distance from the closest path point from the self-position to the end point of the path. Note that the distance means the arc-length along the path, not the Euclidean distance between the two points.","title":"Purpose"},{"location":"common/path_distance_calculator/Readme/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"common/path_distance_calculator/Readme/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"common/path_distance_calculator/Readme/#input","text":"Name Type Description /planning/scenario_planning/lane_driving/behavior_planning/path autoware_auto_planning_msgs::msg::Path Reference path /tf tf2_msgs/TFMessage TF (self-pose)","title":"Input"},{"location":"common/path_distance_calculator/Readme/#output","text":"Name Type Description ~/distance tier4_debug_msgs::msg::Float64Stamped Publish a distance from the closest path point from the self-position to the end point of the path[m]","title":"Output"},{"location":"common/path_distance_calculator/Readme/#parameters","text":"","title":"Parameters"},{"location":"common/path_distance_calculator/Readme/#node-parameters","text":"None.","title":"Node Parameters"},{"location":"common/path_distance_calculator/Readme/#core-parameters","text":"None.","title":"Core Parameters"},{"location":"common/path_distance_calculator/Readme/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"common/polar_grid/Readme/","text":"Polar Grid # Purpose # This plugin displays polar grid around ego vehicle in Rviz. Core Parameters # Name Type Default Value Explanation Max Range float 200.0f max range for polar grid. [m] Wave Velocity float 100.0f wave ring velocity. [m/s] Delta Range float 10.0f wave ring distance for polar grid. [m] Assumptions / Known limits # TBD.","title":"Polar Grid"},{"location":"common/polar_grid/Readme/#polar-grid","text":"","title":"Polar Grid"},{"location":"common/polar_grid/Readme/#purpose","text":"This plugin displays polar grid around ego vehicle in Rviz.","title":"Purpose"},{"location":"common/polar_grid/Readme/#core-parameters","text":"Name Type Default Value Explanation Max Range float 200.0f max range for polar grid. [m] Wave Velocity float 100.0f wave ring velocity. [m/s] Delta Range float 10.0f wave ring distance for polar grid. [m]","title":"Core Parameters"},{"location":"common/polar_grid/Readme/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"common/signal_processing/","text":"signal_processing # low-pass filter currently supports only the 1-D low pass filtering. Assumptions / Known limits # TBD.","title":"signal_processing"},{"location":"common/signal_processing/#signal_processing","text":"low-pass filter currently supports only the 1-D low pass filtering.","title":"signal_processing"},{"location":"common/signal_processing/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"common/tier4_autoware_utils/","text":"tier4_autoware_utils # Purpose # This package contains many common functions used by other packages, so please refer to them as needed.","title":"tier4_autoware_utils"},{"location":"common/tier4_autoware_utils/#tier4_autoware_utils","text":"","title":"tier4_autoware_utils"},{"location":"common/tier4_autoware_utils/#purpose","text":"This package contains many common functions used by other packages, so please refer to them as needed.","title":"Purpose"},{"location":"common/tier4_autoware_utils/docs/vehicle/vehicle/","text":"vehicle utils # Vehicle utils provides a convenient library used to check vehicle status. Feature # The library contains following classes. vehicle_stop_checker # This class check whether the vehicle is stopped or not based on localization result. Subscribed Topics # Name Type Description /localization/kinematic_state nav_msgs::msg::Odometry vehicle odometry Parameters # Name Type Default Value Explanation velocity_buffer_time_sec double 10.0 odometry buffering time [s] Member functions # bool isVehicleStopped ( const double stop_duration ) Check simply whether the vehicle is stopped based on the localization result. Returns true if the vehicle is stopped, even if system outputs a non-zero target velocity. Example Usage # Necessary includes: #include <tier4_autoware_utils/vehicle/vehicle_state_checker.hpp> 1.Create a checker instance. class SampleNode : public rclcpp :: Node { public : SampleNode () : Node ( \"sample_node\" ) { vehicle_stop_checker_ = std :: make_unique < VehicleStopChecker > ( this ); } std :: unique_ptr < VehicleStopChecker > vehicle_stop_checker_ ; bool sampleFunc (); ... } 2.Check the vehicle state. bool SampleNode::sampleFunc () { ... const auto result_1 = vehicle_stop_checker_ -> isVehicleStopped (); ... const auto result_2 = vehicle_stop_checker_ -> isVehicleStopped ( 3.0 ); ... } vehicle_arrival_checker # This class check whether the vehicle arrive at stop point based on localization and planning result. Subscribed Topics # Name Type Description /localization/kinematic_state nav_msgs::msg::Odometry vehicle odometry /planning/scenario_planning/trajectory autoware_auto_planning_msgs::msg::Trajectory trajectory Parameters # Name Type Default Value Explanation velocity_buffer_time_sec double 10.0 odometry buffering time [s] th_arrived_distance_m double 1.0 threshold distance to check if vehicle has arrived at target point [m] Member functions # bool isVehicleStopped ( const double stop_duration ) Check simply whether the vehicle is stopped based on the localization result. Returns true if the vehicle is stopped, even if system outputs a non-zero target velocity. bool isVehicleStoppedAtStopPoint ( const double stop_duration ) Check whether the vehicle is stopped at stop point based on the localization and planning result. Returns true if the vehicle is not only stopped but also arrived at stop point. Example Usage # Necessary includes: #include <tier4_autoware_utils/vehicle/vehicle_state_checker.hpp> 1.Create a checker instance. class SampleNode : public rclcpp :: Node { public : SampleNode () : Node ( \"sample_node\" ) { vehicle_arrival_checker_ = std :: make_unique < VehicleArrivalChecker > ( this ); } std :: unique_ptr < VehicleArrivalChecker > vehicle_arrival_checker_ ; bool sampleFunc (); ... } 2.Check the vehicle state. bool SampleNode::sampleFunc1 () { ... const auto result_1 = vehicle_arrival_checker_ -> isVehicleStopped (); ... const auto result_2 = vehicle_arrival_checker_ -> isVehicleStopped ( 3.0 ); ... const auto result_3 = vehicle_arrival_checker_ -> isVehicleStoppedAtStopPoint (); ... const auto result_4 = vehicle_arrival_checker_ -> isVehicleStoppedAtStopPoint ( 3.0 ); ... } Assumptions / Known limits # vehicle_stop_checker and vehicle_arrival_checker cannot check whether the vehicle is stopped more than velocity_buffer_time_sec second.","title":"vehicle utils"},{"location":"common/tier4_autoware_utils/docs/vehicle/vehicle/#vehicle-utils","text":"Vehicle utils provides a convenient library used to check vehicle status.","title":"vehicle utils"},{"location":"common/tier4_autoware_utils/docs/vehicle/vehicle/#feature","text":"The library contains following classes.","title":"Feature"},{"location":"common/tier4_autoware_utils/docs/vehicle/vehicle/#vehicle_stop_checker","text":"This class check whether the vehicle is stopped or not based on localization result.","title":"vehicle_stop_checker"},{"location":"common/tier4_autoware_utils/docs/vehicle/vehicle/#subscribed-topics","text":"Name Type Description /localization/kinematic_state nav_msgs::msg::Odometry vehicle odometry","title":"Subscribed Topics"},{"location":"common/tier4_autoware_utils/docs/vehicle/vehicle/#parameters","text":"Name Type Default Value Explanation velocity_buffer_time_sec double 10.0 odometry buffering time [s]","title":"Parameters"},{"location":"common/tier4_autoware_utils/docs/vehicle/vehicle/#member-functions","text":"bool isVehicleStopped ( const double stop_duration ) Check simply whether the vehicle is stopped based on the localization result. Returns true if the vehicle is stopped, even if system outputs a non-zero target velocity.","title":"Member functions"},{"location":"common/tier4_autoware_utils/docs/vehicle/vehicle/#example-usage","text":"Necessary includes: #include <tier4_autoware_utils/vehicle/vehicle_state_checker.hpp> 1.Create a checker instance. class SampleNode : public rclcpp :: Node { public : SampleNode () : Node ( \"sample_node\" ) { vehicle_stop_checker_ = std :: make_unique < VehicleStopChecker > ( this ); } std :: unique_ptr < VehicleStopChecker > vehicle_stop_checker_ ; bool sampleFunc (); ... } 2.Check the vehicle state. bool SampleNode::sampleFunc () { ... const auto result_1 = vehicle_stop_checker_ -> isVehicleStopped (); ... const auto result_2 = vehicle_stop_checker_ -> isVehicleStopped ( 3.0 ); ... }","title":"Example Usage"},{"location":"common/tier4_autoware_utils/docs/vehicle/vehicle/#vehicle_arrival_checker","text":"This class check whether the vehicle arrive at stop point based on localization and planning result.","title":"vehicle_arrival_checker"},{"location":"common/tier4_autoware_utils/docs/vehicle/vehicle/#subscribed-topics_1","text":"Name Type Description /localization/kinematic_state nav_msgs::msg::Odometry vehicle odometry /planning/scenario_planning/trajectory autoware_auto_planning_msgs::msg::Trajectory trajectory","title":"Subscribed Topics"},{"location":"common/tier4_autoware_utils/docs/vehicle/vehicle/#parameters_1","text":"Name Type Default Value Explanation velocity_buffer_time_sec double 10.0 odometry buffering time [s] th_arrived_distance_m double 1.0 threshold distance to check if vehicle has arrived at target point [m]","title":"Parameters"},{"location":"common/tier4_autoware_utils/docs/vehicle/vehicle/#member-functions_1","text":"bool isVehicleStopped ( const double stop_duration ) Check simply whether the vehicle is stopped based on the localization result. Returns true if the vehicle is stopped, even if system outputs a non-zero target velocity. bool isVehicleStoppedAtStopPoint ( const double stop_duration ) Check whether the vehicle is stopped at stop point based on the localization and planning result. Returns true if the vehicle is not only stopped but also arrived at stop point.","title":"Member functions"},{"location":"common/tier4_autoware_utils/docs/vehicle/vehicle/#example-usage_1","text":"Necessary includes: #include <tier4_autoware_utils/vehicle/vehicle_state_checker.hpp> 1.Create a checker instance. class SampleNode : public rclcpp :: Node { public : SampleNode () : Node ( \"sample_node\" ) { vehicle_arrival_checker_ = std :: make_unique < VehicleArrivalChecker > ( this ); } std :: unique_ptr < VehicleArrivalChecker > vehicle_arrival_checker_ ; bool sampleFunc (); ... } 2.Check the vehicle state. bool SampleNode::sampleFunc1 () { ... const auto result_1 = vehicle_arrival_checker_ -> isVehicleStopped (); ... const auto result_2 = vehicle_arrival_checker_ -> isVehicleStopped ( 3.0 ); ... const auto result_3 = vehicle_arrival_checker_ -> isVehicleStoppedAtStopPoint (); ... const auto result_4 = vehicle_arrival_checker_ -> isVehicleStoppedAtStopPoint ( 3.0 ); ... }","title":"Example Usage"},{"location":"common/tier4_autoware_utils/docs/vehicle/vehicle/#assumptions-known-limits","text":"vehicle_stop_checker and vehicle_arrival_checker cannot check whether the vehicle is stopped more than velocity_buffer_time_sec second.","title":"Assumptions / Known limits"},{"location":"common/tier4_control_rviz_plugin/","text":"tier4_control_rviz_plugin # This package is to mimic external control for simulation. Inputs / Outputs # Input # Name Type Description /control/current_gate_mode tier4_control_msgs::msg::GateMode Current GATE mode /vehicle/status/velocity_status autoware_auto_vehicle_msgs::msg::VelocityReport Current velocity status /api/autoware/get/engage tier4_external_api_msgs::srv::Engage Getting Engage /vehicle/status/gear_status autoware_auto_vehicle_msgs::msg::GearReport The state of GEAR Output # Name Type Description /control/gate_mode_cmd tier4_control_msgs::msg::GateMode GATE mode /external/selected/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand AckermannControlCommand /external/selected/gear_cmd autoware_auto_vehicle_msgs::msg::GearCommand GEAR Usage # Start rviz and select Panels. Select tier4_control_rviz_plugin/ManualController and press OK. Enter velocity in \"Set Cruise Velocity\" and Press the button to confirm. You can notice that GEAR shows D (DRIVE). Press \"Enable Manual Control\" and you can notice that \"GATE\" and \"Engage\" turn \"Ready\" and the vehicle starts!","title":"tier4_control_rviz_plugin"},{"location":"common/tier4_control_rviz_plugin/#tier4_control_rviz_plugin","text":"This package is to mimic external control for simulation.","title":"tier4_control_rviz_plugin"},{"location":"common/tier4_control_rviz_plugin/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"common/tier4_control_rviz_plugin/#input","text":"Name Type Description /control/current_gate_mode tier4_control_msgs::msg::GateMode Current GATE mode /vehicle/status/velocity_status autoware_auto_vehicle_msgs::msg::VelocityReport Current velocity status /api/autoware/get/engage tier4_external_api_msgs::srv::Engage Getting Engage /vehicle/status/gear_status autoware_auto_vehicle_msgs::msg::GearReport The state of GEAR","title":"Input"},{"location":"common/tier4_control_rviz_plugin/#output","text":"Name Type Description /control/gate_mode_cmd tier4_control_msgs::msg::GateMode GATE mode /external/selected/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand AckermannControlCommand /external/selected/gear_cmd autoware_auto_vehicle_msgs::msg::GearCommand GEAR","title":"Output"},{"location":"common/tier4_control_rviz_plugin/#usage","text":"Start rviz and select Panels. Select tier4_control_rviz_plugin/ManualController and press OK. Enter velocity in \"Set Cruise Velocity\" and Press the button to confirm. You can notice that GEAR shows D (DRIVE). Press \"Enable Manual Control\" and you can notice that \"GATE\" and \"Engage\" turn \"Ready\" and the vehicle starts!","title":"Usage"},{"location":"common/tier4_datetime_rviz_plugin/","text":"tier4_datetime_rviz_plugin # Purpose # This plugin displays the ROS Time and Wall Time in rviz. Assumptions / Known limits # TBD. Usage # Start rviz and select panels/Add new panel. Select tier4_datetime_rviz_plugin/AutowareDateTimePanel and press OK.","title":"tier4_datetime_rviz_plugin"},{"location":"common/tier4_datetime_rviz_plugin/#tier4_datetime_rviz_plugin","text":"","title":"tier4_datetime_rviz_plugin"},{"location":"common/tier4_datetime_rviz_plugin/#purpose","text":"This plugin displays the ROS Time and Wall Time in rviz.","title":"Purpose"},{"location":"common/tier4_datetime_rviz_plugin/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"common/tier4_datetime_rviz_plugin/#usage","text":"Start rviz and select panels/Add new panel. Select tier4_datetime_rviz_plugin/AutowareDateTimePanel and press OK.","title":"Usage"},{"location":"common/tier4_debug_tools/","text":"tier4_debug_tools # This package provides useful features for debugging Autoware. Usage # tf2pose # This tool converts any tf to pose topic. With this tool, for example, you can plot x values of tf in rqt_multiplot . ros2 run tier4_debug_tools tf2pose { tf_from } { tf_to } { hz } Example: $ ros2 run tier4_debug_tools tf2pose base_link ndt_base_link 100 $ ros2 topic echo /tf2pose/pose -n1 header: seq: 13 stamp: secs: 1605168366 nsecs: 549174070 frame_id: \"base_link\" pose: position: x: 0 .0387684271191 y: -0.00320360406477 z: 0 .000276674520819 orientation: x: 0 .000335221893885 y: 0 .000122020672186 z: -0.00539673212896 w: 0 .999985368502 --- pose2tf # This tool converts any pose topic to tf . ros2 run tier4_debug_tools pose2tf { pose_topic_name } { tf_name } Example: $ ros2 run tier4_debug_tools pose2tf /localization/pose_estimator/pose ndt_pose $ ros2 run tf tf_echo ndt_pose ndt_base_link 100 At time 1605168365 .449 - Translation: [ 0 .000, 0 .000, 0 .000 ] - Rotation: in Quaternion [ 0 .000, 0 .000, 0 .000, 1 .000 ] in RPY ( radian ) [ 0 .000, -0.000, 0 .000 ] in RPY ( degree ) [ 0 .000, -0.000, 0 .000 ] stop_reason2pose # This tool extracts pose from stop_reasons . Topics without numbers such as /stop_reason2pose/pose/detection_area are the nearest stop_reasons, and topics with numbers are individual stop_reasons that are roughly matched with previous ones. ros2 run tier4_debug_tools stop_reason2pose { stop_reason_topic_name } Example: $ ros2 run tier4_debug_tools stop_reason2pose /planning/scenario_planning/status/stop_reasons $ ros2 topic list | ag stop_reason2pose /stop_reason2pose/pose/detection_area /stop_reason2pose/pose/detection_area_1 /stop_reason2pose/pose/obstacle_stop /stop_reason2pose/pose/obstacle_stop_1 $ ros2 topic echo /stop_reason2pose/pose/detection_area -n1 header: seq: 1 stamp: secs: 1605168355 nsecs: 821713 frame_id: \"map\" pose: position: x: 60608 .8433457 y: 43886 .2410876 z: 44 .9078212441 orientation: x: 0 .0 y: 0 .0 z: -0.190261378408 w: 0 .981733470901 --- stop_reason2tf # This is an all-in-one script that uses tf2pose , pose2tf , and stop_reason2pose . With this tool, you can view the relative position from base_link to the nearest stop_reason. ros2 run tier4_debug_tools stop_reason2tf { stop_reason_name } Example: $ ros2 run tier4_debug_tools stop_reason2tf obstacle_stop At time 1605168359 .501 - Translation: [ 0 .291, -0.095, 0 .266 ] - Rotation: in Quaternion [ 0 .007, 0 .011, -0.005, 1 .000 ] in RPY ( radian ) [ 0 .014, 0 .023, -0.010 ] in RPY ( degree ) [ 0 .825, 1 .305, -0.573 ] lateral_error_publisher # This node calculate the control error and localization error in the trajectory normal direction as shown in the figure below. Set the reference trajectory, vehicle pose and ground truth pose in the launch file. ros2 launch tier4_debug_tools lateral_error_publisher.launch.xml","title":"tier4_debug_tools"},{"location":"common/tier4_debug_tools/#tier4_debug_tools","text":"This package provides useful features for debugging Autoware.","title":"tier4_debug_tools"},{"location":"common/tier4_debug_tools/#usage","text":"","title":"Usage"},{"location":"common/tier4_debug_tools/#tf2pose","text":"This tool converts any tf to pose topic. With this tool, for example, you can plot x values of tf in rqt_multiplot . ros2 run tier4_debug_tools tf2pose { tf_from } { tf_to } { hz } Example: $ ros2 run tier4_debug_tools tf2pose base_link ndt_base_link 100 $ ros2 topic echo /tf2pose/pose -n1 header: seq: 13 stamp: secs: 1605168366 nsecs: 549174070 frame_id: \"base_link\" pose: position: x: 0 .0387684271191 y: -0.00320360406477 z: 0 .000276674520819 orientation: x: 0 .000335221893885 y: 0 .000122020672186 z: -0.00539673212896 w: 0 .999985368502 ---","title":"tf2pose"},{"location":"common/tier4_debug_tools/#pose2tf","text":"This tool converts any pose topic to tf . ros2 run tier4_debug_tools pose2tf { pose_topic_name } { tf_name } Example: $ ros2 run tier4_debug_tools pose2tf /localization/pose_estimator/pose ndt_pose $ ros2 run tf tf_echo ndt_pose ndt_base_link 100 At time 1605168365 .449 - Translation: [ 0 .000, 0 .000, 0 .000 ] - Rotation: in Quaternion [ 0 .000, 0 .000, 0 .000, 1 .000 ] in RPY ( radian ) [ 0 .000, -0.000, 0 .000 ] in RPY ( degree ) [ 0 .000, -0.000, 0 .000 ]","title":"pose2tf"},{"location":"common/tier4_debug_tools/#stop_reason2pose","text":"This tool extracts pose from stop_reasons . Topics without numbers such as /stop_reason2pose/pose/detection_area are the nearest stop_reasons, and topics with numbers are individual stop_reasons that are roughly matched with previous ones. ros2 run tier4_debug_tools stop_reason2pose { stop_reason_topic_name } Example: $ ros2 run tier4_debug_tools stop_reason2pose /planning/scenario_planning/status/stop_reasons $ ros2 topic list | ag stop_reason2pose /stop_reason2pose/pose/detection_area /stop_reason2pose/pose/detection_area_1 /stop_reason2pose/pose/obstacle_stop /stop_reason2pose/pose/obstacle_stop_1 $ ros2 topic echo /stop_reason2pose/pose/detection_area -n1 header: seq: 1 stamp: secs: 1605168355 nsecs: 821713 frame_id: \"map\" pose: position: x: 60608 .8433457 y: 43886 .2410876 z: 44 .9078212441 orientation: x: 0 .0 y: 0 .0 z: -0.190261378408 w: 0 .981733470901 ---","title":"stop_reason2pose"},{"location":"common/tier4_debug_tools/#stop_reason2tf","text":"This is an all-in-one script that uses tf2pose , pose2tf , and stop_reason2pose . With this tool, you can view the relative position from base_link to the nearest stop_reason. ros2 run tier4_debug_tools stop_reason2tf { stop_reason_name } Example: $ ros2 run tier4_debug_tools stop_reason2tf obstacle_stop At time 1605168359 .501 - Translation: [ 0 .291, -0.095, 0 .266 ] - Rotation: in Quaternion [ 0 .007, 0 .011, -0.005, 1 .000 ] in RPY ( radian ) [ 0 .014, 0 .023, -0.010 ] in RPY ( degree ) [ 0 .825, 1 .305, -0.573 ]","title":"stop_reason2tf"},{"location":"common/tier4_debug_tools/#lateral_error_publisher","text":"This node calculate the control error and localization error in the trajectory normal direction as shown in the figure below. Set the reference trajectory, vehicle pose and ground truth pose in the launch file. ros2 launch tier4_debug_tools lateral_error_publisher.launch.xml","title":"lateral_error_publisher"},{"location":"common/tier4_localization_rviz_plugin/","text":"tier4_localization_rviz_plugin # Purpose # This plugin can display the history of the localization obtained by ekf_localizer or ndt_scan_matching. Inputs / Outputs # Input # Name Type Description input/pose geometry_msgs::msg::PoseStamped In input/pose, put the result of localization calculated by ekf_localizer or ndt_scan_matching Parameters # Core Parameters # Name Type Default Value Description property_buffer_size_ int 100 Buffer size of topic property_line_view_ bool true Use Line property or not property_line_width_ float 0.1 Width of Line property [m] property_line_alpha_ float 1.0 Alpha of Line property property_line_color_ QColor Qt::white Color of Line property Assumptions / Known limits # TBD. Usage # Start rviz and select Add under the Displays panel. Select tier4_localization_rviz_plugin/PoseHistory and press OK. Enter the name of the topic where you want to view the trajectory.","title":"tier4_localization_rviz_plugin"},{"location":"common/tier4_localization_rviz_plugin/#tier4_localization_rviz_plugin","text":"","title":"tier4_localization_rviz_plugin"},{"location":"common/tier4_localization_rviz_plugin/#purpose","text":"This plugin can display the history of the localization obtained by ekf_localizer or ndt_scan_matching.","title":"Purpose"},{"location":"common/tier4_localization_rviz_plugin/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"common/tier4_localization_rviz_plugin/#input","text":"Name Type Description input/pose geometry_msgs::msg::PoseStamped In input/pose, put the result of localization calculated by ekf_localizer or ndt_scan_matching","title":"Input"},{"location":"common/tier4_localization_rviz_plugin/#parameters","text":"","title":"Parameters"},{"location":"common/tier4_localization_rviz_plugin/#core-parameters","text":"Name Type Default Value Description property_buffer_size_ int 100 Buffer size of topic property_line_view_ bool true Use Line property or not property_line_width_ float 0.1 Width of Line property [m] property_line_alpha_ float 1.0 Alpha of Line property property_line_color_ QColor Qt::white Color of Line property","title":"Core Parameters"},{"location":"common/tier4_localization_rviz_plugin/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"common/tier4_localization_rviz_plugin/#usage","text":"Start rviz and select Add under the Displays panel. Select tier4_localization_rviz_plugin/PoseHistory and press OK. Enter the name of the topic where you want to view the trajectory.","title":"Usage"},{"location":"common/tier4_perception_rviz_plugin/","text":"tier4_perception_rviz_plugin # Purpose # This plugin is used to generate dummy pedestrians, cars, and obstacles in planning simulator. Overview # The CarInitialPoseTool sends a topic for generating a dummy car. The PedestrianInitialPoseTool sends a topic for generating a dummy pedestrian. The UnknownInitialPoseTool sends a topic for generating a dummy obstacle. The DeleteAllObjectsTool deletes the dummy cars, pedestrians, and obstacles displayed by the above three tools. Inputs / Outputs # Output # Name Type Description /simulation/dummy_perception_publisher/object_info dummy_perception_publisher::msg::Object The topic on which to publish dummy object info Parameter # Core Parameters # CarPose # Name Type Default Value Description topic_property_ string /simulation/dummy_perception_publisher/object_info The topic on which to publish dummy object info std_dev_x_ float 0.03 X standard deviation for initial pose [m] std_dev_y_ float 0.03 Y standard deviation for initial pose [m] std_dev_z_ float 0.03 Z standard deviation for initial pose [m] std_dev_theta_ float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] length_ float 4.0 X standard deviation for initial pose [m] width_ float 1.8 Y standard deviation for initial pose [m] height_ float 2.0 Z standard deviation for initial pose [m] position_z_ float 0.0 Z position for initial pose [m] velocity_ float 0.0 Velocity [m/s] BusPose # Name Type Default Value Description topic_property_ string /simulation/dummy_perception_publisher/object_info The topic on which to publish dummy object info std_dev_x_ float 0.03 X standard deviation for initial pose [m] std_dev_y_ float 0.03 Y standard deviation for initial pose [m] std_dev_z_ float 0.03 Z standard deviation for initial pose [m] std_dev_theta_ float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] length_ float 10.5 X standard deviation for initial pose [m] width_ float 2.5 Y standard deviation for initial pose [m] height_ float 3.5 Z standard deviation for initial pose [m] position_z_ float 0.0 Z position for initial pose [m] velocity_ float 0.0 Velocity [m/s] PedestrianPose # Name Type Default Value Description topic_property_ string /simulation/dummy_perception_publisher/object_info The topic on which to publish dummy object info std_dev_x_ float 0.03 X standard deviation for initial pose [m] std_dev_y_ float 0.03 Y standard deviation for initial pose [m] std_dev_z_ float 0.03 Z standard deviation for initial pose [m] std_dev_theta_ float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] position_z_ float 0.0 Z position for initial pose [m] velocity_ float 0.0 Velocity [m/s] UnknownPose # Name Type Default Value Description topic_property_ string /simulation/dummy_perception_publisher/object_info The topic on which to publish dummy object info std_dev_x_ float 0.03 X standard deviation for initial pose [m] std_dev_y_ float 0.03 Y standard deviation for initial pose [m] std_dev_z_ float 0.03 Z standard deviation for initial pose [m] std_dev_theta_ float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] position_z_ float 0.0 Z position for initial pose [m] velocity_ float 0.0 Velocity [m/s] DeleteAllObjects # Name Type Default Value Description topic_property_ string /simulation/dummy_perception_publisher/object_info The topic on which to publish dummy object info Assumptions / Known limits # Using a planning simulator Usage # Start rviz and select + on the tool tab. Select one of the following: tier4_perception_rviz_plugin and press OK. Select the new item in the tool tab (2D Dummy Car in the example) and click on it in rviz. Interactive manipulation # You can interactively manipulate the object. Select \"Tool Properties\" in rviz. Select the corresponding object tab in the Tool Properties. Turn the \"Interactive\" checkbox on. Select the item in the tool tab in you haven't chosen yet. Key commands are as follows. action key command ADD Shift + Click Right Button MOVE Hold down Right Button + Drug and Drop DELETE Alt + Click Right Button","title":"tier4_perception_rviz_plugin"},{"location":"common/tier4_perception_rviz_plugin/#tier4_perception_rviz_plugin","text":"","title":"tier4_perception_rviz_plugin"},{"location":"common/tier4_perception_rviz_plugin/#purpose","text":"This plugin is used to generate dummy pedestrians, cars, and obstacles in planning simulator.","title":"Purpose"},{"location":"common/tier4_perception_rviz_plugin/#overview","text":"The CarInitialPoseTool sends a topic for generating a dummy car. The PedestrianInitialPoseTool sends a topic for generating a dummy pedestrian. The UnknownInitialPoseTool sends a topic for generating a dummy obstacle. The DeleteAllObjectsTool deletes the dummy cars, pedestrians, and obstacles displayed by the above three tools.","title":"Overview"},{"location":"common/tier4_perception_rviz_plugin/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"common/tier4_perception_rviz_plugin/#output","text":"Name Type Description /simulation/dummy_perception_publisher/object_info dummy_perception_publisher::msg::Object The topic on which to publish dummy object info","title":"Output"},{"location":"common/tier4_perception_rviz_plugin/#parameter","text":"","title":"Parameter"},{"location":"common/tier4_perception_rviz_plugin/#core-parameters","text":"","title":"Core Parameters"},{"location":"common/tier4_perception_rviz_plugin/#carpose","text":"Name Type Default Value Description topic_property_ string /simulation/dummy_perception_publisher/object_info The topic on which to publish dummy object info std_dev_x_ float 0.03 X standard deviation for initial pose [m] std_dev_y_ float 0.03 Y standard deviation for initial pose [m] std_dev_z_ float 0.03 Z standard deviation for initial pose [m] std_dev_theta_ float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] length_ float 4.0 X standard deviation for initial pose [m] width_ float 1.8 Y standard deviation for initial pose [m] height_ float 2.0 Z standard deviation for initial pose [m] position_z_ float 0.0 Z position for initial pose [m] velocity_ float 0.0 Velocity [m/s]","title":"CarPose"},{"location":"common/tier4_perception_rviz_plugin/#buspose","text":"Name Type Default Value Description topic_property_ string /simulation/dummy_perception_publisher/object_info The topic on which to publish dummy object info std_dev_x_ float 0.03 X standard deviation for initial pose [m] std_dev_y_ float 0.03 Y standard deviation for initial pose [m] std_dev_z_ float 0.03 Z standard deviation for initial pose [m] std_dev_theta_ float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] length_ float 10.5 X standard deviation for initial pose [m] width_ float 2.5 Y standard deviation for initial pose [m] height_ float 3.5 Z standard deviation for initial pose [m] position_z_ float 0.0 Z position for initial pose [m] velocity_ float 0.0 Velocity [m/s]","title":"BusPose"},{"location":"common/tier4_perception_rviz_plugin/#pedestrianpose","text":"Name Type Default Value Description topic_property_ string /simulation/dummy_perception_publisher/object_info The topic on which to publish dummy object info std_dev_x_ float 0.03 X standard deviation for initial pose [m] std_dev_y_ float 0.03 Y standard deviation for initial pose [m] std_dev_z_ float 0.03 Z standard deviation for initial pose [m] std_dev_theta_ float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] position_z_ float 0.0 Z position for initial pose [m] velocity_ float 0.0 Velocity [m/s]","title":"PedestrianPose"},{"location":"common/tier4_perception_rviz_plugin/#unknownpose","text":"Name Type Default Value Description topic_property_ string /simulation/dummy_perception_publisher/object_info The topic on which to publish dummy object info std_dev_x_ float 0.03 X standard deviation for initial pose [m] std_dev_y_ float 0.03 Y standard deviation for initial pose [m] std_dev_z_ float 0.03 Z standard deviation for initial pose [m] std_dev_theta_ float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] position_z_ float 0.0 Z position for initial pose [m] velocity_ float 0.0 Velocity [m/s]","title":"UnknownPose"},{"location":"common/tier4_perception_rviz_plugin/#deleteallobjects","text":"Name Type Default Value Description topic_property_ string /simulation/dummy_perception_publisher/object_info The topic on which to publish dummy object info","title":"DeleteAllObjects"},{"location":"common/tier4_perception_rviz_plugin/#assumptions-known-limits","text":"Using a planning simulator","title":"Assumptions / Known limits"},{"location":"common/tier4_perception_rviz_plugin/#usage","text":"Start rviz and select + on the tool tab. Select one of the following: tier4_perception_rviz_plugin and press OK. Select the new item in the tool tab (2D Dummy Car in the example) and click on it in rviz.","title":"Usage"},{"location":"common/tier4_perception_rviz_plugin/#interactive-manipulation","text":"You can interactively manipulate the object. Select \"Tool Properties\" in rviz. Select the corresponding object tab in the Tool Properties. Turn the \"Interactive\" checkbox on. Select the item in the tool tab in you haven't chosen yet. Key commands are as follows. action key command ADD Shift + Click Right Button MOVE Hold down Right Button + Drug and Drop DELETE Alt + Click Right Button","title":"Interactive manipulation"},{"location":"common/tier4_planning_rviz_plugin/","text":"tier4_planning_rviz_plugin # This package is including jsk code. Note that jsk_overlay_utils.cpp and jsk_overlay_utils.hpp are BSD license. Purpose # This plugin displays the path, trajectory, and maximum speed. Inputs / Outputs # Input # Name Type Description /input/path autoware_auto_planning_msgs::msg::Path The topic on which to subscribe path /input/trajectory autoware_auto_planning_msgs::msg::Trajectory The topic on which to subscribe trajectory /planning/scenario_planning/current_max_velocity tier4_planning_msgs/msg/VelocityLimit The topic on which to publish max velocity Output # Name Type Description /planning/mission_planning/checkpoint geometry_msgs/msg/PoseStamped The topic on which to publish checkpoint Parameter # Core Parameters # MissionCheckpoint # Name Type Default Value Description pose_topic_property_ string mission_checkpoint The topic on which to publish checkpoint std_dev_x_ float 0.5 X standard deviation for checkpoint pose [m] std_dev_y_ float 0.5 Y standard deviation for checkpoint pose [m] std_dev_theta_ float M_PI / 12.0 Theta standard deviation for checkpoint pose [rad] position_z_ float 0.0 Z position for checkpoint pose [m] Path # Name Type Default Value Description property_path_view_ bool true Use Path property or not property_path_width_ float 2.0 Width of Path property [m] property_path_alpha_ float 1.0 Alpha of Path property property_path_color_view_ bool false Use Constant Color or not property_path_color_ QColor Qt::black Color of Path property property_velocity_view_ bool true Use Velocity property or not property_velocity_alpha_ float 1.0 Alpha of Velocity property property_velocity_scale_ float 0.3 Scale of Velocity property property_velocity_color_view_ bool false Use Constant Color or not property_velocity_color_ QColor Qt::black Color of Velocity property property_vel_max_ float 3.0 Max velocity [m/s] DrivableArea # Name Type Default Value Description color_scheme_property_ int 0 Color scheme of DrivableArea property alpha_property_ float 0.2 Alpha of DrivableArea property draw_under_property_ bool false Draw as background or not PathFootprint # Name Type Default Value Description property_path_footprint_view_ bool true Use Path Footprint property or not property_path_footprint_alpha_ float 1.0 Alpha of Path Footprint property property_path_footprint_color_ QColor Qt::black Color of Path Footprint property property_vehicle_length_ float 4.77 Vehicle length [m] property_vehicle_width_ float 1.83 Vehicle width [m] property_rear_overhang_ float 1.03 Rear overhang [m] Trajectory # Name Type Default Value Description property_path_view_ bool true Use Path property or not property_path_width_ float 2.0 Width of Path property [m] property_path_alpha_ float 1.0 Alpha of Path property property_path_color_view_ bool false Use Constant Color or not property_path_color_ QColor Qt::black Color of Path property property_velocity_view_ bool true Use Velocity property or not property_velocity_alpha_ float 1.0 Alpha of Velocity property property_velocity_scale_ float 0.3 Scale of Velocity property property_velocity_color_view_ bool false Use Constant Color or not property_velocity_color_ QColor Qt::black Color of Velocity property property_velocity_text_view_ bool false View text Velocity property_velocity_text_scale_ float 0.3 Scale of Velocity property property_vel_max_ float 3.0 Max velocity [m/s] TrajectoryFootprint # Name Type Default Value Description property_trajectory_footprint_view_ bool true Use Trajectory Footprint property or not property_trajectory_footprint_alpha_ float 1.0 Alpha of Trajectory Footprint property property_trajectory_footprint_color_ QColor QColor(230, 230, 50) Color of Trajectory Footprint property property_vehicle_length_ float 4.77 Vehicle length [m] property_vehicle_width_ float 1.83 Vehicle width [m] property_rear_overhang_ float 1.03 Rear overhang [m] property_trajectory_point_view_ bool false Use Trajectory Point property or not property_trajectory_point_alpha_ float 1.0 Alpha of Trajectory Point property property_trajectory_point_color_ QColor QColor(0, 60, 255) Color of Trajectory Point property property_trajectory_point_radius_ float 0.1 Radius of Trajectory Point property MaxVelocity # Name Type Default Value Description property_topic_name_ string /planning/scenario_planning/current_max_velocity The topic on which to subscribe max velocity property_text_color_ QColor QColor(255, 255, 255) Text color property_left_ int 128 Left of the plotter window [px] property_top_ int 128 Top of the plotter window [px] property_length_ int 96 Length of the plotter window [px] property_value_scale_ float 1.0 / 4.0 Value scale Usage # Start rviz and select Add under the Displays panel. Select any one of the tier4_planning_rviz_plugin and press OK. Enter the name of the topic where you want to view the path or trajectory.","title":"tier4_planning_rviz_plugin"},{"location":"common/tier4_planning_rviz_plugin/#tier4_planning_rviz_plugin","text":"This package is including jsk code. Note that jsk_overlay_utils.cpp and jsk_overlay_utils.hpp are BSD license.","title":"tier4_planning_rviz_plugin"},{"location":"common/tier4_planning_rviz_plugin/#purpose","text":"This plugin displays the path, trajectory, and maximum speed.","title":"Purpose"},{"location":"common/tier4_planning_rviz_plugin/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"common/tier4_planning_rviz_plugin/#input","text":"Name Type Description /input/path autoware_auto_planning_msgs::msg::Path The topic on which to subscribe path /input/trajectory autoware_auto_planning_msgs::msg::Trajectory The topic on which to subscribe trajectory /planning/scenario_planning/current_max_velocity tier4_planning_msgs/msg/VelocityLimit The topic on which to publish max velocity","title":"Input"},{"location":"common/tier4_planning_rviz_plugin/#output","text":"Name Type Description /planning/mission_planning/checkpoint geometry_msgs/msg/PoseStamped The topic on which to publish checkpoint","title":"Output"},{"location":"common/tier4_planning_rviz_plugin/#parameter","text":"","title":"Parameter"},{"location":"common/tier4_planning_rviz_plugin/#core-parameters","text":"","title":"Core Parameters"},{"location":"common/tier4_planning_rviz_plugin/#missioncheckpoint","text":"Name Type Default Value Description pose_topic_property_ string mission_checkpoint The topic on which to publish checkpoint std_dev_x_ float 0.5 X standard deviation for checkpoint pose [m] std_dev_y_ float 0.5 Y standard deviation for checkpoint pose [m] std_dev_theta_ float M_PI / 12.0 Theta standard deviation for checkpoint pose [rad] position_z_ float 0.0 Z position for checkpoint pose [m]","title":"MissionCheckpoint"},{"location":"common/tier4_planning_rviz_plugin/#path","text":"Name Type Default Value Description property_path_view_ bool true Use Path property or not property_path_width_ float 2.0 Width of Path property [m] property_path_alpha_ float 1.0 Alpha of Path property property_path_color_view_ bool false Use Constant Color or not property_path_color_ QColor Qt::black Color of Path property property_velocity_view_ bool true Use Velocity property or not property_velocity_alpha_ float 1.0 Alpha of Velocity property property_velocity_scale_ float 0.3 Scale of Velocity property property_velocity_color_view_ bool false Use Constant Color or not property_velocity_color_ QColor Qt::black Color of Velocity property property_vel_max_ float 3.0 Max velocity [m/s]","title":"Path"},{"location":"common/tier4_planning_rviz_plugin/#drivablearea","text":"Name Type Default Value Description color_scheme_property_ int 0 Color scheme of DrivableArea property alpha_property_ float 0.2 Alpha of DrivableArea property draw_under_property_ bool false Draw as background or not","title":"DrivableArea"},{"location":"common/tier4_planning_rviz_plugin/#pathfootprint","text":"Name Type Default Value Description property_path_footprint_view_ bool true Use Path Footprint property or not property_path_footprint_alpha_ float 1.0 Alpha of Path Footprint property property_path_footprint_color_ QColor Qt::black Color of Path Footprint property property_vehicle_length_ float 4.77 Vehicle length [m] property_vehicle_width_ float 1.83 Vehicle width [m] property_rear_overhang_ float 1.03 Rear overhang [m]","title":"PathFootprint"},{"location":"common/tier4_planning_rviz_plugin/#trajectory","text":"Name Type Default Value Description property_path_view_ bool true Use Path property or not property_path_width_ float 2.0 Width of Path property [m] property_path_alpha_ float 1.0 Alpha of Path property property_path_color_view_ bool false Use Constant Color or not property_path_color_ QColor Qt::black Color of Path property property_velocity_view_ bool true Use Velocity property or not property_velocity_alpha_ float 1.0 Alpha of Velocity property property_velocity_scale_ float 0.3 Scale of Velocity property property_velocity_color_view_ bool false Use Constant Color or not property_velocity_color_ QColor Qt::black Color of Velocity property property_velocity_text_view_ bool false View text Velocity property_velocity_text_scale_ float 0.3 Scale of Velocity property property_vel_max_ float 3.0 Max velocity [m/s]","title":"Trajectory"},{"location":"common/tier4_planning_rviz_plugin/#trajectoryfootprint","text":"Name Type Default Value Description property_trajectory_footprint_view_ bool true Use Trajectory Footprint property or not property_trajectory_footprint_alpha_ float 1.0 Alpha of Trajectory Footprint property property_trajectory_footprint_color_ QColor QColor(230, 230, 50) Color of Trajectory Footprint property property_vehicle_length_ float 4.77 Vehicle length [m] property_vehicle_width_ float 1.83 Vehicle width [m] property_rear_overhang_ float 1.03 Rear overhang [m] property_trajectory_point_view_ bool false Use Trajectory Point property or not property_trajectory_point_alpha_ float 1.0 Alpha of Trajectory Point property property_trajectory_point_color_ QColor QColor(0, 60, 255) Color of Trajectory Point property property_trajectory_point_radius_ float 0.1 Radius of Trajectory Point property","title":"TrajectoryFootprint"},{"location":"common/tier4_planning_rviz_plugin/#maxvelocity","text":"Name Type Default Value Description property_topic_name_ string /planning/scenario_planning/current_max_velocity The topic on which to subscribe max velocity property_text_color_ QColor QColor(255, 255, 255) Text color property_left_ int 128 Left of the plotter window [px] property_top_ int 128 Top of the plotter window [px] property_length_ int 96 Length of the plotter window [px] property_value_scale_ float 1.0 / 4.0 Value scale","title":"MaxVelocity"},{"location":"common/tier4_planning_rviz_plugin/#usage","text":"Start rviz and select Add under the Displays panel. Select any one of the tier4_planning_rviz_plugin and press OK. Enter the name of the topic where you want to view the path or trajectory.","title":"Usage"},{"location":"common/tier4_simulated_clock_rviz_plugin/","text":"tier4_simulated_clock_rviz_plugin # Purpose # This plugin allows publishing and controlling the simulated ROS time. Output # Name Type Description /clock rosgraph_msgs::msg::Clock the current simulated time HowToUse # Start rviz and select panels/Add new panel. Select tier4_clock_rviz_plugin/SimulatedClock and press OK. Use the added panel to control how the simulated clock is published. Pause button: pause/resume the clock. Speed: speed of the clock relative to the system clock. Rate: publishing rate of the clock. Step button: advance the clock by the specified time step. Time step: value used to advance the clock when pressing the step button d). Time unit: time unit associated with the value from e).","title":"tier4_simulated_clock_rviz_plugin"},{"location":"common/tier4_simulated_clock_rviz_plugin/#tier4_simulated_clock_rviz_plugin","text":"","title":"tier4_simulated_clock_rviz_plugin"},{"location":"common/tier4_simulated_clock_rviz_plugin/#purpose","text":"This plugin allows publishing and controlling the simulated ROS time.","title":"Purpose"},{"location":"common/tier4_simulated_clock_rviz_plugin/#output","text":"Name Type Description /clock rosgraph_msgs::msg::Clock the current simulated time","title":"Output"},{"location":"common/tier4_simulated_clock_rviz_plugin/#howtouse","text":"Start rviz and select panels/Add new panel. Select tier4_clock_rviz_plugin/SimulatedClock and press OK. Use the added panel to control how the simulated clock is published. Pause button: pause/resume the clock. Speed: speed of the clock relative to the system clock. Rate: publishing rate of the clock. Step button: advance the clock by the specified time step. Time step: value used to advance the clock when pressing the step button d). Time unit: time unit associated with the value from e).","title":"HowToUse"},{"location":"common/tier4_state_rviz_plugin/","text":"tier4_state_rviz_plugin # Purpose # This plugin displays the current status of autoware. This plugin also can engage from the panel. Inputs / Outputs # Input # Name Type Description /control/current_gate_mode tier4_control_msgs::msg::GateMode The topic represents the state of AUTO or EXTERNAL /autoware/state autoware_auto_system_msgs::msg::AutowareState The topic represents the state of Autoware /vehicle/status/gear_status autoware_auto_vehicle_msgs::msg::GearReport The topic represents the state of Gear /api/external/get/engage tier4_external_api_msgs::msg::EngageStatus The topic represents the state of Engage Output # Name Type Description /api/external/set/engage tier4_external_api_msgs::srv::Engage The service inputs engage true HowToUse # Start rviz and select panels/Add new panel. Select tier4_state_rviz_plugin/AutowareStatePanel and press OK. If the AutowareState is WaitingForEngage, can engage by clicking the Engage button.","title":"tier4_state_rviz_plugin"},{"location":"common/tier4_state_rviz_plugin/#tier4_state_rviz_plugin","text":"","title":"tier4_state_rviz_plugin"},{"location":"common/tier4_state_rviz_plugin/#purpose","text":"This plugin displays the current status of autoware. This plugin also can engage from the panel.","title":"Purpose"},{"location":"common/tier4_state_rviz_plugin/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"common/tier4_state_rviz_plugin/#input","text":"Name Type Description /control/current_gate_mode tier4_control_msgs::msg::GateMode The topic represents the state of AUTO or EXTERNAL /autoware/state autoware_auto_system_msgs::msg::AutowareState The topic represents the state of Autoware /vehicle/status/gear_status autoware_auto_vehicle_msgs::msg::GearReport The topic represents the state of Gear /api/external/get/engage tier4_external_api_msgs::msg::EngageStatus The topic represents the state of Engage","title":"Input"},{"location":"common/tier4_state_rviz_plugin/#output","text":"Name Type Description /api/external/set/engage tier4_external_api_msgs::srv::Engage The service inputs engage true","title":"Output"},{"location":"common/tier4_state_rviz_plugin/#howtouse","text":"Start rviz and select panels/Add new panel. Select tier4_state_rviz_plugin/AutowareStatePanel and press OK. If the AutowareState is WaitingForEngage, can engage by clicking the Engage button.","title":"HowToUse"},{"location":"common/tier4_traffic_light_rviz_plugin/","text":"tier4_traffic_light_rviz_plugin # Purpose # This plugin panel publishes dummy traffic light signals. Inputs / Outputs # Output # Name Type Description /perception/traffic_light_recognition/traffic_signals autoware_auto_perception_msgs::msg::TrafficSignalArray Publish traffic light signals HowToUse # Start rviz and select panels/Add new panel. Select TrafficLightPublishPanel and press OK. Set Traffic Light ID & Traffic Light Status and press SET button. Traffic light signals are published, while PUBLISH button is pushed.","title":"tier4_traffic_light_rviz_plugin"},{"location":"common/tier4_traffic_light_rviz_plugin/#tier4_traffic_light_rviz_plugin","text":"","title":"tier4_traffic_light_rviz_plugin"},{"location":"common/tier4_traffic_light_rviz_plugin/#purpose","text":"This plugin panel publishes dummy traffic light signals.","title":"Purpose"},{"location":"common/tier4_traffic_light_rviz_plugin/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"common/tier4_traffic_light_rviz_plugin/#output","text":"Name Type Description /perception/traffic_light_recognition/traffic_signals autoware_auto_perception_msgs::msg::TrafficSignalArray Publish traffic light signals","title":"Output"},{"location":"common/tier4_traffic_light_rviz_plugin/#howtouse","text":"Start rviz and select panels/Add new panel. Select TrafficLightPublishPanel and press OK. Set Traffic Light ID & Traffic Light Status and press SET button. Traffic light signals are published, while PUBLISH button is pushed.","title":"HowToUse"},{"location":"common/tier4_vehicle_rviz_plugin/","text":"tier4_vehicle_rviz_plugin # This package is including jsk code. Note that jsk_overlay_utils.cpp and jsk_overlay_utils.hpp are BSD license. Purpose # This plugin provides a visual and easy-to-understand display of vehicle speed, turn signal and steering status. Inputs / Outputs # Input # Name Type Description /vehicle/status/velocity_status autoware_auto_vehicle_msgs::msg::VelocityReport The topic is vehicle twist /control/turn_signal_cmd autoware_auto_vehicle_msgs::msg::TurnIndicatorsReport The topic is status of turn signal /vehicle/status/steering_status autoware_auto_vehicle_msgs::msg::SteeringReport The topic is status of steering Parameter # Core Parameters # ConsoleMeter # Name Type Default Value Description property_text_color_ QColor QColor(25, 255, 240) Text color property_left_ int 128 Left of the plotter window [px] property_top_ int 128 Top of the plotter window [px] property_length_ int 256 Height of the plotter window [px] property_value_height_offset_ int 0 Height offset of the plotter window [px] property_value_scale_ float 1.0 / 6.667 Value scale SteeringAngle # Name Type Default Value Description property_text_color_ QColor QColor(25, 255, 240) Text color property_left_ int 128 Left of the plotter window [px] property_top_ int 128 Top of the plotter window [px] property_length_ int 256 Height of the plotter window [px] property_value_height_offset_ int 0 Height offset of the plotter window [px] property_value_scale_ float 1.0 / 6.667 Value scale property_handle_angle_scale_ float 3.0 Scale is steering angle to handle angle TurnSignal # Name Type Default Value Description property_left_ int 128 Left of the plotter window [px] property_top_ int 128 Top of the plotter window [px] property_width_ int 256 Left of the plotter window [px] property_height_ int 256 Width of the plotter window [px] VelocityHistory # Name Type Default Value Description property_velocity_timeout_ float 10.0 Timeout of velocity [s] property_velocity_alpha_ float 1.0 Alpha of velocity property_velocity_scale_ float 0.3 Scale of velocity property_velocity_color_view_ bool false Use Constant Color or not property_velocity_color_ QColor Qt::black Color of velocity history property_vel_max_ float 3.0 Color Border Vel Max [m/s] Assumptions / Known limits # TBD. Usage # Start rviz and select Add under the Displays panel. Select any one of the tier4_vehicle_rviz_plugin and press OK. Enter the name of the topic where you want to view the status.","title":"tier4_vehicle_rviz_plugin"},{"location":"common/tier4_vehicle_rviz_plugin/#tier4_vehicle_rviz_plugin","text":"This package is including jsk code. Note that jsk_overlay_utils.cpp and jsk_overlay_utils.hpp are BSD license.","title":"tier4_vehicle_rviz_plugin"},{"location":"common/tier4_vehicle_rviz_plugin/#purpose","text":"This plugin provides a visual and easy-to-understand display of vehicle speed, turn signal and steering status.","title":"Purpose"},{"location":"common/tier4_vehicle_rviz_plugin/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"common/tier4_vehicle_rviz_plugin/#input","text":"Name Type Description /vehicle/status/velocity_status autoware_auto_vehicle_msgs::msg::VelocityReport The topic is vehicle twist /control/turn_signal_cmd autoware_auto_vehicle_msgs::msg::TurnIndicatorsReport The topic is status of turn signal /vehicle/status/steering_status autoware_auto_vehicle_msgs::msg::SteeringReport The topic is status of steering","title":"Input"},{"location":"common/tier4_vehicle_rviz_plugin/#parameter","text":"","title":"Parameter"},{"location":"common/tier4_vehicle_rviz_plugin/#core-parameters","text":"","title":"Core Parameters"},{"location":"common/tier4_vehicle_rviz_plugin/#consolemeter","text":"Name Type Default Value Description property_text_color_ QColor QColor(25, 255, 240) Text color property_left_ int 128 Left of the plotter window [px] property_top_ int 128 Top of the plotter window [px] property_length_ int 256 Height of the plotter window [px] property_value_height_offset_ int 0 Height offset of the plotter window [px] property_value_scale_ float 1.0 / 6.667 Value scale","title":"ConsoleMeter"},{"location":"common/tier4_vehicle_rviz_plugin/#steeringangle","text":"Name Type Default Value Description property_text_color_ QColor QColor(25, 255, 240) Text color property_left_ int 128 Left of the plotter window [px] property_top_ int 128 Top of the plotter window [px] property_length_ int 256 Height of the plotter window [px] property_value_height_offset_ int 0 Height offset of the plotter window [px] property_value_scale_ float 1.0 / 6.667 Value scale property_handle_angle_scale_ float 3.0 Scale is steering angle to handle angle","title":"SteeringAngle"},{"location":"common/tier4_vehicle_rviz_plugin/#turnsignal","text":"Name Type Default Value Description property_left_ int 128 Left of the plotter window [px] property_top_ int 128 Top of the plotter window [px] property_width_ int 256 Left of the plotter window [px] property_height_ int 256 Width of the plotter window [px]","title":"TurnSignal"},{"location":"common/tier4_vehicle_rviz_plugin/#velocityhistory","text":"Name Type Default Value Description property_velocity_timeout_ float 10.0 Timeout of velocity [s] property_velocity_alpha_ float 1.0 Alpha of velocity property_velocity_scale_ float 0.3 Scale of velocity property_velocity_color_view_ bool false Use Constant Color or not property_velocity_color_ QColor Qt::black Color of velocity history property_vel_max_ float 3.0 Color Border Vel Max [m/s]","title":"VelocityHistory"},{"location":"common/tier4_vehicle_rviz_plugin/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"common/tier4_vehicle_rviz_plugin/#usage","text":"Start rviz and select Add under the Displays panel. Select any one of the tier4_vehicle_rviz_plugin and press OK. Enter the name of the topic where you want to view the status.","title":"Usage"},{"location":"common/vehicle_constants_manager/design/vehicle_constants_manager-design/","text":"vehicle_constants_manager # This is the design document for the vehicle_constants_manager package. Purpose / Use cases # This library provides a struct for holding vehicle specific constants. It also provides a helper method to declare vehicle specific constants which have already been passed into a node and provide a VehicleConstants object. Design # Provided VehicleConstants struct holds vehicle parameters. Its parameters can be split in 2 categories: (The detailed descriptions and units of the variables is in the vehicle_constants_manager.hpp file.) Primary Constants wheel_radius wheel_width wheel_base wheel_tread overhang_front overhang_rear overhang_left overhang_right vehicle_height cg_to_rear tire_cornering_stiffness_front_n_per_deg tire_cornering_stiffness_rear_n_per_deg mass_vehicle inertia_yaw_kg_m_2 Derived Constants cg_to_front vehicle_length vehicle_width offset_longitudinal_min offset_longitudinal_max offset_lateral_min offset_lateral_max offset_height_min offset_height_max The VehicleConstants constructor is initialized with the primary parameters. The library also provides a declare_and_get_vehicle_constants method. Using this method, the user can declare vehicle parameters that are already passed into the node and obtain a VehicleConstants object. Assumptions / Known limits # This library assumes the vehicle is defined with Ackermann steering geometry. declare_and_get_vehicle_constants method requires the passed node to have following parameters overridden: (Pay attention to the vehicle namespace) vehicle : wheel_radius : wheel_width : wheel_base : wheel_tread : overhang_front : overhang_rear : overhang_left : overhang_right : vehicle_height : cg_to_rear : tire_cornering_stiffness_front_n_per_deg : tire_cornering_stiffness_rear_n_per_deg : mass_vehicle : inertia_yaw_kg_m_2 : Inputs / Outputs / API # The constructor of VehicleConstants takes the primary vehicle constants and generates the derived parameters. declare_and_get_vehicle_constants method takes a rclcpp::Node object. And returns a VehicleConstants object if it succeeds. Example usage: // In the constructor of a node which received primary vehicle parameters from a // .yaml file or run args. auto vehicle_constants = declare_and_get_vehicle_constants ( * this ); Inner-workings / Algorithms # Not Available. Error detection and handling # The VehicleConstants struct performs some sanity checks upon construction. It will throw std::runtime_error in case certain parameters are negative or cg_to_rear is larger than wheel_base (to ensure center of gravity is within front and rear axles.) Security considerations # To Be Determined. References / External links # Not Available. Future extensions / Unimplemented parts # Not Available. Related issues # https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1294","title":"vehicle_constants_manager"},{"location":"common/vehicle_constants_manager/design/vehicle_constants_manager-design/#vehicle_constants_manager","text":"This is the design document for the vehicle_constants_manager package.","title":"vehicle_constants_manager"},{"location":"common/vehicle_constants_manager/design/vehicle_constants_manager-design/#purpose-use-cases","text":"This library provides a struct for holding vehicle specific constants. It also provides a helper method to declare vehicle specific constants which have already been passed into a node and provide a VehicleConstants object.","title":"Purpose / Use cases"},{"location":"common/vehicle_constants_manager/design/vehicle_constants_manager-design/#design","text":"Provided VehicleConstants struct holds vehicle parameters. Its parameters can be split in 2 categories: (The detailed descriptions and units of the variables is in the vehicle_constants_manager.hpp file.) Primary Constants wheel_radius wheel_width wheel_base wheel_tread overhang_front overhang_rear overhang_left overhang_right vehicle_height cg_to_rear tire_cornering_stiffness_front_n_per_deg tire_cornering_stiffness_rear_n_per_deg mass_vehicle inertia_yaw_kg_m_2 Derived Constants cg_to_front vehicle_length vehicle_width offset_longitudinal_min offset_longitudinal_max offset_lateral_min offset_lateral_max offset_height_min offset_height_max The VehicleConstants constructor is initialized with the primary parameters. The library also provides a declare_and_get_vehicle_constants method. Using this method, the user can declare vehicle parameters that are already passed into the node and obtain a VehicleConstants object.","title":"Design"},{"location":"common/vehicle_constants_manager/design/vehicle_constants_manager-design/#assumptions-known-limits","text":"This library assumes the vehicle is defined with Ackermann steering geometry. declare_and_get_vehicle_constants method requires the passed node to have following parameters overridden: (Pay attention to the vehicle namespace) vehicle : wheel_radius : wheel_width : wheel_base : wheel_tread : overhang_front : overhang_rear : overhang_left : overhang_right : vehicle_height : cg_to_rear : tire_cornering_stiffness_front_n_per_deg : tire_cornering_stiffness_rear_n_per_deg : mass_vehicle : inertia_yaw_kg_m_2 :","title":"Assumptions / Known limits"},{"location":"common/vehicle_constants_manager/design/vehicle_constants_manager-design/#inputs-outputs-api","text":"The constructor of VehicleConstants takes the primary vehicle constants and generates the derived parameters. declare_and_get_vehicle_constants method takes a rclcpp::Node object. And returns a VehicleConstants object if it succeeds. Example usage: // In the constructor of a node which received primary vehicle parameters from a // .yaml file or run args. auto vehicle_constants = declare_and_get_vehicle_constants ( * this );","title":"Inputs / Outputs / API"},{"location":"common/vehicle_constants_manager/design/vehicle_constants_manager-design/#inner-workings-algorithms","text":"Not Available.","title":"Inner-workings / Algorithms"},{"location":"common/vehicle_constants_manager/design/vehicle_constants_manager-design/#error-detection-and-handling","text":"The VehicleConstants struct performs some sanity checks upon construction. It will throw std::runtime_error in case certain parameters are negative or cg_to_rear is larger than wheel_base (to ensure center of gravity is within front and rear axles.)","title":"Error detection and handling"},{"location":"common/vehicle_constants_manager/design/vehicle_constants_manager-design/#security-considerations","text":"To Be Determined.","title":"Security considerations"},{"location":"common/vehicle_constants_manager/design/vehicle_constants_manager-design/#references-external-links","text":"Not Available.","title":"References / External links"},{"location":"common/vehicle_constants_manager/design/vehicle_constants_manager-design/#future-extensions-unimplemented-parts","text":"Not Available.","title":"Future extensions / Unimplemented parts"},{"location":"common/vehicle_constants_manager/design/vehicle_constants_manager-design/#related-issues","text":"https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1294","title":"Related issues"},{"location":"common/web_controller/","text":"web_controller # Purpose # This packages is for visualizing the status of Autoware and sending topics for Autoware from a web page. Inputs / Outputs # Input # Name Type Description /control/current_gate_mode tier4_control_msgs::msg::GateMode Gate mode (AUTO or EXTERNAL) /autoware/state autoware_auto_system_msgs::msg::AutowareState State of Autoware /autoware/engage autoware_auto_system_msgs::msg::Engage Engage signal for Autoware /vehicle/engage autoware_auto_system_msgs::msg::Engage Engage signal for a vehicle /planning/scenario_planning/max_velocity_default tier4_planning_msgs::msg::VelocityLimit Max velocity of Autoware Output # Name Type Description /planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner/path_change_approval tier4_planning_msgs::msg::Approval Send an approval signal for path change request such as lane change or obstacle avoidance /autoware/engage autoware_auto_system_msgs::msg::Engage Send an engage signal for Autoware /vehicle/engage autoware_auto_system_msgs::msg::Engage Send an engage signal for a vehicle /planning/scenario_planning/max_velocity_default tier4_planning_msgs::msg::VelocityLimit Set a max velocity of Autoware Parameter # Core Parameters # None Assumptions / Known limits # web_controller needs rosbridge which is automatically launched in tier4_autoware_api_launch along with launching Autoware. Usage # Access to http://localhost:8085/web_controller after launching Autoware. Check the status of Autoware or send topics by the buttons.","title":"web_controller"},{"location":"common/web_controller/#web_controller","text":"","title":"web_controller"},{"location":"common/web_controller/#purpose","text":"This packages is for visualizing the status of Autoware and sending topics for Autoware from a web page.","title":"Purpose"},{"location":"common/web_controller/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"common/web_controller/#input","text":"Name Type Description /control/current_gate_mode tier4_control_msgs::msg::GateMode Gate mode (AUTO or EXTERNAL) /autoware/state autoware_auto_system_msgs::msg::AutowareState State of Autoware /autoware/engage autoware_auto_system_msgs::msg::Engage Engage signal for Autoware /vehicle/engage autoware_auto_system_msgs::msg::Engage Engage signal for a vehicle /planning/scenario_planning/max_velocity_default tier4_planning_msgs::msg::VelocityLimit Max velocity of Autoware","title":"Input"},{"location":"common/web_controller/#output","text":"Name Type Description /planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner/path_change_approval tier4_planning_msgs::msg::Approval Send an approval signal for path change request such as lane change or obstacle avoidance /autoware/engage autoware_auto_system_msgs::msg::Engage Send an engage signal for Autoware /vehicle/engage autoware_auto_system_msgs::msg::Engage Send an engage signal for a vehicle /planning/scenario_planning/max_velocity_default tier4_planning_msgs::msg::VelocityLimit Set a max velocity of Autoware","title":"Output"},{"location":"common/web_controller/#parameter","text":"","title":"Parameter"},{"location":"common/web_controller/#core-parameters","text":"None","title":"Core Parameters"},{"location":"common/web_controller/#assumptions-known-limits","text":"web_controller needs rosbridge which is automatically launched in tier4_autoware_api_launch along with launching Autoware.","title":"Assumptions / Known limits"},{"location":"common/web_controller/#usage","text":"Access to http://localhost:8085/web_controller after launching Autoware. Check the status of Autoware or send topics by the buttons.","title":"Usage"},{"location":"control/control_performance_analysis/","text":"control_performance_analysis # Purpose # control_performance_analysis is the package to analyze the tracking performance of a control module and monitor the driving status of the vehicle. This package is used as a tool to quantify the results of the control module. That's why it doesn't interfere with the core logic of autonomous driving. Based on the various input from planning, control, and vehicle, it publishes the result of analysis as control_performance_analysis::msg::ErrorStamped defined in this package. All results in ErrorStamped message are calculated in Frenet Frame of curve. Errors and velocity errors are calculated by using paper below. Werling, Moritz & Groell, Lutz & Bretthauer, Georg. (2010). Invariant Trajectory Tracking With a Full-Size Autonomous Road Vehicle. Robotics, IEEE Transactions on. 26. 758 - 765. 10.1109/TRO.2010.2052325. If you are interested in calculations, you can see the error and error velocity calculations in section C. Asymptotical Trajectory Tracking With Orientation Control . Error acceleration calculations are made based on the velocity calculations above. You can see below the calculation of error acceleration. Input / Output # Input topics # Name Type Description /planning/scenario_planning/trajectory autoware_auto_planning_msgs::msg::Trajectory Output trajectory from planning module. /control/command/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand Output control command from control module. /vehicle/status/steering_status autoware_auto_vehicle_msgs::msg::SteeringReport Steering information from vehicle. /localization/kinematic_state nav_msgs::msg::Odometry Use twist from odometry. /tf tf2_msgs::msg::TFMessage Extract ego pose from tf. Output topics # Name Type Description /control_performance/performance_vars control_performance_analysis::msg::ErrorStamped The result of the performance analysis. /control_performance/driving_status control_performance_analysis::msg::DrivingMonitorStamped Driving status (acceleration, jerk etc.) monitoring Outputs # control_performance_analysis::msg::DrivingMonitorStamped # Name Type Description longitudinal_acceleration float [m / s^2] longitudinal_jerk float [m / s^3] lateral_acceleration float [m / s^2] lateral_jerk float [m / s^3] desired_steering_angle float [rad] controller_processing_time float Timestamp between last two control command messages [ms] control_performance_analysis::msg::ErrorStamped # Name Type Description lateral_error float [m] lateral_error_velocity float [m / s] lateral_error_acceleration float [m / s^2] longitudinal_error float [m] longitudinal_error_velocity float [m / s] longitudinal_error_acceleration float [m / s^2] heading_error float [rad] heading_error_velocity float [rad / s] control_effort_energy float [u * R * u^T] error_energy float lateral_error^2 + heading_error^2 value_approximation float V = xPx' ; Value function from DARE Lyap matrix P curvature_estimate float [1 / m] curvature_estimate_pp float [1 / m] vehicle_velocity_error float [m / s] tracking_curvature_discontinuity_ability float Measures the ability to tracking the curvature changes [ abs(delta(curvature)) / (1 + abs(delta(lateral_error)) ] Parameters # Name Type Description curvature_interval_length double Used for estimating current curvature prevent_zero_division_value double Value to avoid zero division. Default is 0.001 odom_interval unsigned integer Interval between odom messages, increase it for smoother curve. acceptable_min_waypoint_distance double How far the next waypoint can be ahead of the vehicle direction Usage # After launched simulation and control module, launch the control_performance_analysis.launch.xml . You should be able to see the driving monitor and error variables in topics. If you want to visualize the results, you can use Plotjuggler and use config/controller_monitor.xml as layout. After import the layout, please specify the topics that are listed below. /localization/kinematic_state /vehicle/status/steering_status /control_performance/driving_status /control_performance/performance_vars In Plotjuggler you can export the statistic (max, min, average) values as csv file. Use that statistics to compare the control modules.","title":"control_performance_analysis"},{"location":"control/control_performance_analysis/#control_performance_analysis","text":"","title":"control_performance_analysis"},{"location":"control/control_performance_analysis/#purpose","text":"control_performance_analysis is the package to analyze the tracking performance of a control module and monitor the driving status of the vehicle. This package is used as a tool to quantify the results of the control module. That's why it doesn't interfere with the core logic of autonomous driving. Based on the various input from planning, control, and vehicle, it publishes the result of analysis as control_performance_analysis::msg::ErrorStamped defined in this package. All results in ErrorStamped message are calculated in Frenet Frame of curve. Errors and velocity errors are calculated by using paper below. Werling, Moritz & Groell, Lutz & Bretthauer, Georg. (2010). Invariant Trajectory Tracking With a Full-Size Autonomous Road Vehicle. Robotics, IEEE Transactions on. 26. 758 - 765. 10.1109/TRO.2010.2052325. If you are interested in calculations, you can see the error and error velocity calculations in section C. Asymptotical Trajectory Tracking With Orientation Control . Error acceleration calculations are made based on the velocity calculations above. You can see below the calculation of error acceleration.","title":"Purpose"},{"location":"control/control_performance_analysis/#input-output","text":"","title":"Input / Output"},{"location":"control/control_performance_analysis/#input-topics","text":"Name Type Description /planning/scenario_planning/trajectory autoware_auto_planning_msgs::msg::Trajectory Output trajectory from planning module. /control/command/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand Output control command from control module. /vehicle/status/steering_status autoware_auto_vehicle_msgs::msg::SteeringReport Steering information from vehicle. /localization/kinematic_state nav_msgs::msg::Odometry Use twist from odometry. /tf tf2_msgs::msg::TFMessage Extract ego pose from tf.","title":"Input topics"},{"location":"control/control_performance_analysis/#output-topics","text":"Name Type Description /control_performance/performance_vars control_performance_analysis::msg::ErrorStamped The result of the performance analysis. /control_performance/driving_status control_performance_analysis::msg::DrivingMonitorStamped Driving status (acceleration, jerk etc.) monitoring","title":"Output topics"},{"location":"control/control_performance_analysis/#outputs","text":"","title":"Outputs"},{"location":"control/control_performance_analysis/#control_performance_analysismsgdrivingmonitorstamped","text":"Name Type Description longitudinal_acceleration float [m / s^2] longitudinal_jerk float [m / s^3] lateral_acceleration float [m / s^2] lateral_jerk float [m / s^3] desired_steering_angle float [rad] controller_processing_time float Timestamp between last two control command messages [ms]","title":"control_performance_analysis::msg::DrivingMonitorStamped"},{"location":"control/control_performance_analysis/#control_performance_analysismsgerrorstamped","text":"Name Type Description lateral_error float [m] lateral_error_velocity float [m / s] lateral_error_acceleration float [m / s^2] longitudinal_error float [m] longitudinal_error_velocity float [m / s] longitudinal_error_acceleration float [m / s^2] heading_error float [rad] heading_error_velocity float [rad / s] control_effort_energy float [u * R * u^T] error_energy float lateral_error^2 + heading_error^2 value_approximation float V = xPx' ; Value function from DARE Lyap matrix P curvature_estimate float [1 / m] curvature_estimate_pp float [1 / m] vehicle_velocity_error float [m / s] tracking_curvature_discontinuity_ability float Measures the ability to tracking the curvature changes [ abs(delta(curvature)) / (1 + abs(delta(lateral_error)) ]","title":"control_performance_analysis::msg::ErrorStamped"},{"location":"control/control_performance_analysis/#parameters","text":"Name Type Description curvature_interval_length double Used for estimating current curvature prevent_zero_division_value double Value to avoid zero division. Default is 0.001 odom_interval unsigned integer Interval between odom messages, increase it for smoother curve. acceptable_min_waypoint_distance double How far the next waypoint can be ahead of the vehicle direction","title":"Parameters"},{"location":"control/control_performance_analysis/#usage","text":"After launched simulation and control module, launch the control_performance_analysis.launch.xml . You should be able to see the driving monitor and error variables in topics. If you want to visualize the results, you can use Plotjuggler and use config/controller_monitor.xml as layout. After import the layout, please specify the topics that are listed below. /localization/kinematic_state /vehicle/status/steering_status /control_performance/driving_status /control_performance/performance_vars In Plotjuggler you can export the statistic (max, min, average) values as csv file. Use that statistics to compare the control modules.","title":"Usage"},{"location":"control/external_cmd_selector/","text":"external_cmd_selector # Purpose # external_cmd_selector is the package to publish external_control_cmd , gear_cmd , hazard_lights_cmd , heartbeat and turn_indicators_cmd , according to the current mode, which is remote or local . The current mode is set via service, remote is remotely operated, local is to use the values calculated by Autoware. Input / Output # Input topics # Name Type Description /api/external/set/command/local/control TBD Local. Calculated control value. /api/external/set/command/local/heartbeat TBD Local. Heartbeat. /api/external/set/command/local/shift TBD Local. Gear shift like drive, rear and etc. /api/external/set/command/local/turn_signal TBD Local. Turn signal like left turn, right turn and etc. /api/external/set/command/remote/control TBD Remote. Calculated control value. /api/external/set/command/remote/heartbeat TBD Remote. Heartbeat. /api/external/set/command/remote/shift TBD Remote. Gear shift like drive, rear and etc. /api/external/set/command/remote/turn_signal TBD Remote. Turn signal like left turn, right turn and etc. Output topics # Name Type Description /control/external_cmd_selector/current_selector_mode TBD Current selected mode, remote or local. /diagnostics diagnostic_msgs::msg::DiagnosticArray Check if node is active or not. /external/selected/external_control_cmd TBD Pass through control command with current mode. /external/selected/gear_cmd autoware_auto_vehicle_msgs::msg::GearCommand Pass through gear command with current mode. /external/selected/hazard_lights_cmd autoware_auto_vehicle_msgs::msg::HazardLightsCommand Pass through hazard light with current mode. /external/selected/heartbeat TBD Pass through heartbeat with current mode. /external/selected/turn_indicators_cmd autoware_auto_vehicle_msgs::msg::TurnIndicatorsCommand Pass through turn indicator with current mode.","title":"external_cmd_selector"},{"location":"control/external_cmd_selector/#external_cmd_selector","text":"","title":"external_cmd_selector"},{"location":"control/external_cmd_selector/#purpose","text":"external_cmd_selector is the package to publish external_control_cmd , gear_cmd , hazard_lights_cmd , heartbeat and turn_indicators_cmd , according to the current mode, which is remote or local . The current mode is set via service, remote is remotely operated, local is to use the values calculated by Autoware.","title":"Purpose"},{"location":"control/external_cmd_selector/#input-output","text":"","title":"Input / Output"},{"location":"control/external_cmd_selector/#input-topics","text":"Name Type Description /api/external/set/command/local/control TBD Local. Calculated control value. /api/external/set/command/local/heartbeat TBD Local. Heartbeat. /api/external/set/command/local/shift TBD Local. Gear shift like drive, rear and etc. /api/external/set/command/local/turn_signal TBD Local. Turn signal like left turn, right turn and etc. /api/external/set/command/remote/control TBD Remote. Calculated control value. /api/external/set/command/remote/heartbeat TBD Remote. Heartbeat. /api/external/set/command/remote/shift TBD Remote. Gear shift like drive, rear and etc. /api/external/set/command/remote/turn_signal TBD Remote. Turn signal like left turn, right turn and etc.","title":"Input topics"},{"location":"control/external_cmd_selector/#output-topics","text":"Name Type Description /control/external_cmd_selector/current_selector_mode TBD Current selected mode, remote or local. /diagnostics diagnostic_msgs::msg::DiagnosticArray Check if node is active or not. /external/selected/external_control_cmd TBD Pass through control command with current mode. /external/selected/gear_cmd autoware_auto_vehicle_msgs::msg::GearCommand Pass through gear command with current mode. /external/selected/hazard_lights_cmd autoware_auto_vehicle_msgs::msg::HazardLightsCommand Pass through hazard light with current mode. /external/selected/heartbeat TBD Pass through heartbeat with current mode. /external/selected/turn_indicators_cmd autoware_auto_vehicle_msgs::msg::TurnIndicatorsCommand Pass through turn indicator with current mode.","title":"Output topics"},{"location":"control/joy_controller/","text":"joy_controller # Role # joy_controller is the package to convert a joy msg to autoware commands (e.g. steering wheel, shift, turn signal, engage) for a vehicle. Input / Output # Input topics # Name Type Description ~/input/joy sensor_msgs::msg::Joy joy controller command ~/input/odometry nav_msgs::msg::Odometry ego vehicle odometry to get twist Output topics # Name Type Description ~/output/control_command autoware_auto_control_msgs::msg::AckermannControlCommand lateral and longitudinal control command ~/output/external_control_command tier4_external_api_msgs::msg::ControlCommandStamped lateral and longitudinal control command ~/output/shift tier4_external_api_msgs::msg::GearShiftStamped gear command ~/output/turn_signal tier4_external_api_msgs::msg::TurnSignalStamped turn signal command ~/output/gate_mode tier4_control_msgs::msg::GateMode gate mode (Auto or External) ~/output/heartbeat tier4_external_api_msgs::msg::Heartbeat heartbeat ~/output/vehicle_engage autoware_auto_vehicle_msgs::msg::Engage vehicle engage Parameters # Parameter Type Description joy_type string joy controller type (default: DS4) update_rate double update rate to publish control commands accel_ratio double ratio to calculate acceleration (commanded acceleration is ratio * operation) brake_ratio double ratio to calculate deceleration (commanded acceleration is -ratio * operation) steer_ratio double ratio to calculate deceleration (commanded steer is ratio * operation) steering_angle_velocity double steering angle velocity for operation accel_sensitivity double sensitivity to calculate acceleration for external API (commanded acceleration is pow(operation, 1 / sensitivity)) brake_sensitivity double sensitivity to calculate deceleration for external API (commanded acceleration is pow(operation, 1 / sensitivity)) velocity_gain double ratio to calculate velocity by acceleration max_forward_velocity double absolute max velocity to go forward max_backward_velocity double absolute max velocity to go backward backward_accel_ratio double ratio to calculate deceleration (commanded acceleration is -ratio * operation) P65 Joystick Key Map # Acceleration R2 Brake L2 Steering Left Stick Left Right Shift up Cursor Up Shift down Cursor Down Shift Drive Cursor Left Shift Reverse Cursor Right Turn Signal Left L1 Turn Signal Right R1 Clear Turn Signal A Gate Mode B Emergency Stop Select Clear Emergency Stop Start Autoware Engage X Autoware Disengage Y Vehicle Engage PS Vehicle Disengage Right Trigger","title":"joy_controller"},{"location":"control/joy_controller/#joy_controller","text":"","title":"joy_controller"},{"location":"control/joy_controller/#role","text":"joy_controller is the package to convert a joy msg to autoware commands (e.g. steering wheel, shift, turn signal, engage) for a vehicle.","title":"Role"},{"location":"control/joy_controller/#input-output","text":"","title":"Input / Output"},{"location":"control/joy_controller/#input-topics","text":"Name Type Description ~/input/joy sensor_msgs::msg::Joy joy controller command ~/input/odometry nav_msgs::msg::Odometry ego vehicle odometry to get twist","title":"Input topics"},{"location":"control/joy_controller/#output-topics","text":"Name Type Description ~/output/control_command autoware_auto_control_msgs::msg::AckermannControlCommand lateral and longitudinal control command ~/output/external_control_command tier4_external_api_msgs::msg::ControlCommandStamped lateral and longitudinal control command ~/output/shift tier4_external_api_msgs::msg::GearShiftStamped gear command ~/output/turn_signal tier4_external_api_msgs::msg::TurnSignalStamped turn signal command ~/output/gate_mode tier4_control_msgs::msg::GateMode gate mode (Auto or External) ~/output/heartbeat tier4_external_api_msgs::msg::Heartbeat heartbeat ~/output/vehicle_engage autoware_auto_vehicle_msgs::msg::Engage vehicle engage","title":"Output topics"},{"location":"control/joy_controller/#parameters","text":"Parameter Type Description joy_type string joy controller type (default: DS4) update_rate double update rate to publish control commands accel_ratio double ratio to calculate acceleration (commanded acceleration is ratio * operation) brake_ratio double ratio to calculate deceleration (commanded acceleration is -ratio * operation) steer_ratio double ratio to calculate deceleration (commanded steer is ratio * operation) steering_angle_velocity double steering angle velocity for operation accel_sensitivity double sensitivity to calculate acceleration for external API (commanded acceleration is pow(operation, 1 / sensitivity)) brake_sensitivity double sensitivity to calculate deceleration for external API (commanded acceleration is pow(operation, 1 / sensitivity)) velocity_gain double ratio to calculate velocity by acceleration max_forward_velocity double absolute max velocity to go forward max_backward_velocity double absolute max velocity to go backward backward_accel_ratio double ratio to calculate deceleration (commanded acceleration is -ratio * operation)","title":"Parameters"},{"location":"control/joy_controller/#p65-joystick-key-map","text":"Acceleration R2 Brake L2 Steering Left Stick Left Right Shift up Cursor Up Shift down Cursor Down Shift Drive Cursor Left Shift Reverse Cursor Right Turn Signal Left L1 Turn Signal Right R1 Clear Turn Signal A Gate Mode B Emergency Stop Select Clear Emergency Stop Start Autoware Engage X Autoware Disengage Y Vehicle Engage PS Vehicle Disengage Right Trigger","title":"P65 Joystick Key Map"},{"location":"control/lane_departure_checker/","text":"Lane Departure Checker # The Lane Departure Checker checks if vehicle follows a trajectory. If it does not follow the trajectory, it reports its status via diagnostic_updater . Features # This package includes the following features: Lane Departure : Check if ego vehicle is going to be out of lane boundaries based on output from control module (predicted trajectory). Trajectory Deviation : Check if ego vehicle's pose does not deviate from the trajectory. Checking lateral, longitudinal and yaw deviation. Inner-workings / Algorithms # How to extend footprint by covariance # Calculate the standard deviation of error ellipse(covariance) in vehicle coordinate. 1.Transform covariance into vehicle coordinate. \\begin{align} \\left( \\begin{array}{cc} x_{vehicle}\\\\ y_{vehicle}\\\\ \\end{array} \\right) = R_{map2vehicle} \\left( \\begin{array}{cc} x_{map}\\\\ y_{map}\\\\ \\end{array} \\right) \\end{align} Calculate covariance in vehicle coordinate. \\begin{align} Cov_{vehicle} &= E \\left[ \\left( \\begin{array}{cc} x_{vehicle}\\\\ y_{vehicle}\\\\ \\end{array} \\right) (x_{vehicle}, y_{vehicle}) \\right] \\\\ &= E \\left[ R\\left( \\begin{array}{cc} x_{map}\\\\ y_{map}\\\\ \\end{array} \\right) (x_{map}, y_{map})R^t \\right] \\\\ &= R E\\left[ \\left( \\begin{array}{cc} x_{map}\\\\ y_{map}\\\\ \\end{array} \\right) (x_{map}, y_{map}) \\right] R^t \\\\ &= R Cov_{map} R^t \\end{align} 2.The longitudinal length we want to expand is correspond to marginal distribution of x_{vehicle} x_{vehicle} , which is represented in Cov_{vehicle}(0,0) Cov_{vehicle}(0,0) . In the same way, the lateral length is represented in Cov_{vehicle}(1,1) Cov_{vehicle}(1,1) . Wikipedia reference here . Expand footprint based on the standard deviation multiplied with footprint_margin_scale . Interface # Input # /localization/kinematic_state [ nav_msgs::msg::Odometry ] /map/vector_map [ autoware_auto_mapping_msgs::msg::HADMapBin ] /planning/mission_planning/route [ autoware_auto_planning_msgs::msg::HADMapRoute ] /planning/scenario_planning/trajectory [ autoware_auto_planning_msgs::msg::Trajectory ] /control/trajectory_follower/predicted_trajectory [ autoware_auto_planning_msgs::msg::Trajectory ] Output # [ diagnostic_updater ] lane_departure : Update diagnostic level when ego vehicle is out of lane. [ diagnostic_updater ] trajectory_deviation : Update diagnostic level when ego vehicle deviates from trajectory. Parameters # Node Parameters # Name Type Description Default value update_rate double Frequency for publishing [Hz] 10.0 visualize_lanelet bool Flag for visualizing lanelet False Core Parameters # Name Type Description Default value footprint_margin_scale double Coefficient for expanding footprint margin. Multiplied by 1 standard deviation. 1.0 resample_interval double Minimum Euclidean distance between points when resample trajectory.[m] 0.3 max_deceleration double Maximum deceleration when calculating braking distance. 2.8 delay_time double Delay time which took to actuate brake when calculating braking distance. [second] 1.3 max_lateral_deviation double Maximum lateral deviation in vehicle coordinate. [m] 2.0 max_longitudinal_deviation double Maximum longitudinal deviation in vehicle coordinate. [m] 2.0 max_yaw_deviation_deg double Maximum ego yaw deviation from trajectory. [deg] 60.0","title":"Lane Departure Checker"},{"location":"control/lane_departure_checker/#lane-departure-checker","text":"The Lane Departure Checker checks if vehicle follows a trajectory. If it does not follow the trajectory, it reports its status via diagnostic_updater .","title":"Lane Departure Checker"},{"location":"control/lane_departure_checker/#features","text":"This package includes the following features: Lane Departure : Check if ego vehicle is going to be out of lane boundaries based on output from control module (predicted trajectory). Trajectory Deviation : Check if ego vehicle's pose does not deviate from the trajectory. Checking lateral, longitudinal and yaw deviation.","title":"Features"},{"location":"control/lane_departure_checker/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"control/lane_departure_checker/#how-to-extend-footprint-by-covariance","text":"Calculate the standard deviation of error ellipse(covariance) in vehicle coordinate. 1.Transform covariance into vehicle coordinate. \\begin{align} \\left( \\begin{array}{cc} x_{vehicle}\\\\ y_{vehicle}\\\\ \\end{array} \\right) = R_{map2vehicle} \\left( \\begin{array}{cc} x_{map}\\\\ y_{map}\\\\ \\end{array} \\right) \\end{align} Calculate covariance in vehicle coordinate. \\begin{align} Cov_{vehicle} &= E \\left[ \\left( \\begin{array}{cc} x_{vehicle}\\\\ y_{vehicle}\\\\ \\end{array} \\right) (x_{vehicle}, y_{vehicle}) \\right] \\\\ &= E \\left[ R\\left( \\begin{array}{cc} x_{map}\\\\ y_{map}\\\\ \\end{array} \\right) (x_{map}, y_{map})R^t \\right] \\\\ &= R E\\left[ \\left( \\begin{array}{cc} x_{map}\\\\ y_{map}\\\\ \\end{array} \\right) (x_{map}, y_{map}) \\right] R^t \\\\ &= R Cov_{map} R^t \\end{align} 2.The longitudinal length we want to expand is correspond to marginal distribution of x_{vehicle} x_{vehicle} , which is represented in Cov_{vehicle}(0,0) Cov_{vehicle}(0,0) . In the same way, the lateral length is represented in Cov_{vehicle}(1,1) Cov_{vehicle}(1,1) . Wikipedia reference here . Expand footprint based on the standard deviation multiplied with footprint_margin_scale .","title":"How to extend footprint by covariance"},{"location":"control/lane_departure_checker/#interface","text":"","title":"Interface"},{"location":"control/lane_departure_checker/#input","text":"/localization/kinematic_state [ nav_msgs::msg::Odometry ] /map/vector_map [ autoware_auto_mapping_msgs::msg::HADMapBin ] /planning/mission_planning/route [ autoware_auto_planning_msgs::msg::HADMapRoute ] /planning/scenario_planning/trajectory [ autoware_auto_planning_msgs::msg::Trajectory ] /control/trajectory_follower/predicted_trajectory [ autoware_auto_planning_msgs::msg::Trajectory ]","title":"Input"},{"location":"control/lane_departure_checker/#output","text":"[ diagnostic_updater ] lane_departure : Update diagnostic level when ego vehicle is out of lane. [ diagnostic_updater ] trajectory_deviation : Update diagnostic level when ego vehicle deviates from trajectory.","title":"Output"},{"location":"control/lane_departure_checker/#parameters","text":"","title":"Parameters"},{"location":"control/lane_departure_checker/#node-parameters","text":"Name Type Description Default value update_rate double Frequency for publishing [Hz] 10.0 visualize_lanelet bool Flag for visualizing lanelet False","title":"Node Parameters"},{"location":"control/lane_departure_checker/#core-parameters","text":"Name Type Description Default value footprint_margin_scale double Coefficient for expanding footprint margin. Multiplied by 1 standard deviation. 1.0 resample_interval double Minimum Euclidean distance between points when resample trajectory.[m] 0.3 max_deceleration double Maximum deceleration when calculating braking distance. 2.8 delay_time double Delay time which took to actuate brake when calculating braking distance. [second] 1.3 max_lateral_deviation double Maximum lateral deviation in vehicle coordinate. [m] 2.0 max_longitudinal_deviation double Maximum longitudinal deviation in vehicle coordinate. [m] 2.0 max_yaw_deviation_deg double Maximum ego yaw deviation from trajectory. [deg] 60.0","title":"Core Parameters"},{"location":"control/obstacle_collision_checker/","text":"obstacle_collision_checker # Purpose # obstacle_collision_checker is a module to check obstacle collision for predicted trajectory and publish diagnostic errors if collision is found. Inner-workings / Algorithms # Flow chart # Algorithms # Check data # Check that obstacle_collision_checker receives no ground pointcloud, predicted_trajectory, reference trajectory, and current velocity data. Diagnostic update # If any collision is found on predicted path, this module sets ERROR level as diagnostic status else sets OK . Inputs / Outputs # Input # Name Type Description ~/input/trajectory autoware_auto_planning_msgs::msg::Trajectory Reference trajectory ~/input/trajectory autoware_auto_planning_msgs::msg::Trajectory Predicted trajectory /perception/obstacle_segmentation/pointcloud sensor_msgs::msg::PointCloud2 Pointcloud of obstacles which the ego-vehicle should stop or avoid /tf tf2_msgs::msg::TFMessage TF /tf_static tf2_msgs::msg::TFMessage TF static Output # Name Type Description ~/debug/marker visualization_msgs::msg::MarkerArray Marker for visualization Parameters # Name Type Description Default value delay_time double Delay time of vehicle [s] 0.3 footprint_margin double Foot print margin [m] 0.0 max_deceleration double Max deceleration for ego vehicle to stop [m/s^2] 2.0 resample_interval double Interval for resampling trajectory [m] 0.3 search_radius double Search distance from trajectory to point cloud [m] 5.0 Assumptions / Known limits # To perform proper collision check, it is necessary to get probably predicted trajectory and obstacle pointclouds without noise.","title":"obstacle_collision_checker"},{"location":"control/obstacle_collision_checker/#obstacle_collision_checker","text":"","title":"obstacle_collision_checker"},{"location":"control/obstacle_collision_checker/#purpose","text":"obstacle_collision_checker is a module to check obstacle collision for predicted trajectory and publish diagnostic errors if collision is found.","title":"Purpose"},{"location":"control/obstacle_collision_checker/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"control/obstacle_collision_checker/#flow-chart","text":"","title":"Flow chart"},{"location":"control/obstacle_collision_checker/#algorithms","text":"","title":"Algorithms"},{"location":"control/obstacle_collision_checker/#check-data","text":"Check that obstacle_collision_checker receives no ground pointcloud, predicted_trajectory, reference trajectory, and current velocity data.","title":"Check data"},{"location":"control/obstacle_collision_checker/#diagnostic-update","text":"If any collision is found on predicted path, this module sets ERROR level as diagnostic status else sets OK .","title":"Diagnostic update"},{"location":"control/obstacle_collision_checker/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"control/obstacle_collision_checker/#input","text":"Name Type Description ~/input/trajectory autoware_auto_planning_msgs::msg::Trajectory Reference trajectory ~/input/trajectory autoware_auto_planning_msgs::msg::Trajectory Predicted trajectory /perception/obstacle_segmentation/pointcloud sensor_msgs::msg::PointCloud2 Pointcloud of obstacles which the ego-vehicle should stop or avoid /tf tf2_msgs::msg::TFMessage TF /tf_static tf2_msgs::msg::TFMessage TF static","title":"Input"},{"location":"control/obstacle_collision_checker/#output","text":"Name Type Description ~/debug/marker visualization_msgs::msg::MarkerArray Marker for visualization","title":"Output"},{"location":"control/obstacle_collision_checker/#parameters","text":"Name Type Description Default value delay_time double Delay time of vehicle [s] 0.3 footprint_margin double Foot print margin [m] 0.0 max_deceleration double Max deceleration for ego vehicle to stop [m/s^2] 2.0 resample_interval double Interval for resampling trajectory [m] 0.3 search_radius double Search distance from trajectory to point cloud [m] 5.0","title":"Parameters"},{"location":"control/obstacle_collision_checker/#assumptions-known-limits","text":"To perform proper collision check, it is necessary to get probably predicted trajectory and obstacle pointclouds without noise.","title":"Assumptions / Known limits"},{"location":"control/shift_decider/","text":"Shift Decider # Purpose # shift_decider is a module to decide shift from ackermann control command. Inner-workings / Algorithms # Flow chart # Algorithms # Inputs / Outputs # Input # Name Type Description ~/input/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand Control command for vehicle. Output # Name Type Description ~output/gear_cmd autoware_auto_vehicle_msgs::msg::GearCommand Gear for drive forward / backward. Parameters # none. Assumptions / Known limits # TBD.","title":"Shift Decider"},{"location":"control/shift_decider/#shift-decider","text":"","title":"Shift Decider"},{"location":"control/shift_decider/#purpose","text":"shift_decider is a module to decide shift from ackermann control command.","title":"Purpose"},{"location":"control/shift_decider/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"control/shift_decider/#flow-chart","text":"","title":"Flow chart"},{"location":"control/shift_decider/#algorithms","text":"","title":"Algorithms"},{"location":"control/shift_decider/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"control/shift_decider/#input","text":"Name Type Description ~/input/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand Control command for vehicle.","title":"Input"},{"location":"control/shift_decider/#output","text":"Name Type Description ~output/gear_cmd autoware_auto_vehicle_msgs::msg::GearCommand Gear for drive forward / backward.","title":"Output"},{"location":"control/shift_decider/#parameters","text":"none.","title":"Parameters"},{"location":"control/shift_decider/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"control/trajectory_follower/design/trajectory_follower-design/","text":"Trajectory Follower # This is the design document for the trajectory_follower package. Purpose / Use cases # This package provides the library code used by the nodes of the trajectory_follower_nodes package. Mainly, it implements two algorithms: Model-Predictive Control (MPC) for the computation of lateral steering commands. trajectory_follower-mpc-design PID control for the computation of velocity and acceleration commands. trajectory_follower-pid-design Related issues # https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1057 https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1058","title":"Trajectory Follower"},{"location":"control/trajectory_follower/design/trajectory_follower-design/#trajectory-follower","text":"This is the design document for the trajectory_follower package.","title":"Trajectory Follower"},{"location":"control/trajectory_follower/design/trajectory_follower-design/#purpose-use-cases","text":"This package provides the library code used by the nodes of the trajectory_follower_nodes package. Mainly, it implements two algorithms: Model-Predictive Control (MPC) for the computation of lateral steering commands. trajectory_follower-mpc-design PID control for the computation of velocity and acceleration commands. trajectory_follower-pid-design","title":"Purpose / Use cases"},{"location":"control/trajectory_follower/design/trajectory_follower-design/#related-issues","text":"https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1057 https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1058","title":"Related issues"},{"location":"control/trajectory_follower/design/trajectory_follower-mpc-design/","text":"MPC (Trajectory Follower) # This is the design document for the MPC implemented in the trajectory_follower package. Purpose / Use cases # Model Predictive Control (MPC) is used by the trajectory_follower to calculate the lateral commands corresponding to a steering angle and a steering rate. This implementation differs from the one in the mpc_controller package in several aspects. This is a linear MPC that only computes the steering command whereas the mpc_controller uses a non-linear MPC which calculates coupled steering and velocity commands. The optimization problem solved by the linear MPC is simpler, making it less likely to fail. Tuning of the linear MPC is easier. Design # MPC uses predictions of the vehicle's motion to optimize the immediate control command. Different vehicle models are implemented: kinematics : bicycle kinematics model with steering 1st-order delay. kinematics_no_delay : bicycle kinematics model without steering delay. dynamics : bicycle dynamics model considering slip angle. The kinematics model is being used by default. Please see the reference [1] for more details. For the optimization, a Quadratic Programming (QP) solver is used with two options are currently implemented: unconstraint : use least square method to solve unconstraint QP with eigen. unconstraint_fast : similar to unconstraint. This is faster, but lower accuracy for optimization. Filtering # Filtering is required for good noise reduction. A Butterworth filter is used for the yaw and lateral errors used as input of the MPC as well as for the output steering angle. Other filtering methods can be considered as long as the noise reduction performances are good enough. The moving average filter for example is not suited and can yield worse results than without any filtering. Inputs / Outputs / API # The MPC class (defined in mpc.hpp ) provides the interface with the MPC algorithm. Once a vehicle model, a QP solver, and the reference trajectory to follow have been set (using setVehicleModel() , setQPSolver() , setReferenceTrajectory() ), a lateral control command can be calculated by providing the current steer, velocity, and pose to function calculateMPC() . References / External links # [1] Jarrod M. Snider, \"Automatic Steering Methods for Autonomous Automobile Path Tracking\", Robotics Institute, Carnegie Mellon University, February 2009. Related issues # https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1057 https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1058","title":"MPC (Trajectory Follower)"},{"location":"control/trajectory_follower/design/trajectory_follower-mpc-design/#mpc-trajectory-follower","text":"This is the design document for the MPC implemented in the trajectory_follower package.","title":"MPC (Trajectory Follower)"},{"location":"control/trajectory_follower/design/trajectory_follower-mpc-design/#purpose-use-cases","text":"Model Predictive Control (MPC) is used by the trajectory_follower to calculate the lateral commands corresponding to a steering angle and a steering rate. This implementation differs from the one in the mpc_controller package in several aspects. This is a linear MPC that only computes the steering command whereas the mpc_controller uses a non-linear MPC which calculates coupled steering and velocity commands. The optimization problem solved by the linear MPC is simpler, making it less likely to fail. Tuning of the linear MPC is easier.","title":"Purpose / Use cases"},{"location":"control/trajectory_follower/design/trajectory_follower-mpc-design/#design","text":"MPC uses predictions of the vehicle's motion to optimize the immediate control command. Different vehicle models are implemented: kinematics : bicycle kinematics model with steering 1st-order delay. kinematics_no_delay : bicycle kinematics model without steering delay. dynamics : bicycle dynamics model considering slip angle. The kinematics model is being used by default. Please see the reference [1] for more details. For the optimization, a Quadratic Programming (QP) solver is used with two options are currently implemented: unconstraint : use least square method to solve unconstraint QP with eigen. unconstraint_fast : similar to unconstraint. This is faster, but lower accuracy for optimization.","title":"Design"},{"location":"control/trajectory_follower/design/trajectory_follower-mpc-design/#filtering","text":"Filtering is required for good noise reduction. A Butterworth filter is used for the yaw and lateral errors used as input of the MPC as well as for the output steering angle. Other filtering methods can be considered as long as the noise reduction performances are good enough. The moving average filter for example is not suited and can yield worse results than without any filtering.","title":"Filtering"},{"location":"control/trajectory_follower/design/trajectory_follower-mpc-design/#inputs-outputs-api","text":"The MPC class (defined in mpc.hpp ) provides the interface with the MPC algorithm. Once a vehicle model, a QP solver, and the reference trajectory to follow have been set (using setVehicleModel() , setQPSolver() , setReferenceTrajectory() ), a lateral control command can be calculated by providing the current steer, velocity, and pose to function calculateMPC() .","title":"Inputs / Outputs / API"},{"location":"control/trajectory_follower/design/trajectory_follower-mpc-design/#references-external-links","text":"[1] Jarrod M. Snider, \"Automatic Steering Methods for Autonomous Automobile Path Tracking\", Robotics Institute, Carnegie Mellon University, February 2009.","title":"References / External links"},{"location":"control/trajectory_follower/design/trajectory_follower-mpc-design/#related-issues","text":"https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1057 https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1058","title":"Related issues"},{"location":"control/trajectory_follower/design/trajectory_follower-pid-design/","text":"PID (Trajectory Follower) # This is the design document for the PID implemented in the trajectory_follower package. Purpose / Use cases # PID control is used by the trajectory_follower to calculate longitudinal commands corresponding to a velocity and an acceleration. Design # This PID control calculates the target acceleration from the deviation between the current ego-velocity and the target velocity. This PID logic has a maximum value for the output of each term. This is to prevent the following: Large integral terms may cause unintended behavior by users. Unintended noise may cause the output of the derivative term to be very large. Also, the integral term is not accumulated when the vehicle is stopped. This is to prevent unintended accumulation of the integral term in cases such as \"Autoware assumes that the vehicle is engaged, but an external system has locked the vehicle to start. On the other hand, if the vehicle gets stuck in a depression in the road surface when starting, the vehicle will not start forever, which is currently being addressed by developers. At present, PID control is implemented from the viewpoint of trade-off between development/maintenance cost and performance. This may be replaced by a higher performance controller (adaptive control or robust control) in future development. @image html images/trajectory_follower-pid-diagram.svg \"PID controller diagram\" States # This module has four state transitions as shown below in order to handle special processing in a specific situation. DRIVE Executes target velocity tracking by PID control. It also applies the delay compensation and slope compensation. STOPPING Controls the motion just before stopping. Special sequence is performed to achieve accurate and smooth stopping. STOPPED Performs operations in the stopped state (e.g. brake hold) EMERGENCY . Enters an emergency state when certain conditions are met (e.g., when the vehicle has crossed a certain distance of a stop line). The recovery condition (whether or not to keep emergency state until the vehicle completely stops) or the deceleration in the emergency state are defined by parameters. The state transition diagram is shown below. @image html images/trajectory_follower-pid-states-diagram.svg \"State Transitions\" Time delay compensation # At high speeds, the delay of actuator systems such as gas pedals and brakes has a significant impact on driving accuracy. Depending on the actuating principle of the vehicle, the mechanism that physically controls the gas pedal and brake typically has a delay of about a hundred millisecond. In this controller, the predicted ego-velocity and the target velocity after the delay time are calculated and used for the feedback to address the time delay problem. Slope compensation # Based on the slope information, a compensation term is added to the target acceleration. There are two sources of the slope information, which can be switched by a parameter. Pitch of the estimated ego-pose (default) Calculates the current slope from the pitch angle of the estimated ego-pose Pros: Easily available Cons: Cannot extract accurate slope information due to the influence of vehicle vibration. Z coordinate on the trajectory Calculates the road slope from the difference of z-coordinates between the front and rear wheel positions in the target trajectory Pros: More accurate than pitch information, if the z-coordinates of the route are properly maintained Pros: Can be used in combination with delay compensation (not yet implemented) Cons: z-coordinates of high-precision map is needed. Cons: Does not support free space planning (for now) Inputs / Outputs / API # The PIDController class is straightforward to use. First, gains and limits must be set (using setGains() and setLimits() ) for the proportional (P), integral (I), and derivative (D) components. Then, the velocity can be calculated by providing the current error and time step duration to the calculate() function. Related issues # https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1058","title":"PID (Trajectory Follower)"},{"location":"control/trajectory_follower/design/trajectory_follower-pid-design/#pid-trajectory-follower","text":"This is the design document for the PID implemented in the trajectory_follower package.","title":"PID (Trajectory Follower)"},{"location":"control/trajectory_follower/design/trajectory_follower-pid-design/#purpose-use-cases","text":"PID control is used by the trajectory_follower to calculate longitudinal commands corresponding to a velocity and an acceleration.","title":"Purpose / Use cases"},{"location":"control/trajectory_follower/design/trajectory_follower-pid-design/#design","text":"This PID control calculates the target acceleration from the deviation between the current ego-velocity and the target velocity. This PID logic has a maximum value for the output of each term. This is to prevent the following: Large integral terms may cause unintended behavior by users. Unintended noise may cause the output of the derivative term to be very large. Also, the integral term is not accumulated when the vehicle is stopped. This is to prevent unintended accumulation of the integral term in cases such as \"Autoware assumes that the vehicle is engaged, but an external system has locked the vehicle to start. On the other hand, if the vehicle gets stuck in a depression in the road surface when starting, the vehicle will not start forever, which is currently being addressed by developers. At present, PID control is implemented from the viewpoint of trade-off between development/maintenance cost and performance. This may be replaced by a higher performance controller (adaptive control or robust control) in future development. @image html images/trajectory_follower-pid-diagram.svg \"PID controller diagram\"","title":"Design"},{"location":"control/trajectory_follower/design/trajectory_follower-pid-design/#states","text":"This module has four state transitions as shown below in order to handle special processing in a specific situation. DRIVE Executes target velocity tracking by PID control. It also applies the delay compensation and slope compensation. STOPPING Controls the motion just before stopping. Special sequence is performed to achieve accurate and smooth stopping. STOPPED Performs operations in the stopped state (e.g. brake hold) EMERGENCY . Enters an emergency state when certain conditions are met (e.g., when the vehicle has crossed a certain distance of a stop line). The recovery condition (whether or not to keep emergency state until the vehicle completely stops) or the deceleration in the emergency state are defined by parameters. The state transition diagram is shown below. @image html images/trajectory_follower-pid-states-diagram.svg \"State Transitions\"","title":"States"},{"location":"control/trajectory_follower/design/trajectory_follower-pid-design/#time-delay-compensation","text":"At high speeds, the delay of actuator systems such as gas pedals and brakes has a significant impact on driving accuracy. Depending on the actuating principle of the vehicle, the mechanism that physically controls the gas pedal and brake typically has a delay of about a hundred millisecond. In this controller, the predicted ego-velocity and the target velocity after the delay time are calculated and used for the feedback to address the time delay problem.","title":"Time delay compensation"},{"location":"control/trajectory_follower/design/trajectory_follower-pid-design/#slope-compensation","text":"Based on the slope information, a compensation term is added to the target acceleration. There are two sources of the slope information, which can be switched by a parameter. Pitch of the estimated ego-pose (default) Calculates the current slope from the pitch angle of the estimated ego-pose Pros: Easily available Cons: Cannot extract accurate slope information due to the influence of vehicle vibration. Z coordinate on the trajectory Calculates the road slope from the difference of z-coordinates between the front and rear wheel positions in the target trajectory Pros: More accurate than pitch information, if the z-coordinates of the route are properly maintained Pros: Can be used in combination with delay compensation (not yet implemented) Cons: z-coordinates of high-precision map is needed. Cons: Does not support free space planning (for now)","title":"Slope compensation"},{"location":"control/trajectory_follower/design/trajectory_follower-pid-design/#inputs-outputs-api","text":"The PIDController class is straightforward to use. First, gains and limits must be set (using setGains() and setLimits() ) for the proportional (P), integral (I), and derivative (D) components. Then, the velocity can be calculated by providing the current error and time step duration to the calculate() function.","title":"Inputs / Outputs / API"},{"location":"control/trajectory_follower/design/trajectory_follower-pid-design/#related-issues","text":"https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1058","title":"Related issues"},{"location":"control/trajectory_follower_nodes/design/lateral_controller-design/","text":"Lateral Controller # This is the design document for the lateral controller node in the trajectory_follower_nodes package. Purpose / Use cases # This node is used to general lateral control commands (steering angle and steering rate) when following a path. Design # The node uses an implementation of linear model predictive control (MPC) for accurate path tracking. The MPC uses a model of the vehicle to simulate the trajectory resulting from the control command. The optimization of the control command is formulated as a Quadratic Program (QP). These functionalities are implemented in the trajectory_follower package (see trajectory_follower-mpc-design ) Assumptions / Known limits # The tracking is not accurate if the first point of the reference trajectory is at or in front of the current ego pose. Issue to add points behind ego: https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1273 Inputs / Outputs / API # Inputs input/reference_trajectory : reference trajectory to follow. input/current_kinematic_state : current state of the vehicle (position, velocity, etc). Output output/lateral_control_cmd : generated lateral control command. Parameter description # The default parameters defined in param/lateral_controller_defaults.param.yaml are adjusted to the AutonomouStuff Lexus RX 450h for under 40 km/h driving. Name Type Description Default value show_debug_info bool display debug info false ctrl_period double control period [s] 0.03 traj_resample_dist double distance of waypoints in resampling [m] 0.1 enable_path_smoothing bool path smoothing flag. This should be true when uses path resampling to reduce resampling noise. true path_filter_moving_ave_num int number of data points moving average filter for path smoothing 35 path_smoothing_times int number of times of applying path smoothing filter 1 curvature_smoothing_num_ref_steer double index distance of points used in curvature calculation for reference steer command: p(i-num), p(i), p(i+num). larger num makes less noisy values. 35 curvature_smoothing_num_traj double index distance of points used in curvature calculation for trajectory: p(i-num), p(i), p(i+num). larger num makes less noisy values. 1 steering_lpf_cutoff_hz double cutoff frequency of lowpass filter for steering output command [hz] 3.0 admissible_position_error double stop vehicle when following position error is larger than this value [m]. 5.0 admissible_yaw_error_rad double stop vehicle when following yaw angle error is larger than this value [rad]. 1.57 stop_state_entry_ego_speed *1 double threshold value of the ego vehicle speed used to the stop state entry condition 0.0 stop_state_entry_target_speed *1 double threshold value of the target speed used to the stop state entry condition 0.0 (*1) To prevent unnecessary steering movement, the steering command is fixed to the previous value in the stop state. MPC algorithm # Name Type Description Default value qp_solver_type string QP solver option. described below in detail. unconstraint_fast vehicle_model_type string vehicle model option. described below in detail. kinematics prediction_horizon int total prediction step for MPC 70 prediction_sampling_time double prediction period for one step [s] 0.1 weight_lat_error double weight for lateral error 0.1 weight_heading_error double weight for heading error 0.0 weight_heading_error_squared_vel_coeff double weight for heading error * velocity 5.0 weight_steering_input double weight for steering error (steer command - reference steer) 1.0 weight_steering_input_squared_vel_coeff double weight for steering error (steer command - reference steer) * velocity 0.25 weight_lat_jerk double weight for lateral jerk (steer(i) - steer(i-1)) * velocity 0.0 weight_terminal_lat_error double terminal cost weight for lateral error 1.0 weight_terminal_heading_error double terminal cost weight for heading error 0.1 zero_ff_steer_deg double threshold of feedforward angle [deg]. feedforward angle smaller than this value is set to zero. 2.0 Vehicle # Name Type Description Default value cg_to_front_m double distance from baselink to the front axle[m] 1.228 cg_to_rear_m double distance from baselink to the rear axle [m] 1.5618 mass_fl double mass applied to front left tire [kg] 600 mass_fr double mass applied to front right tire [kg] 600 mass_rl double mass applied to rear left tire [kg] 600 mass_rr double mass applied to rear right tire [kg] 600 cf double front cornering power [N/rad] 155494.663 cr double rear cornering power [N/rad] 155494.663 steering_tau double steering dynamics time constant (1d approximation) for vehicle model [s] 0.3 steer_lim_deg double steering angle limit for vehicle model [deg]. This is also used for QP constraint. 35.0 How to tune MPC parameters # Set appropriate vehicle kinematics parameters for distance to front and rear axle, and steer_lim_deg . Also check that the input VehicleKinematicState has appropriate values (speed: vehicle rear-wheels-center velocity [km/h], angle: steering (tire) angle [rad]). These values give a vehicle information to the controller for path following. Errors in these values cause fundamental tracking error. Set appropriate vehicle dynamics parameters of steering_tau , which is the approximated delay from steering angle command to actual steering angle. Set weight_steering_input = 1.0, weight_lat_error = 0.1, and other weights to 0. If the vehicle oscillates when driving with low speed, set weight_lat_error smaller. Adjust other weights. One of the simple way for tuning is to increase weight_lat_error until oscillation occurs. If the vehicle is unstable with very small weight_lat_error , increase terminal weight : weight_terminal_lat_error and weight_terminal_heading_error to improve tracking stability. Larger prediction_horizon and smaller prediction_sampling_time is effective for tracking performance, but it is a trade-off between computational costs. Other parameters can be adjusted like below. weight_lat_error : Reduce lateral tracking error. This acts like P gain in PID. weight_heading_error : Make a drive straight. This acts like D gain in PID. weight_heading_error_squared_vel_coeff : Make a drive straight in high speed range. weight_steering_input : Reduce oscillation of tracking. weight_steering_input_squared_vel_coeff : Reduce oscillation of tracking in high speed range. weight_lat_jerk : Reduce lateral jerk. weight_terminal_lat_error : Preferable to set a higher value than normal lateral weight weight_lat_error for stability. weight_terminal_heading_error : Preferable to set a higher value than normal heading weight weight_heading_error for stability. Related issues # https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1057","title":"Lateral Controller"},{"location":"control/trajectory_follower_nodes/design/lateral_controller-design/#lateral-controller","text":"This is the design document for the lateral controller node in the trajectory_follower_nodes package.","title":"Lateral Controller"},{"location":"control/trajectory_follower_nodes/design/lateral_controller-design/#purpose-use-cases","text":"This node is used to general lateral control commands (steering angle and steering rate) when following a path.","title":"Purpose / Use cases"},{"location":"control/trajectory_follower_nodes/design/lateral_controller-design/#design","text":"The node uses an implementation of linear model predictive control (MPC) for accurate path tracking. The MPC uses a model of the vehicle to simulate the trajectory resulting from the control command. The optimization of the control command is formulated as a Quadratic Program (QP). These functionalities are implemented in the trajectory_follower package (see trajectory_follower-mpc-design )","title":"Design"},{"location":"control/trajectory_follower_nodes/design/lateral_controller-design/#assumptions-known-limits","text":"The tracking is not accurate if the first point of the reference trajectory is at or in front of the current ego pose. Issue to add points behind ego: https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1273","title":"Assumptions / Known limits"},{"location":"control/trajectory_follower_nodes/design/lateral_controller-design/#inputs-outputs-api","text":"Inputs input/reference_trajectory : reference trajectory to follow. input/current_kinematic_state : current state of the vehicle (position, velocity, etc). Output output/lateral_control_cmd : generated lateral control command.","title":"Inputs / Outputs / API"},{"location":"control/trajectory_follower_nodes/design/lateral_controller-design/#parameter-description","text":"The default parameters defined in param/lateral_controller_defaults.param.yaml are adjusted to the AutonomouStuff Lexus RX 450h for under 40 km/h driving. Name Type Description Default value show_debug_info bool display debug info false ctrl_period double control period [s] 0.03 traj_resample_dist double distance of waypoints in resampling [m] 0.1 enable_path_smoothing bool path smoothing flag. This should be true when uses path resampling to reduce resampling noise. true path_filter_moving_ave_num int number of data points moving average filter for path smoothing 35 path_smoothing_times int number of times of applying path smoothing filter 1 curvature_smoothing_num_ref_steer double index distance of points used in curvature calculation for reference steer command: p(i-num), p(i), p(i+num). larger num makes less noisy values. 35 curvature_smoothing_num_traj double index distance of points used in curvature calculation for trajectory: p(i-num), p(i), p(i+num). larger num makes less noisy values. 1 steering_lpf_cutoff_hz double cutoff frequency of lowpass filter for steering output command [hz] 3.0 admissible_position_error double stop vehicle when following position error is larger than this value [m]. 5.0 admissible_yaw_error_rad double stop vehicle when following yaw angle error is larger than this value [rad]. 1.57 stop_state_entry_ego_speed *1 double threshold value of the ego vehicle speed used to the stop state entry condition 0.0 stop_state_entry_target_speed *1 double threshold value of the target speed used to the stop state entry condition 0.0 (*1) To prevent unnecessary steering movement, the steering command is fixed to the previous value in the stop state.","title":"Parameter description"},{"location":"control/trajectory_follower_nodes/design/lateral_controller-design/#mpc-algorithm","text":"Name Type Description Default value qp_solver_type string QP solver option. described below in detail. unconstraint_fast vehicle_model_type string vehicle model option. described below in detail. kinematics prediction_horizon int total prediction step for MPC 70 prediction_sampling_time double prediction period for one step [s] 0.1 weight_lat_error double weight for lateral error 0.1 weight_heading_error double weight for heading error 0.0 weight_heading_error_squared_vel_coeff double weight for heading error * velocity 5.0 weight_steering_input double weight for steering error (steer command - reference steer) 1.0 weight_steering_input_squared_vel_coeff double weight for steering error (steer command - reference steer) * velocity 0.25 weight_lat_jerk double weight for lateral jerk (steer(i) - steer(i-1)) * velocity 0.0 weight_terminal_lat_error double terminal cost weight for lateral error 1.0 weight_terminal_heading_error double terminal cost weight for heading error 0.1 zero_ff_steer_deg double threshold of feedforward angle [deg]. feedforward angle smaller than this value is set to zero. 2.0","title":"MPC algorithm"},{"location":"control/trajectory_follower_nodes/design/lateral_controller-design/#vehicle","text":"Name Type Description Default value cg_to_front_m double distance from baselink to the front axle[m] 1.228 cg_to_rear_m double distance from baselink to the rear axle [m] 1.5618 mass_fl double mass applied to front left tire [kg] 600 mass_fr double mass applied to front right tire [kg] 600 mass_rl double mass applied to rear left tire [kg] 600 mass_rr double mass applied to rear right tire [kg] 600 cf double front cornering power [N/rad] 155494.663 cr double rear cornering power [N/rad] 155494.663 steering_tau double steering dynamics time constant (1d approximation) for vehicle model [s] 0.3 steer_lim_deg double steering angle limit for vehicle model [deg]. This is also used for QP constraint. 35.0","title":"Vehicle"},{"location":"control/trajectory_follower_nodes/design/lateral_controller-design/#how-to-tune-mpc-parameters","text":"Set appropriate vehicle kinematics parameters for distance to front and rear axle, and steer_lim_deg . Also check that the input VehicleKinematicState has appropriate values (speed: vehicle rear-wheels-center velocity [km/h], angle: steering (tire) angle [rad]). These values give a vehicle information to the controller for path following. Errors in these values cause fundamental tracking error. Set appropriate vehicle dynamics parameters of steering_tau , which is the approximated delay from steering angle command to actual steering angle. Set weight_steering_input = 1.0, weight_lat_error = 0.1, and other weights to 0. If the vehicle oscillates when driving with low speed, set weight_lat_error smaller. Adjust other weights. One of the simple way for tuning is to increase weight_lat_error until oscillation occurs. If the vehicle is unstable with very small weight_lat_error , increase terminal weight : weight_terminal_lat_error and weight_terminal_heading_error to improve tracking stability. Larger prediction_horizon and smaller prediction_sampling_time is effective for tracking performance, but it is a trade-off between computational costs. Other parameters can be adjusted like below. weight_lat_error : Reduce lateral tracking error. This acts like P gain in PID. weight_heading_error : Make a drive straight. This acts like D gain in PID. weight_heading_error_squared_vel_coeff : Make a drive straight in high speed range. weight_steering_input : Reduce oscillation of tracking. weight_steering_input_squared_vel_coeff : Reduce oscillation of tracking in high speed range. weight_lat_jerk : Reduce lateral jerk. weight_terminal_lat_error : Preferable to set a higher value than normal lateral weight weight_lat_error for stability. weight_terminal_heading_error : Preferable to set a higher value than normal heading weight weight_heading_error for stability.","title":"How to tune MPC parameters"},{"location":"control/trajectory_follower_nodes/design/lateral_controller-design/#related-issues","text":"https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1057","title":"Related issues"},{"location":"control/trajectory_follower_nodes/design/latlon_muxer-design/","text":"Lateral/Longitudinal Control Muxer # Purpose # When using controllers that independently compute lateral and longitudinal commands, this node combines the resulting messages into a single control command message. Design # Inputs. AckermannLateralCommand : lateral command. LongitudinalCommand : longitudinal command. Output. AckermannControlCommand : message containing both lateral and longitudinal commands. Parameter. timeout_thr_sec : duration in second after which input messages are discarded. Each time the node receives an input message it publishes an AckermannControlCommand if the following two conditions are met. Both inputs have been received. The last received input messages are not older than defined by timeout_thr_sec . Implementation Details # Callbacks latCtrlCmdCallback and lonCtrlCmdCallback are defined for each input message. Upon reception, the message is stored and function publishCmd is called. Function publishCmd first checks that both messages have been received and that the stored messages are not older than the timeout. If both conditions are true, the combined control message is built and published. checkTimeout is used to check that the stored messages are not too old by comparing the timeout parameter timeout_thr_sec with the difference between rclcpp::Time stamp and the current time now() .","title":"Lateral/Longitudinal Control Muxer"},{"location":"control/trajectory_follower_nodes/design/latlon_muxer-design/#laterallongitudinal-control-muxer","text":"","title":"Lateral/Longitudinal Control Muxer"},{"location":"control/trajectory_follower_nodes/design/latlon_muxer-design/#purpose","text":"When using controllers that independently compute lateral and longitudinal commands, this node combines the resulting messages into a single control command message.","title":"Purpose"},{"location":"control/trajectory_follower_nodes/design/latlon_muxer-design/#design","text":"Inputs. AckermannLateralCommand : lateral command. LongitudinalCommand : longitudinal command. Output. AckermannControlCommand : message containing both lateral and longitudinal commands. Parameter. timeout_thr_sec : duration in second after which input messages are discarded. Each time the node receives an input message it publishes an AckermannControlCommand if the following two conditions are met. Both inputs have been received. The last received input messages are not older than defined by timeout_thr_sec .","title":"Design"},{"location":"control/trajectory_follower_nodes/design/latlon_muxer-design/#implementation-details","text":"Callbacks latCtrlCmdCallback and lonCtrlCmdCallback are defined for each input message. Upon reception, the message is stored and function publishCmd is called. Function publishCmd first checks that both messages have been received and that the stored messages are not older than the timeout. If both conditions are true, the combined control message is built and published. checkTimeout is used to check that the stored messages are not too old by comparing the timeout parameter timeout_thr_sec with the difference between rclcpp::Time stamp and the current time now() .","title":"Implementation Details"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/","text":"Longitudinal Controller # Purpose / Use cases # The longitudinal_controller computes the target acceleration to achieve the target velocity set at each point of the target trajectory using a feed-forward/back control. It also contains a slope force correction that takes into account road slope information, and a delay compensation function. It is assumed that the target acceleration calculated here will be properly realized by the vehicle interface. Note that the use of this module is not mandatory for Autoware if the vehicle supports the \"target speed\" interface. Separated lateral (steering) and longitudinal (velocity) controls # This longitudinal controller assumes that the roles of lateral and longitudinal control are separated as follows. Lateral control computes a target steering to keep the vehicle on the trajectory, assuming perfect velocity tracking. Longitudinal control computes a target velocity/acceleration to keep the vehicle velocity on the trajectory speed, assuming perfect trajectory tracking. Ideally, dealing with the lateral and longitudinal control as a single mixed problem can achieve high performance. In contrast, there are two reasons to provide velocity controller as a stand-alone function, described below. Complex requirements for longitudinal motion # The longitudinal vehicle behavior that humans expect is difficult to express in a single logic. For example, the expected behavior just before stopping differs depending on whether the ego-position is ahead/behind of the stop line, or whether the current speed is higher/lower than the target speed to achieve a human-like movement. In addition, some vehicles have difficulty measuring the ego-speed at extremely low speeds. In such cases, a configuration that can improve the functionality of the longitudinal control without affecting the lateral control is important. There are many characteristics and needs that are unique to longitudinal control. Designing them separately from the lateral control keeps the modules less coupled and improves maintainability. Nonlinear coupling of lateral and longitudinal motion # The lat-lon mixed control problem is very complex and uses nonlinear optimization to achieve high performance. Since it is difficult to guarantee the convergence of the nonlinear optimization, a simple control logic is also necessary for development. Also, the benefits of simultaneous longitudinal and lateral control are small if the vehicle doesn't move at high speed. Design # Assumptions / Known limits # Smoothed target velocity and its acceleration shall be set in the trajectory The velocity command is not smoothed inside the controller (only noise may be removed). For step-like target signal, tracking is performed as fast as possible. The vehicle velocity must be an appropriate value The ego-velocity must be a signed-value corresponding to the forward/backward direction The ego-velocity should be given with appropriate noise processing. If there is a large amount of noise in the ego-velocity, the tracking performance will be significantly reduced. The output of this controller must be achieved by later modules (e.g. vehicle interface). If the vehicle interface does not have the target velocity or acceleration interface (e.g., the vehicle only has a gas pedal and brake interface), an appropriate conversion must be done after this controller. Inputs / Outputs / API # Output # control_cmd [ autoware_auto_msgs/LongitudinalCommand ] : command to control the longitudinal motion of the vehicle. It contains the target velocity and target acceleration. debug_values [ autoware_auto_msgs/Float32MultiArrayDiagnostic ] : debug values used for the control command generation (e.g. the contributions of each P-I-D terms). Input # current_state [ autoware_auto_msgs/VehicleKinematicState ] : Current ego state including the current pose and velocity. current_trajectory [ autoware_auto_msgs/Trajectory ] : Current target trajectory for the desired velocity on the each trajectory points. Inner-workings / Algorithms # States # This module has four state transitions as shown below in order to handle special processing in a specific situation. DRIVE Executes target velocity tracking by PID control. It also applies the delay compensation and slope compensation. STOPPING Controls the motion just before stopping. Special sequence is performed to achieve accurate and smooth stopping. STOPPED Performs operations in the stopped state (e.g. brake hold) EMERGENCY . Enters an emergency state when certain conditions are met (e.g., when the vehicle has crossed a certain distance of a stop line). The recovery condition (whether or not to keep emergency state until the vehicle completely stops) or the deceleration in the emergency state are defined by parameters. The state transition diagram is shown below. Logics # Control Block Diagram # FeedForward (FF) # The reference acceleration set in the trajectory and slope compensation terms are output as a feedforward. Under ideal conditions with no modeling error, this FF term alone should be sufficient for velocity tracking. Tracking errors causing modeling or discretization errors are removed by the feedback control (now using PID). Brake keeping # From the viewpoint of ride comfort, stopping with 0 acceleration is important because it reduces the impact of braking. However, if the target acceleration when stopping is 0, the vehicle may cross over the stop line or accelerate a little in front of the stop line due to vehicle model error or gradient estimation error. For reliable stopping, the target acceleration calculated by the FeedForward system is limited to a negative acceleration when stopping. Slope compensation # Based on the slope information, a compensation term is added to the target acceleration. There are two sources of the slope information, which can be switched by a parameter. Pitch of the estimated ego-pose (default) Calculates the current slope from the pitch angle of the estimated ego-pose Pros: Easily available Cons: Cannot extract accurate slope information due to the influence of vehicle vibration. Z coordinate on the trajectory Calculates the road slope from the difference of z-coordinates between the front and rear wheel positions in the target trajectory Pros: More accurate than pitch information, if the z-coordinates of the route are properly maintained Pros: Can be used in combination with delay compensation (not yet implemented) Cons: z-coordinates of high-precision map is needed. Cons: Does not support free space planning (for now) PID control # For deviations that cannot be handled by FeedForward control, such as model errors, PID control is used to construct a feedback system. This PID control calculates the target acceleration from the deviation between the current ego-velocity and the target velocity. This PID logic has a maximum value for the output of each term. This is to prevent the following: Large integral terms may cause unintended behavior by users. Unintended noise may cause the output of the derivative term to be very large. Also, the integral term is not accumulated when the vehicle is stopped. This is to prevent unintended accumulation of the integral term in cases such as \"Autoware assumes that the vehicle is engaged, but an external system has locked the vehicle to start. On the other hand, if the vehicle gets stuck in a depression in the road surface when starting, the vehicle will not start forever, which is currently being addressed by developers. At present, PID control is implemented from the viewpoint of trade-off between development/maintenance cost and performance. This may be replaced by a higher performance controller (adaptive control or robust control) in future development. Time delay compensation # At high speeds, the delay of actuator systems such as gas pedals and brakes has a significant impact on driving accuracy. Depending on the actuating principle of the vehicle, the mechanism that physically controls the gas pedal and brake typically has a delay of about a hundred millisecond. In this controller, the predicted ego-velocity and the target velocity after the delay time are calculated and used for the feedback to address the time delay problem. Parameter description # The default parameters defined in param/lateral_controller_defaults.param.yaml are adjusted to the AutonomouStuff Lexus RX 450h for under 40 km/h driving. Name Type Description Default value longitudinal_ctrl_period double longitudinal control period [s] 0.03 delay_compensation_time double delay for longitudinal control [s] 0.17 enable_smooth_stop bool flag to enable transition to STOPPING true enable_overshoot_emergency bool flag to enable transition to EMERGENCY when the ego is over the stop line with a certain distance. See emergency_state_overshoot_stop_dist . true enable_slope_compensation bool flag to modify output acceleration for slope compensation. The source of the slope angle can be selected from ego-pose or trajectory angle. See use_trajectory_for_pitch_calculation . true enable_brake_keeping_before_stop bool flag to keep a certain acceleration during DRIVE state before the ego stops. See Brake keeping . false max_acc double max value of output acceleration [m/s^2] 3.0 min_acc double min value of output acceleration [m/s^2] -5.0 max_jerk double max value of jerk of output acceleration [m/s^3] 2.0 min_jerk double min value of jerk of output acceleration [m/s^3] -5.0 use_trajectory_for_pitch_calculation bool If true, the slope is estimated from trajectory z-level. Otherwise the pitch angle of the ego pose is used. false lpf_pitch_gain double gain of low-pass filter for pitch estimation 0.95 max_pitch_rad double max value of estimated pitch [rad] 0.1 min_pitch_rad double min value of estimated pitch [rad] -0.1 State transition # Name Type Description Default value drive_state_stop_dist double The state will transit to DRIVE when the distance to the stop point is longer than drive_state_stop_dist + drive_state_offset_stop_dist [m] 0.5 drive_state_offset_stop_dist double The state will transit to DRIVE when the distance to the stop point is longer than drive_state_stop_dist + drive_state_offset_stop_dist [m] 1.0 stopping_state_stop_dist double The state will transit to STOPPING when the distance to the stop point is shorter than stopping_state_stop_dist [m] 0.5 stopped_state_entry_vel double threshold of the ego velocity in transition to the STOPPED state [m/s] 0.01 stopped_state_entry_acc double threshold of the ego acceleration in transition to the STOPPED state [m/s^2] 0.1 emergency_state_overshoot_stop_dist double If enable_overshoot_emergency is true and the ego is emergency_state_overshoot_stop_dist -meter ahead of the stop point, the state will transit to EMERGENCY. [m] 1.5 emergency_state_traj_trans_dev double If the ego's position is emergency_state_traj_tran_dev meter away from the nearest trajectory point, the state will transit to EMERGENCY. [m] 3.0 emergency_state_traj_rot_dev double If the ego's orientation is emergency_state_traj_rot_dev rad away from the nearest trajectory point orientation, the state will transit to EMERGENCY. [rad] 0.784 DRIVE Parameter # Name Type Description Default value kp double p gain for longitudinal control 1.0 ki double i gain for longitudinal control 0.1 kd double d gain for longitudinal control 0.0 max_out double max value of PID's output acceleration during DRIVE state [m/s^2] 1.0 min_out double min value of PID's output acceleration during DRIVE state [m/s^2] -1.0 max_p_effort double max value of acceleration with p gain 1.0 min_p_effort double min value of acceleration with p gain -1.0 max_i_effort double max value of acceleration with i gain 0.3 min_i_effort double min value of acceleration with i gain -0.3 max_d_effort double max value of acceleration with d gain 0.0 min_d_effort double min value of acceleration with d gain 0.0 lpf_vel_error_gain double gain of low-pass filter for velocity error 0.9 current_vel_threshold_pid_integration double Velocity error is integrated for I-term only when the absolute value of current velocity is larger than this parameter. [m/s] 0.5 brake_keeping_acc double If enable_brake_keeping_before_stop is true, a certain acceleration is kept during DRIVE state before the ego stops [m/s^2] See Brake keeping . 0.2 STOPPING Parameter (smooth stop) # Smooth stop is enabled if enable_smooth_stop is true. In smooth stop, strong acceleration ( strong_acc ) will be output first to decrease the ego velocity. Then weak acceleration ( weak_acc ) will be output to stop smoothly by decreasing the ego jerk. If the ego does not stop in a certain time or some-meter over the stop point, weak acceleration to stop right ( weak_stop_acc ) now will be output. If the ego is still running, strong acceleration ( strong_stop_acc ) to stop right now will be output. Name Type Description Default value smooth_stop_max_strong_acc double max strong acceleration [m/s^2] -0.5 smooth_stop_min_strong_acc double min strong acceleration [m/s^2] -0.8 smooth_stop_weak_acc double weak acceleration [m/s^2] -0.3 smooth_stop_weak_stop_acc double weak acceleration to stop right now [m/s^2] -0.8 smooth_stop_strong_stop_acc double strong acceleration to be output when the ego is smooth_stop_strong_stop_dist -meter over the stop point. [m/s^2] -3.4 smooth_stop_max_fast_vel double max fast vel to judge the ego is running fast [m/s]. If the ego is running fast, strong acceleration will be output. 0.5 smooth_stop_min_running_vel double min ego velocity to judge if the ego is running or not [m/s] 0.01 smooth_stop_min_running_acc double min ego acceleration to judge if the ego is running or not [m/s^2] 0.01 smooth_stop_weak_stop_time double max time to output weak acceleration [s]. After this, strong acceleration will be output. 0.8 smooth_stop_weak_stop_dist double Weak acceleration will be output when the ego is smooth_stop_weak_stop_dist -meter before the stop point. [m] -0.3 smooth_stop_strong_stop_dist double Strong acceleration will be output when the ego is smooth_stop_strong_stop_dist -meter over the stop point. [m] -0.5 STOPPED Parameter # Name Type Description Default value stopped_vel double target velocity in STOPPED state [m/s] 0.0 stopped_acc double target acceleration in STOPPED state [m/s^2] -3.4 stopped_jerk double target jerk in STOPPED state [m/s^3] -5.0 EMERGENCY Parameter # Name Type Description Default value emergency_vel double target velocity in EMERGENCY state [m/s] 0.0 emergency_acc double target acceleration in an EMERGENCY state [m/s^2] -5.0 emergency_jerk double target jerk in an EMERGENCY state [m/s^3] -3.0 References / External links # Future extensions / Unimplemented parts # Related issues # https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1058 https://github.com/autowarefoundation/autoware.universe/issues/701","title":"Longitudinal Controller"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#longitudinal-controller","text":"","title":"Longitudinal Controller"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#purpose-use-cases","text":"The longitudinal_controller computes the target acceleration to achieve the target velocity set at each point of the target trajectory using a feed-forward/back control. It also contains a slope force correction that takes into account road slope information, and a delay compensation function. It is assumed that the target acceleration calculated here will be properly realized by the vehicle interface. Note that the use of this module is not mandatory for Autoware if the vehicle supports the \"target speed\" interface.","title":"Purpose / Use cases"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#separated-lateral-steering-and-longitudinal-velocity-controls","text":"This longitudinal controller assumes that the roles of lateral and longitudinal control are separated as follows. Lateral control computes a target steering to keep the vehicle on the trajectory, assuming perfect velocity tracking. Longitudinal control computes a target velocity/acceleration to keep the vehicle velocity on the trajectory speed, assuming perfect trajectory tracking. Ideally, dealing with the lateral and longitudinal control as a single mixed problem can achieve high performance. In contrast, there are two reasons to provide velocity controller as a stand-alone function, described below.","title":"Separated lateral (steering) and longitudinal (velocity) controls"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#complex-requirements-for-longitudinal-motion","text":"The longitudinal vehicle behavior that humans expect is difficult to express in a single logic. For example, the expected behavior just before stopping differs depending on whether the ego-position is ahead/behind of the stop line, or whether the current speed is higher/lower than the target speed to achieve a human-like movement. In addition, some vehicles have difficulty measuring the ego-speed at extremely low speeds. In such cases, a configuration that can improve the functionality of the longitudinal control without affecting the lateral control is important. There are many characteristics and needs that are unique to longitudinal control. Designing them separately from the lateral control keeps the modules less coupled and improves maintainability.","title":"Complex requirements for longitudinal motion"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#nonlinear-coupling-of-lateral-and-longitudinal-motion","text":"The lat-lon mixed control problem is very complex and uses nonlinear optimization to achieve high performance. Since it is difficult to guarantee the convergence of the nonlinear optimization, a simple control logic is also necessary for development. Also, the benefits of simultaneous longitudinal and lateral control are small if the vehicle doesn't move at high speed.","title":"Nonlinear coupling of lateral and longitudinal motion"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#design","text":"","title":"Design"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#assumptions-known-limits","text":"Smoothed target velocity and its acceleration shall be set in the trajectory The velocity command is not smoothed inside the controller (only noise may be removed). For step-like target signal, tracking is performed as fast as possible. The vehicle velocity must be an appropriate value The ego-velocity must be a signed-value corresponding to the forward/backward direction The ego-velocity should be given with appropriate noise processing. If there is a large amount of noise in the ego-velocity, the tracking performance will be significantly reduced. The output of this controller must be achieved by later modules (e.g. vehicle interface). If the vehicle interface does not have the target velocity or acceleration interface (e.g., the vehicle only has a gas pedal and brake interface), an appropriate conversion must be done after this controller.","title":"Assumptions / Known limits"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#inputs-outputs-api","text":"","title":"Inputs / Outputs / API"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#output","text":"control_cmd [ autoware_auto_msgs/LongitudinalCommand ] : command to control the longitudinal motion of the vehicle. It contains the target velocity and target acceleration. debug_values [ autoware_auto_msgs/Float32MultiArrayDiagnostic ] : debug values used for the control command generation (e.g. the contributions of each P-I-D terms).","title":"Output"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#input","text":"current_state [ autoware_auto_msgs/VehicleKinematicState ] : Current ego state including the current pose and velocity. current_trajectory [ autoware_auto_msgs/Trajectory ] : Current target trajectory for the desired velocity on the each trajectory points.","title":"Input"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#states","text":"This module has four state transitions as shown below in order to handle special processing in a specific situation. DRIVE Executes target velocity tracking by PID control. It also applies the delay compensation and slope compensation. STOPPING Controls the motion just before stopping. Special sequence is performed to achieve accurate and smooth stopping. STOPPED Performs operations in the stopped state (e.g. brake hold) EMERGENCY . Enters an emergency state when certain conditions are met (e.g., when the vehicle has crossed a certain distance of a stop line). The recovery condition (whether or not to keep emergency state until the vehicle completely stops) or the deceleration in the emergency state are defined by parameters. The state transition diagram is shown below.","title":"States"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#logics","text":"","title":"Logics"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#control-block-diagram","text":"","title":"Control Block Diagram"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#feedforward-ff","text":"The reference acceleration set in the trajectory and slope compensation terms are output as a feedforward. Under ideal conditions with no modeling error, this FF term alone should be sufficient for velocity tracking. Tracking errors causing modeling or discretization errors are removed by the feedback control (now using PID).","title":"FeedForward (FF)"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#brake-keeping","text":"From the viewpoint of ride comfort, stopping with 0 acceleration is important because it reduces the impact of braking. However, if the target acceleration when stopping is 0, the vehicle may cross over the stop line or accelerate a little in front of the stop line due to vehicle model error or gradient estimation error. For reliable stopping, the target acceleration calculated by the FeedForward system is limited to a negative acceleration when stopping.","title":"Brake keeping"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#slope-compensation","text":"Based on the slope information, a compensation term is added to the target acceleration. There are two sources of the slope information, which can be switched by a parameter. Pitch of the estimated ego-pose (default) Calculates the current slope from the pitch angle of the estimated ego-pose Pros: Easily available Cons: Cannot extract accurate slope information due to the influence of vehicle vibration. Z coordinate on the trajectory Calculates the road slope from the difference of z-coordinates between the front and rear wheel positions in the target trajectory Pros: More accurate than pitch information, if the z-coordinates of the route are properly maintained Pros: Can be used in combination with delay compensation (not yet implemented) Cons: z-coordinates of high-precision map is needed. Cons: Does not support free space planning (for now)","title":"Slope compensation"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#pid-control","text":"For deviations that cannot be handled by FeedForward control, such as model errors, PID control is used to construct a feedback system. This PID control calculates the target acceleration from the deviation between the current ego-velocity and the target velocity. This PID logic has a maximum value for the output of each term. This is to prevent the following: Large integral terms may cause unintended behavior by users. Unintended noise may cause the output of the derivative term to be very large. Also, the integral term is not accumulated when the vehicle is stopped. This is to prevent unintended accumulation of the integral term in cases such as \"Autoware assumes that the vehicle is engaged, but an external system has locked the vehicle to start. On the other hand, if the vehicle gets stuck in a depression in the road surface when starting, the vehicle will not start forever, which is currently being addressed by developers. At present, PID control is implemented from the viewpoint of trade-off between development/maintenance cost and performance. This may be replaced by a higher performance controller (adaptive control or robust control) in future development.","title":"PID control"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#time-delay-compensation","text":"At high speeds, the delay of actuator systems such as gas pedals and brakes has a significant impact on driving accuracy. Depending on the actuating principle of the vehicle, the mechanism that physically controls the gas pedal and brake typically has a delay of about a hundred millisecond. In this controller, the predicted ego-velocity and the target velocity after the delay time are calculated and used for the feedback to address the time delay problem.","title":"Time delay compensation"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#parameter-description","text":"The default parameters defined in param/lateral_controller_defaults.param.yaml are adjusted to the AutonomouStuff Lexus RX 450h for under 40 km/h driving. Name Type Description Default value longitudinal_ctrl_period double longitudinal control period [s] 0.03 delay_compensation_time double delay for longitudinal control [s] 0.17 enable_smooth_stop bool flag to enable transition to STOPPING true enable_overshoot_emergency bool flag to enable transition to EMERGENCY when the ego is over the stop line with a certain distance. See emergency_state_overshoot_stop_dist . true enable_slope_compensation bool flag to modify output acceleration for slope compensation. The source of the slope angle can be selected from ego-pose or trajectory angle. See use_trajectory_for_pitch_calculation . true enable_brake_keeping_before_stop bool flag to keep a certain acceleration during DRIVE state before the ego stops. See Brake keeping . false max_acc double max value of output acceleration [m/s^2] 3.0 min_acc double min value of output acceleration [m/s^2] -5.0 max_jerk double max value of jerk of output acceleration [m/s^3] 2.0 min_jerk double min value of jerk of output acceleration [m/s^3] -5.0 use_trajectory_for_pitch_calculation bool If true, the slope is estimated from trajectory z-level. Otherwise the pitch angle of the ego pose is used. false lpf_pitch_gain double gain of low-pass filter for pitch estimation 0.95 max_pitch_rad double max value of estimated pitch [rad] 0.1 min_pitch_rad double min value of estimated pitch [rad] -0.1","title":"Parameter description"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#state-transition","text":"Name Type Description Default value drive_state_stop_dist double The state will transit to DRIVE when the distance to the stop point is longer than drive_state_stop_dist + drive_state_offset_stop_dist [m] 0.5 drive_state_offset_stop_dist double The state will transit to DRIVE when the distance to the stop point is longer than drive_state_stop_dist + drive_state_offset_stop_dist [m] 1.0 stopping_state_stop_dist double The state will transit to STOPPING when the distance to the stop point is shorter than stopping_state_stop_dist [m] 0.5 stopped_state_entry_vel double threshold of the ego velocity in transition to the STOPPED state [m/s] 0.01 stopped_state_entry_acc double threshold of the ego acceleration in transition to the STOPPED state [m/s^2] 0.1 emergency_state_overshoot_stop_dist double If enable_overshoot_emergency is true and the ego is emergency_state_overshoot_stop_dist -meter ahead of the stop point, the state will transit to EMERGENCY. [m] 1.5 emergency_state_traj_trans_dev double If the ego's position is emergency_state_traj_tran_dev meter away from the nearest trajectory point, the state will transit to EMERGENCY. [m] 3.0 emergency_state_traj_rot_dev double If the ego's orientation is emergency_state_traj_rot_dev rad away from the nearest trajectory point orientation, the state will transit to EMERGENCY. [rad] 0.784","title":"State transition"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#drive-parameter","text":"Name Type Description Default value kp double p gain for longitudinal control 1.0 ki double i gain for longitudinal control 0.1 kd double d gain for longitudinal control 0.0 max_out double max value of PID's output acceleration during DRIVE state [m/s^2] 1.0 min_out double min value of PID's output acceleration during DRIVE state [m/s^2] -1.0 max_p_effort double max value of acceleration with p gain 1.0 min_p_effort double min value of acceleration with p gain -1.0 max_i_effort double max value of acceleration with i gain 0.3 min_i_effort double min value of acceleration with i gain -0.3 max_d_effort double max value of acceleration with d gain 0.0 min_d_effort double min value of acceleration with d gain 0.0 lpf_vel_error_gain double gain of low-pass filter for velocity error 0.9 current_vel_threshold_pid_integration double Velocity error is integrated for I-term only when the absolute value of current velocity is larger than this parameter. [m/s] 0.5 brake_keeping_acc double If enable_brake_keeping_before_stop is true, a certain acceleration is kept during DRIVE state before the ego stops [m/s^2] See Brake keeping . 0.2","title":"DRIVE Parameter"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#stopping-parameter-smooth-stop","text":"Smooth stop is enabled if enable_smooth_stop is true. In smooth stop, strong acceleration ( strong_acc ) will be output first to decrease the ego velocity. Then weak acceleration ( weak_acc ) will be output to stop smoothly by decreasing the ego jerk. If the ego does not stop in a certain time or some-meter over the stop point, weak acceleration to stop right ( weak_stop_acc ) now will be output. If the ego is still running, strong acceleration ( strong_stop_acc ) to stop right now will be output. Name Type Description Default value smooth_stop_max_strong_acc double max strong acceleration [m/s^2] -0.5 smooth_stop_min_strong_acc double min strong acceleration [m/s^2] -0.8 smooth_stop_weak_acc double weak acceleration [m/s^2] -0.3 smooth_stop_weak_stop_acc double weak acceleration to stop right now [m/s^2] -0.8 smooth_stop_strong_stop_acc double strong acceleration to be output when the ego is smooth_stop_strong_stop_dist -meter over the stop point. [m/s^2] -3.4 smooth_stop_max_fast_vel double max fast vel to judge the ego is running fast [m/s]. If the ego is running fast, strong acceleration will be output. 0.5 smooth_stop_min_running_vel double min ego velocity to judge if the ego is running or not [m/s] 0.01 smooth_stop_min_running_acc double min ego acceleration to judge if the ego is running or not [m/s^2] 0.01 smooth_stop_weak_stop_time double max time to output weak acceleration [s]. After this, strong acceleration will be output. 0.8 smooth_stop_weak_stop_dist double Weak acceleration will be output when the ego is smooth_stop_weak_stop_dist -meter before the stop point. [m] -0.3 smooth_stop_strong_stop_dist double Strong acceleration will be output when the ego is smooth_stop_strong_stop_dist -meter over the stop point. [m] -0.5","title":"STOPPING Parameter (smooth stop)"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#stopped-parameter","text":"Name Type Description Default value stopped_vel double target velocity in STOPPED state [m/s] 0.0 stopped_acc double target acceleration in STOPPED state [m/s^2] -3.4 stopped_jerk double target jerk in STOPPED state [m/s^3] -5.0","title":"STOPPED Parameter"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#emergency-parameter","text":"Name Type Description Default value emergency_vel double target velocity in EMERGENCY state [m/s] 0.0 emergency_acc double target acceleration in an EMERGENCY state [m/s^2] -5.0 emergency_jerk double target jerk in an EMERGENCY state [m/s^3] -3.0","title":"EMERGENCY Parameter"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#references-external-links","text":"","title":"References / External links"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#future-extensions-unimplemented-parts","text":"","title":"Future extensions / Unimplemented parts"},{"location":"control/trajectory_follower_nodes/design/longitudinal_controller-design/#related-issues","text":"https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/issues/1058 https://github.com/autowarefoundation/autoware.universe/issues/701","title":"Related issues"},{"location":"control/trajectory_follower_nodes/design/trajectory_follower-design/","text":"Trajectory Follower Nodes # Purpose # Generate control commands to follow a given Trajectory. Design # This functionality is decomposed into three nodes. lateral-controller-design : generates lateral control messages. longitudinal-controller-design : generates longitudinal control messages. latlon-muxer-design : combines the lateral and longitudinal control commands into a single control command. Core functionalities are implemented in the trajectory_follower package. @image html images/trajectory_follower-diagram.png \"Overview of the Trajectory Follower package\" Debugging # Debug information are published by the lateral and longitudinal controller using autoware_auto_msgs/Float32MultiArrayDiagnostic messages. A configuration file for PlotJuggler is provided in the config folder which, when loaded, allow to automatically subscribe and visualize information useful for debugging. In addition, the predicted MPC trajectory is published on topic output/lateral/predicted_trajectory and can be visualized in Rviz.","title":"Trajectory Follower Nodes"},{"location":"control/trajectory_follower_nodes/design/trajectory_follower-design/#trajectory-follower-nodes","text":"","title":"Trajectory Follower Nodes"},{"location":"control/trajectory_follower_nodes/design/trajectory_follower-design/#purpose","text":"Generate control commands to follow a given Trajectory.","title":"Purpose"},{"location":"control/trajectory_follower_nodes/design/trajectory_follower-design/#design","text":"This functionality is decomposed into three nodes. lateral-controller-design : generates lateral control messages. longitudinal-controller-design : generates longitudinal control messages. latlon-muxer-design : combines the lateral and longitudinal control commands into a single control command. Core functionalities are implemented in the trajectory_follower package. @image html images/trajectory_follower-diagram.png \"Overview of the Trajectory Follower package\"","title":"Design"},{"location":"control/trajectory_follower_nodes/design/trajectory_follower-design/#debugging","text":"Debug information are published by the lateral and longitudinal controller using autoware_auto_msgs/Float32MultiArrayDiagnostic messages. A configuration file for PlotJuggler is provided in the config folder which, when loaded, allow to automatically subscribe and visualize information useful for debugging. In addition, the predicted MPC trajectory is published on topic output/lateral/predicted_trajectory and can be visualized in Rviz.","title":"Debugging"},{"location":"control/vehicle_cmd_gate/","text":"vehicle_cmd_gate # Purpose # vehicle_cmd_gate is the package to get information from emergency handler, planning module, external controller, and send a msg to vehicle. Inputs / Outputs # Input # Name Type Description ~/input/steering autoware_auto_vehicle_msgs::msg::SteeringReport steering status ~/input/auto/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand command for lateral and longitudinal velocity from planning module ~/input/auto/turn_indicators_cmd autoware_auto_vehicle_msgs::msg::TurnIndicatorsCommand turn indicators command from planning module ~/input/auto/hazard_lights_cmd autoware_auto_vehicle_msgs::msg::HazardLightsCommand hazard lights command from planning module ~/input/auto/gear_cmd autoware_auto_vehicle_msgs::msg::GearCommand gear command from planning module ~/input/external/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand command for lateral and longitudinal velocity from external ~/input/external/turn_indicators_cmd autoware_auto_vehicle_msgs::msg::TurnIndicatorsCommand turn indicators command from external ~/input/external/hazard_lights_cmd autoware_auto_vehicle_msgs::msg::HazardLightsCommand hazard lights command from external ~/input/external/gear_cmd autoware_auto_vehicle_msgs::msg::GearCommand gear command from external ~/input/external_emergency_stop_heartbeat tier4_external_api_msgs::msg::Heartbeat heartbeat ~/input/gate_mode tier4_control_msgs::msg::GateMode gate mode (AUTO or EXTERNAL) ~/input/emergency_state autoware_auto_system_msgs::msg::EmergencyState used to detect the emergency situation of the vehicle ~/input/emergency/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand command for lateral and longitudinal velocity from emergency handler ~/input/emergency/hazard_lights_cmd autoware_auto_vehicle_msgs::msg::HazardLightsCommand hazard lights command from emergency handler ~/input/emergency/gear_cmd autoware_auto_vehicle_msgs::msg::GearCommand gear command from emergency handler ~/input/engage autoware_auto_vehicle_msgs::msg::Engage engage signal Output # Name Type Description ~/output/vehicle_cmd_emergency autoware_auto_system_msgs::msg::EmergencyState emergency state which was originally in vehicle command ~/output/command/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand command for lateral and longitudinal velocity to vehicle ~/output/command/turn_indicators_cmd autoware_auto_vehicle_msgs::msg::TurnIndicatorsCommand turn indicators command to vehicle ~/output/command/hazard_lights_cmd autoware_auto_vehicle_msgs::msg::HazardLightsCommand hazard lights command to vehicle ~/output/command/gear_cmd autoware_auto_vehicle_msgs::msg::GearCommand gear command to vehicle ~/output/gate_mode tier4_control_msgs::msg::GateMode gate mode (AUTO or EXTERNAL) ~/output/engage autoware_auto_vehicle_msgs::msg::Engage engage signal ~/output/external_emergency tier4_external_api_msgs::msg::Emergency external emergency signal Parameters # Parameter Type Description update_period double update period use_emergency_handling bool true when emergency handler is used use_external_emergency_stop bool true when external emergency stop information is used system_emergency_heartbeat_timeout double timeout for system emergency external_emergency_stop_heartbeat_timeout double timeout for external emergency stop_hold_acceleration double longitudinal acceleration cmd when vehicle should stop emergency_acceleration double longitudinal acceleration cmd when vehicle stop with emergency vel_lim double limit of longitudinal velocity lon_acc_lim double limit of longitudinal acceleration lon_jerk_lim double limit of longitudinal jerk lat_acc_lim double limit of lateral acceleration lat_jerk_lim double limit of lateral jerk Assumptions / Known limits # TBD.","title":"vehicle_cmd_gate"},{"location":"control/vehicle_cmd_gate/#vehicle_cmd_gate","text":"","title":"vehicle_cmd_gate"},{"location":"control/vehicle_cmd_gate/#purpose","text":"vehicle_cmd_gate is the package to get information from emergency handler, planning module, external controller, and send a msg to vehicle.","title":"Purpose"},{"location":"control/vehicle_cmd_gate/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"control/vehicle_cmd_gate/#input","text":"Name Type Description ~/input/steering autoware_auto_vehicle_msgs::msg::SteeringReport steering status ~/input/auto/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand command for lateral and longitudinal velocity from planning module ~/input/auto/turn_indicators_cmd autoware_auto_vehicle_msgs::msg::TurnIndicatorsCommand turn indicators command from planning module ~/input/auto/hazard_lights_cmd autoware_auto_vehicle_msgs::msg::HazardLightsCommand hazard lights command from planning module ~/input/auto/gear_cmd autoware_auto_vehicle_msgs::msg::GearCommand gear command from planning module ~/input/external/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand command for lateral and longitudinal velocity from external ~/input/external/turn_indicators_cmd autoware_auto_vehicle_msgs::msg::TurnIndicatorsCommand turn indicators command from external ~/input/external/hazard_lights_cmd autoware_auto_vehicle_msgs::msg::HazardLightsCommand hazard lights command from external ~/input/external/gear_cmd autoware_auto_vehicle_msgs::msg::GearCommand gear command from external ~/input/external_emergency_stop_heartbeat tier4_external_api_msgs::msg::Heartbeat heartbeat ~/input/gate_mode tier4_control_msgs::msg::GateMode gate mode (AUTO or EXTERNAL) ~/input/emergency_state autoware_auto_system_msgs::msg::EmergencyState used to detect the emergency situation of the vehicle ~/input/emergency/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand command for lateral and longitudinal velocity from emergency handler ~/input/emergency/hazard_lights_cmd autoware_auto_vehicle_msgs::msg::HazardLightsCommand hazard lights command from emergency handler ~/input/emergency/gear_cmd autoware_auto_vehicle_msgs::msg::GearCommand gear command from emergency handler ~/input/engage autoware_auto_vehicle_msgs::msg::Engage engage signal","title":"Input"},{"location":"control/vehicle_cmd_gate/#output","text":"Name Type Description ~/output/vehicle_cmd_emergency autoware_auto_system_msgs::msg::EmergencyState emergency state which was originally in vehicle command ~/output/command/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand command for lateral and longitudinal velocity to vehicle ~/output/command/turn_indicators_cmd autoware_auto_vehicle_msgs::msg::TurnIndicatorsCommand turn indicators command to vehicle ~/output/command/hazard_lights_cmd autoware_auto_vehicle_msgs::msg::HazardLightsCommand hazard lights command to vehicle ~/output/command/gear_cmd autoware_auto_vehicle_msgs::msg::GearCommand gear command to vehicle ~/output/gate_mode tier4_control_msgs::msg::GateMode gate mode (AUTO or EXTERNAL) ~/output/engage autoware_auto_vehicle_msgs::msg::Engage engage signal ~/output/external_emergency tier4_external_api_msgs::msg::Emergency external emergency signal","title":"Output"},{"location":"control/vehicle_cmd_gate/#parameters","text":"Parameter Type Description update_period double update period use_emergency_handling bool true when emergency handler is used use_external_emergency_stop bool true when external emergency stop information is used system_emergency_heartbeat_timeout double timeout for system emergency external_emergency_stop_heartbeat_timeout double timeout for external emergency stop_hold_acceleration double longitudinal acceleration cmd when vehicle should stop emergency_acceleration double longitudinal acceleration cmd when vehicle stop with emergency vel_lim double limit of longitudinal velocity lon_acc_lim double limit of longitudinal acceleration lon_jerk_lim double limit of longitudinal jerk lat_acc_lim double limit of lateral acceleration lat_jerk_lim double limit of lateral jerk","title":"Parameters"},{"location":"control/vehicle_cmd_gate/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"launch/tier4_autoware_api_launch/","text":"tier4_autoware_api_launch # Description # This package contains launch files that run nodes to convert Autoware internal topics into consistent API used by external software (e.g., fleet management system, simulator). Package Dependencies # Please see <exec_depend> in package.xml . Usage # You can include as follows in *.launch.xml to use control.launch.py . <include file= \"$(find-pkg-share tier4_autoware_api_launch)/launch/autoware_api.launch.xml\" > <arg name= \"init_simulator_pose\" value= \"true\" /> <arg name= \"init_localization_pose\" value= \"false\" /> </include> Notes # For reducing processing load, we use the Component feature in ROS2 (similar to Nodelet in ROS1 )","title":"tier4_autoware_api_launch"},{"location":"launch/tier4_autoware_api_launch/#tier4_autoware_api_launch","text":"","title":"tier4_autoware_api_launch"},{"location":"launch/tier4_autoware_api_launch/#description","text":"This package contains launch files that run nodes to convert Autoware internal topics into consistent API used by external software (e.g., fleet management system, simulator).","title":"Description"},{"location":"launch/tier4_autoware_api_launch/#package-dependencies","text":"Please see <exec_depend> in package.xml .","title":"Package Dependencies"},{"location":"launch/tier4_autoware_api_launch/#usage","text":"You can include as follows in *.launch.xml to use control.launch.py . <include file= \"$(find-pkg-share tier4_autoware_api_launch)/launch/autoware_api.launch.xml\" > <arg name= \"init_simulator_pose\" value= \"true\" /> <arg name= \"init_localization_pose\" value= \"false\" /> </include>","title":"Usage"},{"location":"launch/tier4_autoware_api_launch/#notes","text":"For reducing processing load, we use the Component feature in ROS2 (similar to Nodelet in ROS1 )","title":"Notes"},{"location":"launch/tier4_control_launch/","text":"tier4_control_launch # Structure # Package Dependencies # Please see <exec_depend> in package.xml . Usage # You can include as follows in *.launch.xml to use control.launch.py . <include file= \"$(find-pkg-share tier4_control_launch)/launch/control.launch.py\" > <!-- options for lateral_controller_mode: mpc_follower, pure_pursuit --> <arg name= \"lateral_controller_mode\" value= \"mpc_follower\" /> </include> Notes # For reducing processing load, we use the Component feature in ROS2 (similar to Nodelet in ROS1 )","title":"tier4_control_launch"},{"location":"launch/tier4_control_launch/#tier4_control_launch","text":"","title":"tier4_control_launch"},{"location":"launch/tier4_control_launch/#structure","text":"","title":"Structure"},{"location":"launch/tier4_control_launch/#package-dependencies","text":"Please see <exec_depend> in package.xml .","title":"Package Dependencies"},{"location":"launch/tier4_control_launch/#usage","text":"You can include as follows in *.launch.xml to use control.launch.py . <include file= \"$(find-pkg-share tier4_control_launch)/launch/control.launch.py\" > <!-- options for lateral_controller_mode: mpc_follower, pure_pursuit --> <arg name= \"lateral_controller_mode\" value= \"mpc_follower\" /> </include>","title":"Usage"},{"location":"launch/tier4_control_launch/#notes","text":"For reducing processing load, we use the Component feature in ROS2 (similar to Nodelet in ROS1 )","title":"Notes"},{"location":"launch/tier4_integration_launch/","text":"tier4_integration_launch # Structure # Package Dependencies # autoware_launch Notes # This package is only used for continuous integration.","title":"tier4_integration_launch"},{"location":"launch/tier4_integration_launch/#tier4_integration_launch","text":"","title":"tier4_integration_launch"},{"location":"launch/tier4_integration_launch/#structure","text":"","title":"Structure"},{"location":"launch/tier4_integration_launch/#package-dependencies","text":"autoware_launch","title":"Package Dependencies"},{"location":"launch/tier4_integration_launch/#notes","text":"This package is only used for continuous integration.","title":"Notes"},{"location":"launch/tier4_localization_launch/","text":"tier4_localization_launch # Structure # Package Dependencies # Please see <exec_depend> in package.xml . Usage # You can include as follows in *.launch.xml to use localization.launch.xml . <include file= \"$(find-pkg-share tier4_localization_launch)/launch/localization.launch.xml\" > </include> Notes # There are some param.yaml files in config directory. ndt_scan_matcher.param.yaml","title":"tier4_localization_launch"},{"location":"launch/tier4_localization_launch/#tier4_localization_launch","text":"","title":"tier4_localization_launch"},{"location":"launch/tier4_localization_launch/#structure","text":"","title":"Structure"},{"location":"launch/tier4_localization_launch/#package-dependencies","text":"Please see <exec_depend> in package.xml .","title":"Package Dependencies"},{"location":"launch/tier4_localization_launch/#usage","text":"You can include as follows in *.launch.xml to use localization.launch.xml . <include file= \"$(find-pkg-share tier4_localization_launch)/launch/localization.launch.xml\" > </include>","title":"Usage"},{"location":"launch/tier4_localization_launch/#notes","text":"There are some param.yaml files in config directory. ndt_scan_matcher.param.yaml","title":"Notes"},{"location":"launch/tier4_map_launch/","text":"tier4_map_launch # Structure # Package Dependencies # Please see <exec_depend> in package.xml . Usage # You can include as follows in *.launch.xml to use map.launch.py . <arg name= \"map_path\" description= \"point cloud and lanelet2 map directory path\" /> <arg name= \"lanelet2_map_file\" default= \"lanelet2_map.osm\" description= \"lanelet2 map file name\" /> <arg name= \"pointcloud_map_file\" default= \"pointcloud_map.pcd\" description= \"pointcloud map file name\" /> <include file= \"$(find-pkg-share tier4_map_launch)/launch/map.launch.py\" > <arg name= \"lanelet2_map_path\" value= \"$(var map_path)/$(var lanelet2_map_file)\" /> <arg name= \"pointcloud_map_path\" value= \"$(var map_path)/$(var pointcloud_map_file)\" /> </include> Notes # For reducing processing load, we use the Component feature in ROS2 (similar to Nodelet in ROS1 )","title":"tier4_map_launch"},{"location":"launch/tier4_map_launch/#tier4_map_launch","text":"","title":"tier4_map_launch"},{"location":"launch/tier4_map_launch/#structure","text":"","title":"Structure"},{"location":"launch/tier4_map_launch/#package-dependencies","text":"Please see <exec_depend> in package.xml .","title":"Package Dependencies"},{"location":"launch/tier4_map_launch/#usage","text":"You can include as follows in *.launch.xml to use map.launch.py . <arg name= \"map_path\" description= \"point cloud and lanelet2 map directory path\" /> <arg name= \"lanelet2_map_file\" default= \"lanelet2_map.osm\" description= \"lanelet2 map file name\" /> <arg name= \"pointcloud_map_file\" default= \"pointcloud_map.pcd\" description= \"pointcloud map file name\" /> <include file= \"$(find-pkg-share tier4_map_launch)/launch/map.launch.py\" > <arg name= \"lanelet2_map_path\" value= \"$(var map_path)/$(var lanelet2_map_file)\" /> <arg name= \"pointcloud_map_path\" value= \"$(var map_path)/$(var pointcloud_map_file)\" /> </include>","title":"Usage"},{"location":"launch/tier4_map_launch/#notes","text":"For reducing processing load, we use the Component feature in ROS2 (similar to Nodelet in ROS1 )","title":"Notes"},{"location":"launch/tier4_perception_launch/","text":"tier4_perception_launch # Structure # Package Dependencies # Please see <exec_depend> in package.xml . Usage # You can include as follows in *.launch.xml to use perception.launch.xml . <include file= \"$(find-pkg-share tier4_perception_launch)/launch/perception.launch.xml\" > <!-- options for mode: camera_lidar_fusion, lidar, camera --> <arg name= \"mode\" value= \"lidar\" /> </include>","title":"tier4_perception_launch"},{"location":"launch/tier4_perception_launch/#tier4_perception_launch","text":"","title":"tier4_perception_launch"},{"location":"launch/tier4_perception_launch/#structure","text":"","title":"Structure"},{"location":"launch/tier4_perception_launch/#package-dependencies","text":"Please see <exec_depend> in package.xml .","title":"Package Dependencies"},{"location":"launch/tier4_perception_launch/#usage","text":"You can include as follows in *.launch.xml to use perception.launch.xml . <include file= \"$(find-pkg-share tier4_perception_launch)/launch/perception.launch.xml\" > <!-- options for mode: camera_lidar_fusion, lidar, camera --> <arg name= \"mode\" value= \"lidar\" /> </include>","title":"Usage"},{"location":"launch/tier4_planning_launch/","text":"tier4_planning_launch # Structure # Package Dependencies # Please see <exec_depend> in package.xml . Usage # <include file= \"$(find-pkg-share tier4_planning_launch)/launch/planning.launch.xml\" > </include>","title":"tier4_planning_launch"},{"location":"launch/tier4_planning_launch/#tier4_planning_launch","text":"","title":"tier4_planning_launch"},{"location":"launch/tier4_planning_launch/#structure","text":"","title":"Structure"},{"location":"launch/tier4_planning_launch/#package-dependencies","text":"Please see <exec_depend> in package.xml .","title":"Package Dependencies"},{"location":"launch/tier4_planning_launch/#usage","text":"<include file= \"$(find-pkg-share tier4_planning_launch)/launch/planning.launch.xml\" > </include>","title":"Usage"},{"location":"launch/tier4_sensing_launch/","text":"tier4_sensing_launch # Structure # Package Dependencies # Please see <exec_depend> in package.xml . Usage # You can include as follows in *.launch.xml to use sensing.launch.xml . <include file= \"$(find-pkg-share tier4_sensing_launch)/launch/sensing.launch.xml\" > <arg name= \"launch_driver\" value= \"true\" /> <arg name= \"sensor_model\" value= \"$(var sensor_model)\" /> <arg name= \"vehicle_param_file\" value= \"$(find-pkg-share $(var vehicle_model)_description)/config/vehicle_info.param.yaml\" /> <arg name= \"vehicle_mirror_param_file\" value= \"$(find-pkg-share $(var vehicle_model)_description)/config/mirror.param.yaml\" /> </include> Launch Directory Structure # This package finds sensor settings of specified sensor model in launch . launch/ \u251c\u2500\u2500 aip_x1 # Sensor model name \u2502 \u251c\u2500\u2500 camera.launch.xml # Camera \u2502 \u251c\u2500\u2500 gnss.launch.xml # GNSS \u2502 \u251c\u2500\u2500 imu.launch.xml # IMU \u2502 \u251c\u2500\u2500 lidar.launch.xml # LiDAR \u2502 \u2514\u2500\u2500 pointcloud_preprocessor.launch.py # for preprocessing pointcloud ... Notes # This package finds settings with variables. ex.) <include file= \"$(find-pkg-share tier4_sensing_launch)/launch/$(var sensor_model)/lidar.launch.xml\" >","title":"tier4_sensing_launch"},{"location":"launch/tier4_sensing_launch/#tier4_sensing_launch","text":"","title":"tier4_sensing_launch"},{"location":"launch/tier4_sensing_launch/#structure","text":"","title":"Structure"},{"location":"launch/tier4_sensing_launch/#package-dependencies","text":"Please see <exec_depend> in package.xml .","title":"Package Dependencies"},{"location":"launch/tier4_sensing_launch/#usage","text":"You can include as follows in *.launch.xml to use sensing.launch.xml . <include file= \"$(find-pkg-share tier4_sensing_launch)/launch/sensing.launch.xml\" > <arg name= \"launch_driver\" value= \"true\" /> <arg name= \"sensor_model\" value= \"$(var sensor_model)\" /> <arg name= \"vehicle_param_file\" value= \"$(find-pkg-share $(var vehicle_model)_description)/config/vehicle_info.param.yaml\" /> <arg name= \"vehicle_mirror_param_file\" value= \"$(find-pkg-share $(var vehicle_model)_description)/config/mirror.param.yaml\" /> </include>","title":"Usage"},{"location":"launch/tier4_sensing_launch/#launch-directory-structure","text":"This package finds sensor settings of specified sensor model in launch . launch/ \u251c\u2500\u2500 aip_x1 # Sensor model name \u2502 \u251c\u2500\u2500 camera.launch.xml # Camera \u2502 \u251c\u2500\u2500 gnss.launch.xml # GNSS \u2502 \u251c\u2500\u2500 imu.launch.xml # IMU \u2502 \u251c\u2500\u2500 lidar.launch.xml # LiDAR \u2502 \u2514\u2500\u2500 pointcloud_preprocessor.launch.py # for preprocessing pointcloud ...","title":"Launch Directory Structure"},{"location":"launch/tier4_sensing_launch/#notes","text":"This package finds settings with variables. ex.) <include file= \"$(find-pkg-share tier4_sensing_launch)/launch/$(var sensor_model)/lidar.launch.xml\" >","title":"Notes"},{"location":"launch/tier4_simulator_launch/","text":"tier4_simulator_launch # Structure # Package Dependencies # Please see <exec_depend> in package.xml . Usage # <include file= \"$(find-pkg-share tier4_simulator_launch)/launch/simulator.launch.xml\" > <arg name= \"vehicle_info_param_file\" value= \"VEHICLE_INFO_PARAM_FILE\" /> <arg name= \"vehicle_model\" value= \"VEHICLE_MODEL\" /> </include> The simulator model used in simple_planning_simulator is loaded from \"config/simulator_model.param.yaml\" in the \" VEHICLE_MODEL _description\" package.","title":"tier4_simulator_launch"},{"location":"launch/tier4_simulator_launch/#tier4_simulator_launch","text":"","title":"tier4_simulator_launch"},{"location":"launch/tier4_simulator_launch/#structure","text":"","title":"Structure"},{"location":"launch/tier4_simulator_launch/#package-dependencies","text":"Please see <exec_depend> in package.xml .","title":"Package Dependencies"},{"location":"launch/tier4_simulator_launch/#usage","text":"<include file= \"$(find-pkg-share tier4_simulator_launch)/launch/simulator.launch.xml\" > <arg name= \"vehicle_info_param_file\" value= \"VEHICLE_INFO_PARAM_FILE\" /> <arg name= \"vehicle_model\" value= \"VEHICLE_MODEL\" /> </include> The simulator model used in simple_planning_simulator is loaded from \"config/simulator_model.param.yaml\" in the \" VEHICLE_MODEL _description\" package.","title":"Usage"},{"location":"launch/tier4_system_launch/","text":"tier4_system_launch # Structure # Package Dependencies # Please see <exec_depend> in package.xml . Usage # <include file= \"$(find-pkg-share tier4_system_launch)/launch/system.launch.xml\" > <arg name= \"run_mode\" value= \"online\" /> <arg name= \"sensor_model\" value= \"SENSOR_MODEL\" /> </include> The sensing configuration parameters used in system_error_monitor are loaded from \"config/diagnostic_aggregator/sensor_kit.param.yaml\" in the \" SENSOR_MODEL _description\" package.","title":"tier4_system_launch"},{"location":"launch/tier4_system_launch/#tier4_system_launch","text":"","title":"tier4_system_launch"},{"location":"launch/tier4_system_launch/#structure","text":"","title":"Structure"},{"location":"launch/tier4_system_launch/#package-dependencies","text":"Please see <exec_depend> in package.xml .","title":"Package Dependencies"},{"location":"launch/tier4_system_launch/#usage","text":"<include file= \"$(find-pkg-share tier4_system_launch)/launch/system.launch.xml\" > <arg name= \"run_mode\" value= \"online\" /> <arg name= \"sensor_model\" value= \"SENSOR_MODEL\" /> </include> The sensing configuration parameters used in system_error_monitor are loaded from \"config/diagnostic_aggregator/sensor_kit.param.yaml\" in the \" SENSOR_MODEL _description\" package.","title":"Usage"},{"location":"launch/tier4_vehicle_launch/","text":"tier4_vehicle_launch # Structure # Package Dependencies # Please see <exec_depend> in package.xml . Usage # You can include as follows in *.launch.xml to use vehicle.launch.xml . <arg name= \"vehicle_model\" default= \"sample_vehicle\" description= \"vehicle model name\" /> <arg name= \"sensor_model\" default= \"sample_sensor_kit\" description= \"sensor model name\" /> <include file= \"$(find-pkg-share tier4_vehicle_launch)/launch/vehicle.launch.xml\" > <arg name= \"vehicle_model\" value= \"$(var vehicle_model)\" /> <arg name= \"sensor_model\" value= \"$(var sensor_model)\" /> </include> Notes # This package finds some external packages and settings with variables and package names. ex.) <let name= \"vehicle_model_pkg\" value= \"$(find-pkg-share $(var vehicle_model)_description)\" /> <arg name= \"config_dir\" default= \"$(find-pkg-share individual_params)/config/$(var vehicle_id)/$(var sensor_model)\" /> vehicle.xacro # Arguments # Name Type Description Default sensor_model String sensor model name \"\" vehicle_model String vehicle model name \"\" Usage # You can write as follows in *.launch.xml . <arg name= \"vehicle_model\" default= \"sample_vehicle\" description= \"vehicle model name\" /> <arg name= \"sensor_model\" default= \"sample_sensor_kit\" description= \"sensor model name\" /> <arg name= \"model\" default= \"$(find-pkg-share tier4_vehicle_launch)/urdf/vehicle.xacro\" /> <node name= \"robot_state_publisher\" pkg= \"robot_state_publisher\" exec= \"robot_state_publisher\" > <param name= \"robot_description\" value= \"$(command 'xacro $(var model) vehicle_model:=$(var vehicle_model) sensor_model:=$(var sensor_model)')\" /> </node>","title":"tier4_vehicle_launch"},{"location":"launch/tier4_vehicle_launch/#tier4_vehicle_launch","text":"","title":"tier4_vehicle_launch"},{"location":"launch/tier4_vehicle_launch/#structure","text":"","title":"Structure"},{"location":"launch/tier4_vehicle_launch/#package-dependencies","text":"Please see <exec_depend> in package.xml .","title":"Package Dependencies"},{"location":"launch/tier4_vehicle_launch/#usage","text":"You can include as follows in *.launch.xml to use vehicle.launch.xml . <arg name= \"vehicle_model\" default= \"sample_vehicle\" description= \"vehicle model name\" /> <arg name= \"sensor_model\" default= \"sample_sensor_kit\" description= \"sensor model name\" /> <include file= \"$(find-pkg-share tier4_vehicle_launch)/launch/vehicle.launch.xml\" > <arg name= \"vehicle_model\" value= \"$(var vehicle_model)\" /> <arg name= \"sensor_model\" value= \"$(var sensor_model)\" /> </include>","title":"Usage"},{"location":"launch/tier4_vehicle_launch/#notes","text":"This package finds some external packages and settings with variables and package names. ex.) <let name= \"vehicle_model_pkg\" value= \"$(find-pkg-share $(var vehicle_model)_description)\" /> <arg name= \"config_dir\" default= \"$(find-pkg-share individual_params)/config/$(var vehicle_id)/$(var sensor_model)\" />","title":"Notes"},{"location":"launch/tier4_vehicle_launch/#vehiclexacro","text":"","title":"vehicle.xacro"},{"location":"launch/tier4_vehicle_launch/#arguments","text":"Name Type Description Default sensor_model String sensor model name \"\" vehicle_model String vehicle model name \"\"","title":"Arguments"},{"location":"launch/tier4_vehicle_launch/#usage_1","text":"You can write as follows in *.launch.xml . <arg name= \"vehicle_model\" default= \"sample_vehicle\" description= \"vehicle model name\" /> <arg name= \"sensor_model\" default= \"sample_sensor_kit\" description= \"sensor model name\" /> <arg name= \"model\" default= \"$(find-pkg-share tier4_vehicle_launch)/urdf/vehicle.xacro\" /> <node name= \"robot_state_publisher\" pkg= \"robot_state_publisher\" exec= \"robot_state_publisher\" > <param name= \"robot_description\" value= \"$(command 'xacro $(var model) vehicle_model:=$(var vehicle_model) sensor_model:=$(var sensor_model)')\" /> </node>","title":"Usage"},{"location":"localization/ekf_localizer/","text":"Overview # The Extend Kalman Filter Localizer estimates robust and less noisy robot pose and twist by integrating the 2D vehicle dynamics model with input ego-pose and ego-twist messages. The algorithm is designed especially for fast moving robot such as autonomous driving system. Flowchart # The overall flowchart of the ekf_localizer is described below. Features # This package includes the following features: Time delay compensation for input messages, which enables proper integration of input information with varying time delay. This is important especially for high speed moving robot, such as autonomous driving vehicle. (see following figure). Automatic estimation of yaw bias prevents modeling errors caused by sensor mounting angle errors, which can improve estimation accuracy. Mahalanobis distance gate enables probabilistic outlier detection to determine which inputs should be used or ignored. Smooth update , the Kalman Filter measurement update is typically performed when a measurement is obtained, but it can cause large changes in the estimated value especially for low frequency measurements. Since the algorithm can consider the measurement time, the measurement data can be divided into multiple pieces and integrated smoothly while maintaining consistency (see following figure). Launch # The ekf_localizer starts with the default parameters with the following command. roslaunch ekf_localizer ekf_localizer.launch The parameters and input topic names can be set in the ekf_localizer.launch file. Node # Subscribed Topics # measured_pose_with_covariance (geometry_msgs/PoseWithCovarianceStamped) Input pose source with measurement covariance matrix. measured_twist_with_covariance (geometry_msgs/TwistWithCovarianceStamped) Input twist source with measurement covariance matrix. initialpose (geometry_msgs/PoseWithCovarianceStamped) Initial pose for EKF. The estimated pose is initialized with zeros at start. It is initialized with this message whenever published. Published Topics # ekf_odom (nav_msgs/Odometry) Estimated odometry. ekf_pose (geometry_msgs/PoseStamped) Estimated pose. ekf_pose_with_covariance (geometry_msgs/PoseWithCovarianceStamped) Estimated pose with covariance. ekf_pose_with_covariance (geometry_msgs/PoseStamped) Estimated pose without yawbias effect. ekf_pose_with_covariance_without_yawbias (geometry_msgs/PoseWithCovarianceStamped) Estimated pose with covariance without yawbias effect. ekf_twist (geometry_msgs/TwistStamped) Estimated twist. ekf_twist_with_covariance (geometry_msgs/TwistWithCovarianceStamped) Estimated twist with covariance. Published TF # base_link TF from \"map\" coordinate to estimated pose. Functions # Predict # The current robot state is predicted from previously estimated data using a given prediction model. This calculation is called at constant interval ( predict_frequency [Hz] ). The prediction equation is described at the end of this page. Measurement Update # Before update, the Mahalanobis distance is calculated between the measured input and the predicted state, the measurement update is not performed for inputs where the Mahalanobis distance exceeds the given threshold. The predicted state is updated with the latest measured inputs, measured_pose and measured_twist. The updates are performed with the same frequency as prediction, usually at a high frequency, in order to enable smooth state estimation. Parameter description # The parameters are set in launch/ekf_localizer.launch . For Node # Name Type Description Default value show_debug_info bool Flag to display debug info false predict_frequency double Frequency for filtering and publishing [Hz] 50.0 tf_rate double Frequency for tf broadcasting [Hz] 10.0 extend_state_step int Max delay step which can be dealt with in EKF. Large number increases computational cost. 50 enable_yaw_bias_estimation bool Flag to enable yaw bias estimation true For pose measurement # Name Type Description Default value pose_additional_delay double Additional delay time for pose measurement [s] 0.0 pose_measure_uncertainty_time double Measured time uncertainty used for covariance calculation [s] 0.01 pose_rate double Approximated input pose rate used for covariance calculation [Hz] 10.0 pose_gate_dist double Limit of Mahalanobis distance used for outliers detection 10000.0 For twist measurement # Name Type Description Default value twist_additional_delay double Additional delay time for twist [s] 0.0 twist_rate double Approximated input twist rate used for covariance calculation [Hz] 10.0 twist_gate_dist double Limit of Mahalanobis distance used for outliers detection 10000.0 For process noise # Name Type Description Default value proc_stddev_vx_c double Standard deviation of process noise in time differentiation expression of linear velocity x, noise for d_vx = 0 2.0 proc_stddev_wz_c double Standard deviation of process noise in time differentiation expression of angular velocity z, noise for d_wz = 0 0.2 proc_stddev_yaw_c double Standard deviation of process noise in time differentiation expression of yaw, noise for d_yaw = omega 0.005 proc_stddev_yaw_bias_c double Standard deviation of process noise in time differentiation expression of yaw_bias, noise for d_yaw_bias = 0 0.001 note: process noise for position x & y are calculated automatically from nonlinear dynamics. How to turn EKF parameters # 0. Preliminaries # Check header time in pose and twist message is set to sensor time appropriately, because time delay is calculated from this value. If it is difficult to set appropriate time due to timer synchronization problem, use twist_additional_delay and pose_additional_delay to correct the time. Check the relation between measurement pose and twist is appropriate (whether the derivative of pose has similar value to twist). This discrepancy is caused mainly by unit error (such as confusing radian/degree) or bias noise, and it causes large estimation errors. 1. Set sensor parameters # Set sensor-rate and standard-deviation from the basic information of the sensor. The pose_measure_uncertainty_time is for uncertainty of the header timestamp data. pose_measure_uncertainty_time pose_rate twist_rate 2. Set process model parameters # proc_stddev_vx_c : set to maximum linear acceleration proc_stddev_wz_c : set to maximum angular acceleration proc_stddev_yaw_c : This parameter describes the correlation between the yaw and yaw-rate. Large value means the change in yaw does not correlate to the estimated yaw-rate. If this is set to 0, it means the change in estimate yaw is equal to yaw-rate. Usually this should be set to 0. proc_stddev_yaw_bias_c : This parameter is the standard deviation for the rate of change in yaw bias. In most cases, yaw bias is constant, so it can be very small, but must be non-zero. Kalman Filter Model # kinematics model in update function # where b_k is the yaw-bias. time delay model # The measurement time delay is handled by an augmented states [1] (See, Section 7.3 FIXED-LAG SMOOTHING). Note that, although the dimension gets larger, since the analytical expansion can be applied based on the specific structures of the augmented states, the computational complexity does not significantly change. Test Result with Autoware NDT # reference # [1] Anderson, B. D. O., & Moore, J. B. (1979). Optimal filtering. Englewood Cliffs, NJ: Prentice-Hall.","title":"Overview"},{"location":"localization/ekf_localizer/#overview","text":"The Extend Kalman Filter Localizer estimates robust and less noisy robot pose and twist by integrating the 2D vehicle dynamics model with input ego-pose and ego-twist messages. The algorithm is designed especially for fast moving robot such as autonomous driving system.","title":"Overview"},{"location":"localization/ekf_localizer/#flowchart","text":"The overall flowchart of the ekf_localizer is described below.","title":"Flowchart"},{"location":"localization/ekf_localizer/#features","text":"This package includes the following features: Time delay compensation for input messages, which enables proper integration of input information with varying time delay. This is important especially for high speed moving robot, such as autonomous driving vehicle. (see following figure). Automatic estimation of yaw bias prevents modeling errors caused by sensor mounting angle errors, which can improve estimation accuracy. Mahalanobis distance gate enables probabilistic outlier detection to determine which inputs should be used or ignored. Smooth update , the Kalman Filter measurement update is typically performed when a measurement is obtained, but it can cause large changes in the estimated value especially for low frequency measurements. Since the algorithm can consider the measurement time, the measurement data can be divided into multiple pieces and integrated smoothly while maintaining consistency (see following figure).","title":"Features"},{"location":"localization/ekf_localizer/#launch","text":"The ekf_localizer starts with the default parameters with the following command. roslaunch ekf_localizer ekf_localizer.launch The parameters and input topic names can be set in the ekf_localizer.launch file.","title":"Launch"},{"location":"localization/ekf_localizer/#node","text":"","title":"Node"},{"location":"localization/ekf_localizer/#subscribed-topics","text":"measured_pose_with_covariance (geometry_msgs/PoseWithCovarianceStamped) Input pose source with measurement covariance matrix. measured_twist_with_covariance (geometry_msgs/TwistWithCovarianceStamped) Input twist source with measurement covariance matrix. initialpose (geometry_msgs/PoseWithCovarianceStamped) Initial pose for EKF. The estimated pose is initialized with zeros at start. It is initialized with this message whenever published.","title":"Subscribed Topics"},{"location":"localization/ekf_localizer/#published-topics","text":"ekf_odom (nav_msgs/Odometry) Estimated odometry. ekf_pose (geometry_msgs/PoseStamped) Estimated pose. ekf_pose_with_covariance (geometry_msgs/PoseWithCovarianceStamped) Estimated pose with covariance. ekf_pose_with_covariance (geometry_msgs/PoseStamped) Estimated pose without yawbias effect. ekf_pose_with_covariance_without_yawbias (geometry_msgs/PoseWithCovarianceStamped) Estimated pose with covariance without yawbias effect. ekf_twist (geometry_msgs/TwistStamped) Estimated twist. ekf_twist_with_covariance (geometry_msgs/TwistWithCovarianceStamped) Estimated twist with covariance.","title":"Published Topics"},{"location":"localization/ekf_localizer/#published-tf","text":"base_link TF from \"map\" coordinate to estimated pose.","title":"Published TF"},{"location":"localization/ekf_localizer/#functions","text":"","title":"Functions"},{"location":"localization/ekf_localizer/#predict","text":"The current robot state is predicted from previously estimated data using a given prediction model. This calculation is called at constant interval ( predict_frequency [Hz] ). The prediction equation is described at the end of this page.","title":"Predict"},{"location":"localization/ekf_localizer/#measurement-update","text":"Before update, the Mahalanobis distance is calculated between the measured input and the predicted state, the measurement update is not performed for inputs where the Mahalanobis distance exceeds the given threshold. The predicted state is updated with the latest measured inputs, measured_pose and measured_twist. The updates are performed with the same frequency as prediction, usually at a high frequency, in order to enable smooth state estimation.","title":"Measurement Update"},{"location":"localization/ekf_localizer/#parameter-description","text":"The parameters are set in launch/ekf_localizer.launch .","title":"Parameter description"},{"location":"localization/ekf_localizer/#for-node","text":"Name Type Description Default value show_debug_info bool Flag to display debug info false predict_frequency double Frequency for filtering and publishing [Hz] 50.0 tf_rate double Frequency for tf broadcasting [Hz] 10.0 extend_state_step int Max delay step which can be dealt with in EKF. Large number increases computational cost. 50 enable_yaw_bias_estimation bool Flag to enable yaw bias estimation true","title":"For Node"},{"location":"localization/ekf_localizer/#for-pose-measurement","text":"Name Type Description Default value pose_additional_delay double Additional delay time for pose measurement [s] 0.0 pose_measure_uncertainty_time double Measured time uncertainty used for covariance calculation [s] 0.01 pose_rate double Approximated input pose rate used for covariance calculation [Hz] 10.0 pose_gate_dist double Limit of Mahalanobis distance used for outliers detection 10000.0","title":"For pose measurement"},{"location":"localization/ekf_localizer/#for-twist-measurement","text":"Name Type Description Default value twist_additional_delay double Additional delay time for twist [s] 0.0 twist_rate double Approximated input twist rate used for covariance calculation [Hz] 10.0 twist_gate_dist double Limit of Mahalanobis distance used for outliers detection 10000.0","title":"For twist measurement"},{"location":"localization/ekf_localizer/#for-process-noise","text":"Name Type Description Default value proc_stddev_vx_c double Standard deviation of process noise in time differentiation expression of linear velocity x, noise for d_vx = 0 2.0 proc_stddev_wz_c double Standard deviation of process noise in time differentiation expression of angular velocity z, noise for d_wz = 0 0.2 proc_stddev_yaw_c double Standard deviation of process noise in time differentiation expression of yaw, noise for d_yaw = omega 0.005 proc_stddev_yaw_bias_c double Standard deviation of process noise in time differentiation expression of yaw_bias, noise for d_yaw_bias = 0 0.001 note: process noise for position x & y are calculated automatically from nonlinear dynamics.","title":"For process noise"},{"location":"localization/ekf_localizer/#how-to-turn-ekf-parameters","text":"","title":"How to turn EKF parameters"},{"location":"localization/ekf_localizer/#0-preliminaries","text":"Check header time in pose and twist message is set to sensor time appropriately, because time delay is calculated from this value. If it is difficult to set appropriate time due to timer synchronization problem, use twist_additional_delay and pose_additional_delay to correct the time. Check the relation between measurement pose and twist is appropriate (whether the derivative of pose has similar value to twist). This discrepancy is caused mainly by unit error (such as confusing radian/degree) or bias noise, and it causes large estimation errors.","title":"0. Preliminaries"},{"location":"localization/ekf_localizer/#1-set-sensor-parameters","text":"Set sensor-rate and standard-deviation from the basic information of the sensor. The pose_measure_uncertainty_time is for uncertainty of the header timestamp data. pose_measure_uncertainty_time pose_rate twist_rate","title":"1. Set sensor parameters"},{"location":"localization/ekf_localizer/#2-set-process-model-parameters","text":"proc_stddev_vx_c : set to maximum linear acceleration proc_stddev_wz_c : set to maximum angular acceleration proc_stddev_yaw_c : This parameter describes the correlation between the yaw and yaw-rate. Large value means the change in yaw does not correlate to the estimated yaw-rate. If this is set to 0, it means the change in estimate yaw is equal to yaw-rate. Usually this should be set to 0. proc_stddev_yaw_bias_c : This parameter is the standard deviation for the rate of change in yaw bias. In most cases, yaw bias is constant, so it can be very small, but must be non-zero.","title":"2. Set process model parameters"},{"location":"localization/ekf_localizer/#kalman-filter-model","text":"","title":"Kalman Filter Model"},{"location":"localization/ekf_localizer/#kinematics-model-in-update-function","text":"where b_k is the yaw-bias.","title":"kinematics model in update function"},{"location":"localization/ekf_localizer/#time-delay-model","text":"The measurement time delay is handled by an augmented states [1] (See, Section 7.3 FIXED-LAG SMOOTHING). Note that, although the dimension gets larger, since the analytical expansion can be applied based on the specific structures of the augmented states, the computational complexity does not significantly change.","title":"time delay model"},{"location":"localization/ekf_localizer/#test-result-with-autoware-ndt","text":"","title":"Test Result with Autoware NDT"},{"location":"localization/ekf_localizer/#reference","text":"[1] Anderson, B. D. O., & Moore, J. B. (1979). Optimal filtering. Englewood Cliffs, NJ: Prentice-Hall.","title":"reference"},{"location":"localization/gyro_odometer/","text":"gyro_odometer # Purpose # gyro_odometer is the package to estimate twist by combining imu and vehicle speed. Inputs / Outputs # Input # Name Type Description vehicle/twist_with_covariance geometry_msgs::msg::TwistWithCovarianceStamped twist with covariance from vehicle imu sensor_msgs::msg::Imu imu from sensor Output # Name Type Description twist_with_covariance geometry_msgs::msg::TwistWithCovarianceStamped estimated twist with covariance Parameters # Parameter Type Description output_frame String output's frame id message_timeout_sec Double delay tolerance time for message Assumptions / Known limits # [Assumption] The frame_id of input twist message must be set to base_link. [Assumption] The covariance in the input messages must be properly assigned. [Assumption] The angular velocity is set to zero if both the longitudinal vehicle velocity and the angular velocity around the yaw axis are sufficiently small. This is for suppression of the IMU angular velocity bias. Without this process, we misestimate the vehicle status when stationary. [Limitation] The frequency of the output messages depends on the frequency of the input IMU message. [Limitation] We cannot produce reliable values for the lateral and vertical velocities. Therefore we assign large values to the corresponding elements in the output covariance matrix.","title":"gyro_odometer"},{"location":"localization/gyro_odometer/#gyro_odometer","text":"","title":"gyro_odometer"},{"location":"localization/gyro_odometer/#purpose","text":"gyro_odometer is the package to estimate twist by combining imu and vehicle speed.","title":"Purpose"},{"location":"localization/gyro_odometer/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"localization/gyro_odometer/#input","text":"Name Type Description vehicle/twist_with_covariance geometry_msgs::msg::TwistWithCovarianceStamped twist with covariance from vehicle imu sensor_msgs::msg::Imu imu from sensor","title":"Input"},{"location":"localization/gyro_odometer/#output","text":"Name Type Description twist_with_covariance geometry_msgs::msg::TwistWithCovarianceStamped estimated twist with covariance","title":"Output"},{"location":"localization/gyro_odometer/#parameters","text":"Parameter Type Description output_frame String output's frame id message_timeout_sec Double delay tolerance time for message","title":"Parameters"},{"location":"localization/gyro_odometer/#assumptions-known-limits","text":"[Assumption] The frame_id of input twist message must be set to base_link. [Assumption] The covariance in the input messages must be properly assigned. [Assumption] The angular velocity is set to zero if both the longitudinal vehicle velocity and the angular velocity around the yaw axis are sufficiently small. This is for suppression of the IMU angular velocity bias. Without this process, we misestimate the vehicle status when stationary. [Limitation] The frequency of the output messages depends on the frequency of the input IMU message. [Limitation] We cannot produce reliable values for the lateral and vertical velocities. Therefore we assign large values to the corresponding elements in the output covariance matrix.","title":"Assumptions / Known limits"},{"location":"localization/initial_pose_button_panel/","text":"initial_pose_button_panel # Role # initial_pose_button_panel is the package to send a request to the localization module to calculate the current ego pose. It starts calculating the current ego pose by pushing the button on Rviz, implemented as an Rviz plugin. You can see the button on the right bottom of Rviz. Input / Output # Input topics # Name Type Description /sensing/gnss/pose_with_covariance (default) geometry_msgs::msg::PoseWithCovarianceStamped initial pose with covariance to calculate the current ego pose","title":"initial_pose_button_panel"},{"location":"localization/initial_pose_button_panel/#initial_pose_button_panel","text":"","title":"initial_pose_button_panel"},{"location":"localization/initial_pose_button_panel/#role","text":"initial_pose_button_panel is the package to send a request to the localization module to calculate the current ego pose. It starts calculating the current ego pose by pushing the button on Rviz, implemented as an Rviz plugin. You can see the button on the right bottom of Rviz.","title":"Role"},{"location":"localization/initial_pose_button_panel/#input-output","text":"","title":"Input / Output"},{"location":"localization/initial_pose_button_panel/#input-topics","text":"Name Type Description /sensing/gnss/pose_with_covariance (default) geometry_msgs::msg::PoseWithCovarianceStamped initial pose with covariance to calculate the current ego pose","title":"Input topics"},{"location":"localization/localization_error_monitor/","text":"localization_error_monitor # Purpose # localization_error_monitor is a package for diagnosing localization errors by monitoring uncertainty of the localization results. The package monitors the following two values: size of long radius of confidence ellipse size of confidence ellipse along lateral direction (body-frame) Inputs / Outputs # Input # Name Type Description input/pose_with_cov geometry_msgs::msg::PoseWithCovarianceStamped localization result Output # Name Type Description debug/ellipse_marker visualization_msgs::msg::Marker ellipse marker diagnostics diagnostic_msgs::msg::DiagnosticArray diagnostics outputs Parameters # Name Type Description scale double scale factor for monitored values (default: 3.0) error_ellipse_size double error threshold for long radius of confidence ellipse [m] (default: 1.0) warn_ellipse_size double warning threshold for long radius of confidence ellipse [m] (default: 0.8) error_ellipse_size_lateral_direction double error threshold for size of confidence ellipse along lateral direction [m] (default: 0.3) warn_ellipse_size_lateral_direction double warning threshold for size of confidence ellipse along lateral direction [m] (default: 0.2)","title":"localization_error_monitor"},{"location":"localization/localization_error_monitor/#localization_error_monitor","text":"","title":"localization_error_monitor"},{"location":"localization/localization_error_monitor/#purpose","text":"localization_error_monitor is a package for diagnosing localization errors by monitoring uncertainty of the localization results. The package monitors the following two values: size of long radius of confidence ellipse size of confidence ellipse along lateral direction (body-frame)","title":"Purpose"},{"location":"localization/localization_error_monitor/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"localization/localization_error_monitor/#input","text":"Name Type Description input/pose_with_cov geometry_msgs::msg::PoseWithCovarianceStamped localization result","title":"Input"},{"location":"localization/localization_error_monitor/#output","text":"Name Type Description debug/ellipse_marker visualization_msgs::msg::Marker ellipse marker diagnostics diagnostic_msgs::msg::DiagnosticArray diagnostics outputs","title":"Output"},{"location":"localization/localization_error_monitor/#parameters","text":"Name Type Description scale double scale factor for monitored values (default: 3.0) error_ellipse_size double error threshold for long radius of confidence ellipse [m] (default: 1.0) warn_ellipse_size double warning threshold for long radius of confidence ellipse [m] (default: 0.8) error_ellipse_size_lateral_direction double error threshold for size of confidence ellipse along lateral direction [m] (default: 0.3) warn_ellipse_size_lateral_direction double warning threshold for size of confidence ellipse along lateral direction [m] (default: 0.2)","title":"Parameters"},{"location":"localization/ndt/","text":"ndt # This package aims to absorb the differences of several NDT implementations and provide a common interface.","title":"ndt"},{"location":"localization/ndt/#ndt","text":"This package aims to absorb the differences of several NDT implementations and provide a common interface.","title":"ndt"},{"location":"localization/ndt_pcl_modified/","text":"ndt_pcl_modified # Purpose # This is a modification of PCL 's NDT. Modifications # You can get the Hessian matrix by getHessian(). You can get the estimated position for each iteration by getFinalTransformationArray(). It optimizes rotational axes first, then jointly optimizes rotational and translational axes. [experimental feature]","title":"ndt_pcl_modified"},{"location":"localization/ndt_pcl_modified/#ndt_pcl_modified","text":"","title":"ndt_pcl_modified"},{"location":"localization/ndt_pcl_modified/#purpose","text":"This is a modification of PCL 's NDT.","title":"Purpose"},{"location":"localization/ndt_pcl_modified/#modifications","text":"You can get the Hessian matrix by getHessian(). You can get the estimated position for each iteration by getFinalTransformationArray(). It optimizes rotational axes first, then jointly optimizes rotational and translational axes. [experimental feature]","title":"Modifications"},{"location":"localization/ndt_scan_matcher/","text":"ndt_scan_matcher # Purpose # ndt_scan_matcher is a package for position estimation using the NDT scan matching method. There are two main functions in this package: estimate position by scan matching estimate initial position via the ROS service using the Monte Carlo method One optional function is regularization. Please see the regularization chapter in the back for details. It is disabled by default. Inputs / Outputs # Input # Name Type Description ekf_pose_with_covariance geometry_msgs::msg::PoseWithCovarianceStamped initial pose pointcloud_map sensor_msgs::msg::PointCloud2 map pointcloud points_raw sensor_msgs::msg::PointCloud2 sensor pointcloud sensing/gnss/pose_with_covariance sensor_msgs::msg::PoseWithCovarianceStamped base position for regularization term sensing/gnss/pose_with_covariance is required only when regularization is enabled. Output # Name Type Description ndt_pose geometry_msgs::msg::PoseStamped estimated pose ndt_pose_with_covariance geometry_msgs::msg::PoseWithCovarianceStamped estimated pose with covariance /diagnostics diagnostic_msgs::msg::DiagnosticArray diagnostics points_aligned sensor_msgs::msg::PointCloud2 [debug topic] pointcloud aligned by scan matching initial_pose_with_covariance geometry_msgs::msg::PoseWithCovarianceStamped [debug topic] initial pose used in scan matching exe_time_ms tier4_debug_msgs::msg::Float32Stamped [debug topic] execution time for scan matching [ms] transform_probability tier4_debug_msgs::msg::Float32Stamped [debug topic] score of scan matching iteration_num tier4_debug_msgs::msg::Int32Stamped [debug topic] number of scan matching iterations initial_to_result_distance tier4_debug_msgs::msg::Float32Stamped [debug topic] distance difference between the initial point and the convergence point [m] initial_to_result_distance_old tier4_debug_msgs::msg::Float32Stamped [debug topic] distance difference between the older of the two initial points used in linear interpolation and the convergence point [m] initial_to_result_distance_new tier4_debug_msgs::msg::Float32Stamped [debug topic] distance difference between the newer of the two initial points used in linear interpolation and the convergence point [m] ndt_marker visualization_msgs::msg::MarkerArray [debug topic] markers for debugging monte_carlo_initial_pose_marker visualization_msgs::msg::MarkerArray [debug topic] particles used in initial position estimation Service # Name Type Description ndt_align_srv autoware_localization_srvs::srv::PoseWithCovarianceStamped service to estimate initial pose Parameters # Core Parameters # Name Type Description base_frame string Vehicle reference frame input_sensor_points_queue_size int Subscriber queue size ndt_implement_type int NDT implementation type (0=PCL_GENERIC, 1=PCL_MODIFIED, 2=OMP) trans_epsilon double The maximum difference between two consecutive transformations in order to consider convergence step_size double The newton line search maximum step length resolution double The ND voxel grid resolution [m] max_iterations int The number of iterations required to calculate alignment converged_param_transform_probability double Threshold for deciding whether to trust the estimation result omp_neighborhood_search_method int neighborhood search method in OMP (0=KDTREE, 1=DIRECT26, 2=DIRECT7, 3=DIRECT1) omp_num_threads int Number of threads used for parallel computing Regularization # Abstract # This is a function that adds the regularization term to the NDT optimization problem as follows. \\begin{align} \\min_{\\mathbf{R},\\mathbf{t}} \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) +\\mathrm{scale\\ factor}\\cdot \\left| \\mathbf{R}^\\top (\\mathbf{t_{base}-\\mathbf{t}}) \\cdot \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} \\right|^2 \\end{align} \\begin{align} \\min_{\\mathbf{R},\\mathbf{t}} \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) +\\mathrm{scale\\ factor}\\cdot \\left| \\mathbf{R}^\\top (\\mathbf{t_{base}-\\mathbf{t}}) \\cdot \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} \\right|^2 \\end{align} , where t_base is base position measured by GNSS or other means. NDT(R,t) stands for the pure NDT cost function. The regularization term shifts the optimal solution to the base position in the longitudinal direction of the vehicle. Only errors along the longitudinal direction with respect to the base position are considered; errors along Z-axis and lateral-axis error are not considered. Although the regularization term has rotation as a parameter, the gradient and hessian associated with it is not computed to stabilize the optimization. Specifically, the gradients are computed as follows. \\begin{align} &g_x=\\nabla_x \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) + 2 \\mathrm{scale\\ factor} \\cos\\theta_z\\cdot e_{\\mathrm{longitudinal}} \\\\ &g_y=\\nabla_y \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) + 2 \\mathrm{scale\\ factor} \\sin\\theta_z\\cdot e_{\\mathrm{longitudinal}} \\\\ &g_z=\\nabla_z \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) \\\\ &g_\\mathbf{R}=\\nabla_\\mathbf{R} \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) \\end{align} \\begin{align} &g_x=\\nabla_x \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) + 2 \\mathrm{scale\\ factor} \\cos\\theta_z\\cdot e_{\\mathrm{longitudinal}} \\\\ &g_y=\\nabla_y \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) + 2 \\mathrm{scale\\ factor} \\sin\\theta_z\\cdot e_{\\mathrm{longitudinal}} \\\\ &g_z=\\nabla_z \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) \\\\ &g_\\mathbf{R}=\\nabla_\\mathbf{R} \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) \\end{align} Regularization is disabled by default. If you wish to use it, please edit the following parameters to enable it. Regularization is only available for NDT_OMP and not for other NDT implementation types ( PCL_GENERIC , PCL_MODIFIED ). Where is regularization available # This feature is effective on feature-less roads where GNSS is available, such as bridges highways farm roads By remapping the base position topic to something other than GNSS, as described below, it can be valid outside of these. Using other base position # Other than GNSS, you can give other global position topics obtained from magnetic markers, visual markers or etc. if they are available in your environment. (Currently Autoware does not provide a node that gives such pose.) To use your topic for regularization, you need to remap the input_regularization_pose_topic with your topic in ndt_scan_matcher.launch.xml . By default, it is remapped with /sensing/gnss/pose_with_covariance . Limitations # Since this function determines the base position by linear interpolation from the recently subscribed poses, topics that are published at a low frequency relative to the driving speed cannot be used. Inappropriate linear interpolation may result in bad optimization results. When using GNSS for base location, the regularization can have negative effects in tunnels, indoors, and near skyscrapers. This is because if the base position is far off from the true value, NDT scan matching may converge to inappropriate optimal position. Parameters # Name Type Description regularization_enabled bool Flag to add regularization term to NDT optimization (FALSE by default) regularization_scale_factor double Coefficient of the regularization term. Regularization is disabled by default because GNSS is not always accurate enough to serve the appropriate base position in any scenes. If the scale_factor is too large, the NDT will be drawn to the base position and scan matching may fail. Conversely, if it is too small, the regularization benefit will be lost. Note that setting scale_factor to 0 is equivalent to disabling regularization. Example # The following figures show tested maps. The left is a map with enough features that NDT can successfully localize. The right is a map with so few features that the NDT cannot localize well. The following figures show the trajectories estimated on the feature-less map with standard NDT and regularization-enabled NDT, respectively. The color of the trajectory indicates the error (meter) from the reference trajectory, which is computed with the feature-rich map. The left figure shows that the pure NDT causes a longitudinal error in the bridge and is not able to recover. The right figure shows that the regularization suppresses the longitudinal error.","title":"ndt_scan_matcher"},{"location":"localization/ndt_scan_matcher/#ndt_scan_matcher","text":"","title":"ndt_scan_matcher"},{"location":"localization/ndt_scan_matcher/#purpose","text":"ndt_scan_matcher is a package for position estimation using the NDT scan matching method. There are two main functions in this package: estimate position by scan matching estimate initial position via the ROS service using the Monte Carlo method One optional function is regularization. Please see the regularization chapter in the back for details. It is disabled by default.","title":"Purpose"},{"location":"localization/ndt_scan_matcher/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"localization/ndt_scan_matcher/#input","text":"Name Type Description ekf_pose_with_covariance geometry_msgs::msg::PoseWithCovarianceStamped initial pose pointcloud_map sensor_msgs::msg::PointCloud2 map pointcloud points_raw sensor_msgs::msg::PointCloud2 sensor pointcloud sensing/gnss/pose_with_covariance sensor_msgs::msg::PoseWithCovarianceStamped base position for regularization term sensing/gnss/pose_with_covariance is required only when regularization is enabled.","title":"Input"},{"location":"localization/ndt_scan_matcher/#output","text":"Name Type Description ndt_pose geometry_msgs::msg::PoseStamped estimated pose ndt_pose_with_covariance geometry_msgs::msg::PoseWithCovarianceStamped estimated pose with covariance /diagnostics diagnostic_msgs::msg::DiagnosticArray diagnostics points_aligned sensor_msgs::msg::PointCloud2 [debug topic] pointcloud aligned by scan matching initial_pose_with_covariance geometry_msgs::msg::PoseWithCovarianceStamped [debug topic] initial pose used in scan matching exe_time_ms tier4_debug_msgs::msg::Float32Stamped [debug topic] execution time for scan matching [ms] transform_probability tier4_debug_msgs::msg::Float32Stamped [debug topic] score of scan matching iteration_num tier4_debug_msgs::msg::Int32Stamped [debug topic] number of scan matching iterations initial_to_result_distance tier4_debug_msgs::msg::Float32Stamped [debug topic] distance difference between the initial point and the convergence point [m] initial_to_result_distance_old tier4_debug_msgs::msg::Float32Stamped [debug topic] distance difference between the older of the two initial points used in linear interpolation and the convergence point [m] initial_to_result_distance_new tier4_debug_msgs::msg::Float32Stamped [debug topic] distance difference between the newer of the two initial points used in linear interpolation and the convergence point [m] ndt_marker visualization_msgs::msg::MarkerArray [debug topic] markers for debugging monte_carlo_initial_pose_marker visualization_msgs::msg::MarkerArray [debug topic] particles used in initial position estimation","title":"Output"},{"location":"localization/ndt_scan_matcher/#service","text":"Name Type Description ndt_align_srv autoware_localization_srvs::srv::PoseWithCovarianceStamped service to estimate initial pose","title":"Service"},{"location":"localization/ndt_scan_matcher/#parameters","text":"","title":"Parameters"},{"location":"localization/ndt_scan_matcher/#core-parameters","text":"Name Type Description base_frame string Vehicle reference frame input_sensor_points_queue_size int Subscriber queue size ndt_implement_type int NDT implementation type (0=PCL_GENERIC, 1=PCL_MODIFIED, 2=OMP) trans_epsilon double The maximum difference between two consecutive transformations in order to consider convergence step_size double The newton line search maximum step length resolution double The ND voxel grid resolution [m] max_iterations int The number of iterations required to calculate alignment converged_param_transform_probability double Threshold for deciding whether to trust the estimation result omp_neighborhood_search_method int neighborhood search method in OMP (0=KDTREE, 1=DIRECT26, 2=DIRECT7, 3=DIRECT1) omp_num_threads int Number of threads used for parallel computing","title":"Core Parameters"},{"location":"localization/ndt_scan_matcher/#regularization","text":"","title":"Regularization"},{"location":"localization/ndt_scan_matcher/#abstract","text":"This is a function that adds the regularization term to the NDT optimization problem as follows. \\begin{align} \\min_{\\mathbf{R},\\mathbf{t}} \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) +\\mathrm{scale\\ factor}\\cdot \\left| \\mathbf{R}^\\top (\\mathbf{t_{base}-\\mathbf{t}}) \\cdot \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} \\right|^2 \\end{align} \\begin{align} \\min_{\\mathbf{R},\\mathbf{t}} \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) +\\mathrm{scale\\ factor}\\cdot \\left| \\mathbf{R}^\\top (\\mathbf{t_{base}-\\mathbf{t}}) \\cdot \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} \\right|^2 \\end{align} , where t_base is base position measured by GNSS or other means. NDT(R,t) stands for the pure NDT cost function. The regularization term shifts the optimal solution to the base position in the longitudinal direction of the vehicle. Only errors along the longitudinal direction with respect to the base position are considered; errors along Z-axis and lateral-axis error are not considered. Although the regularization term has rotation as a parameter, the gradient and hessian associated with it is not computed to stabilize the optimization. Specifically, the gradients are computed as follows. \\begin{align} &g_x=\\nabla_x \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) + 2 \\mathrm{scale\\ factor} \\cos\\theta_z\\cdot e_{\\mathrm{longitudinal}} \\\\ &g_y=\\nabla_y \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) + 2 \\mathrm{scale\\ factor} \\sin\\theta_z\\cdot e_{\\mathrm{longitudinal}} \\\\ &g_z=\\nabla_z \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) \\\\ &g_\\mathbf{R}=\\nabla_\\mathbf{R} \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) \\end{align} \\begin{align} &g_x=\\nabla_x \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) + 2 \\mathrm{scale\\ factor} \\cos\\theta_z\\cdot e_{\\mathrm{longitudinal}} \\\\ &g_y=\\nabla_y \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) + 2 \\mathrm{scale\\ factor} \\sin\\theta_z\\cdot e_{\\mathrm{longitudinal}} \\\\ &g_z=\\nabla_z \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) \\\\ &g_\\mathbf{R}=\\nabla_\\mathbf{R} \\mathrm{NDT}(\\mathbf{R},\\mathbf{t}) \\end{align} Regularization is disabled by default. If you wish to use it, please edit the following parameters to enable it. Regularization is only available for NDT_OMP and not for other NDT implementation types ( PCL_GENERIC , PCL_MODIFIED ).","title":"Abstract"},{"location":"localization/ndt_scan_matcher/#where-is-regularization-available","text":"This feature is effective on feature-less roads where GNSS is available, such as bridges highways farm roads By remapping the base position topic to something other than GNSS, as described below, it can be valid outside of these.","title":"Where is regularization available"},{"location":"localization/ndt_scan_matcher/#using-other-base-position","text":"Other than GNSS, you can give other global position topics obtained from magnetic markers, visual markers or etc. if they are available in your environment. (Currently Autoware does not provide a node that gives such pose.) To use your topic for regularization, you need to remap the input_regularization_pose_topic with your topic in ndt_scan_matcher.launch.xml . By default, it is remapped with /sensing/gnss/pose_with_covariance .","title":"Using other base position"},{"location":"localization/ndt_scan_matcher/#limitations","text":"Since this function determines the base position by linear interpolation from the recently subscribed poses, topics that are published at a low frequency relative to the driving speed cannot be used. Inappropriate linear interpolation may result in bad optimization results. When using GNSS for base location, the regularization can have negative effects in tunnels, indoors, and near skyscrapers. This is because if the base position is far off from the true value, NDT scan matching may converge to inappropriate optimal position.","title":"Limitations"},{"location":"localization/ndt_scan_matcher/#parameters_1","text":"Name Type Description regularization_enabled bool Flag to add regularization term to NDT optimization (FALSE by default) regularization_scale_factor double Coefficient of the regularization term. Regularization is disabled by default because GNSS is not always accurate enough to serve the appropriate base position in any scenes. If the scale_factor is too large, the NDT will be drawn to the base position and scan matching may fail. Conversely, if it is too small, the regularization benefit will be lost. Note that setting scale_factor to 0 is equivalent to disabling regularization.","title":"Parameters"},{"location":"localization/ndt_scan_matcher/#example","text":"The following figures show tested maps. The left is a map with enough features that NDT can successfully localize. The right is a map with so few features that the NDT cannot localize well. The following figures show the trajectories estimated on the feature-less map with standard NDT and regularization-enabled NDT, respectively. The color of the trajectory indicates the error (meter) from the reference trajectory, which is computed with the feature-rich map. The left figure shows that the pure NDT causes a longitudinal error in the bridge and is not able to recover. The right figure shows that the regularization suppresses the longitudinal error.","title":"Example"},{"location":"localization/pose2twist/","text":"pose2twist # Purpose # This pose2twist calculates the velocity from the input pose history. In addition to the computed twist, this node outputs the linear-x and angular-z components as a float message to simplify debugging. The twist.linear.x is calculated as sqrt(dx * dx + dy * dy + dz * dz) / dt , and the values in the y and z fields are zero. The twist.angular is calculated as d_roll / dt , d_pitch / dt and d_yaw / dt for each field. Inputs / Outputs # Input # Name Type Description pose geometry_msgs::msg::PoseStamped pose source to used for the velocity calculation. Output # Name Type Description twist geometry_msgs::msg::TwistStamped twist calculated from the input pose history. linear_x tier4_debug_msgs::msg::Float32Stamped linear-x field of the output twist. angular_z tier4_debug_msgs::msg::Float32Stamped angular-z field of the output twist. Parameters # none. Assumptions / Known limits # none.","title":"pose2twist"},{"location":"localization/pose2twist/#pose2twist","text":"","title":"pose2twist"},{"location":"localization/pose2twist/#purpose","text":"This pose2twist calculates the velocity from the input pose history. In addition to the computed twist, this node outputs the linear-x and angular-z components as a float message to simplify debugging. The twist.linear.x is calculated as sqrt(dx * dx + dy * dy + dz * dz) / dt , and the values in the y and z fields are zero. The twist.angular is calculated as d_roll / dt , d_pitch / dt and d_yaw / dt for each field.","title":"Purpose"},{"location":"localization/pose2twist/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"localization/pose2twist/#input","text":"Name Type Description pose geometry_msgs::msg::PoseStamped pose source to used for the velocity calculation.","title":"Input"},{"location":"localization/pose2twist/#output","text":"Name Type Description twist geometry_msgs::msg::TwistStamped twist calculated from the input pose history. linear_x tier4_debug_msgs::msg::Float32Stamped linear-x field of the output twist. angular_z tier4_debug_msgs::msg::Float32Stamped angular-z field of the output twist.","title":"Output"},{"location":"localization/pose2twist/#parameters","text":"none.","title":"Parameters"},{"location":"localization/pose2twist/#assumptions-known-limits","text":"none.","title":"Assumptions / Known limits"},{"location":"localization/pose_initializer/","text":"pose_initializer # Purpose # pose_initializer is the package to send an initial pose to ekf_localizer . It receives roughly estimated initial pose from GNSS/user. Passing the pose to ndt_scan_matcher , and it gets a calculated ego pose from ndt_scan_matcher via service. Finally, it publishes the initial pose to ekf_localizer . Input / Output # Input topics # Name Type Description /initialpose geometry_msgs::msg::PoseWithCovarianceStamped initial pose from rviz /sensing/gnss/pose_with_covariance geometry_msgs::msg::PoseWithCovarianceStamped pose from gnss /map/pointcloud_map sensor_msgs::msg::PointCloud2 pointcloud map Output topics # Name Type Description /initialpose3d geometry_msgs::msg::PoseWithCovarianceStamped calculated initial ego pose","title":"pose_initializer"},{"location":"localization/pose_initializer/#pose_initializer","text":"","title":"pose_initializer"},{"location":"localization/pose_initializer/#purpose","text":"pose_initializer is the package to send an initial pose to ekf_localizer . It receives roughly estimated initial pose from GNSS/user. Passing the pose to ndt_scan_matcher , and it gets a calculated ego pose from ndt_scan_matcher via service. Finally, it publishes the initial pose to ekf_localizer .","title":"Purpose"},{"location":"localization/pose_initializer/#input-output","text":"","title":"Input / Output"},{"location":"localization/pose_initializer/#input-topics","text":"Name Type Description /initialpose geometry_msgs::msg::PoseWithCovarianceStamped initial pose from rviz /sensing/gnss/pose_with_covariance geometry_msgs::msg::PoseWithCovarianceStamped pose from gnss /map/pointcloud_map sensor_msgs::msg::PointCloud2 pointcloud map","title":"Input topics"},{"location":"localization/pose_initializer/#output-topics","text":"Name Type Description /initialpose3d geometry_msgs::msg::PoseWithCovarianceStamped calculated initial ego pose","title":"Output topics"},{"location":"localization/stop_filter/","text":"stop_filter # Purpose # When this function did not exist, each node used a different criterion to determine whether the vehicle is stopping or not, resulting that some nodes were in operation of stopping the vehicle and some nodes continued running in the drive mode. This node aims to: apply a uniform stopping decision criterion to several nodes. suppress the control noise by overwriting the velocity and angular velocity with zero. Inputs / Outputs # Input # Name Type Description input/odom nav_msgs::msg::Odometry localization odometry Output # Name Type Description output/odom nav_msgs::msg::Odometry odometry with suppressed longitudinal and yaw twist debug/stop_flag tier4_debug_msgs::msg::BoolStamped flag to represent whether the vehicle is stopping or not Parameters # Name Type Description vx_threshold double longitudinal velocity threshold to determine if the vehicle is stopping [m/s] (default: 0.01) wz_threshold double yaw velocity threshold to determine if the vehicle is stopping [rad/s] (default: 0.01)","title":"stop_filter"},{"location":"localization/stop_filter/#stop_filter","text":"","title":"stop_filter"},{"location":"localization/stop_filter/#purpose","text":"When this function did not exist, each node used a different criterion to determine whether the vehicle is stopping or not, resulting that some nodes were in operation of stopping the vehicle and some nodes continued running in the drive mode. This node aims to: apply a uniform stopping decision criterion to several nodes. suppress the control noise by overwriting the velocity and angular velocity with zero.","title":"Purpose"},{"location":"localization/stop_filter/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"localization/stop_filter/#input","text":"Name Type Description input/odom nav_msgs::msg::Odometry localization odometry","title":"Input"},{"location":"localization/stop_filter/#output","text":"Name Type Description output/odom nav_msgs::msg::Odometry odometry with suppressed longitudinal and yaw twist debug/stop_flag tier4_debug_msgs::msg::BoolStamped flag to represent whether the vehicle is stopping or not","title":"Output"},{"location":"localization/stop_filter/#parameters","text":"Name Type Description vx_threshold double longitudinal velocity threshold to determine if the vehicle is stopping [m/s] (default: 0.01) wz_threshold double yaw velocity threshold to determine if the vehicle is stopping [rad/s] (default: 0.01)","title":"Parameters"},{"location":"localization/twist2accel/","text":"twist2accel # Purpose # This package is responsible for estimating acceleration using the output of ekf_localizer . It uses lowpass filter to mitigate the noise. Inputs / Outputs # Input # Name Type Description input/odom nav_msgs::msg::Odometry localization odometry input/twist geometry_msgs::msg::TwistWithCovarianceStamped twist Output # Name Type Description output/accel geometry_msgs::msg::AccelWithCovarianceStamped estimated acceleration Parameters # Name Type Description use_odom bool use odometry if true, else use twist input (default: true) accel_lowpass_gain double lowpass gain for lowpass filter in estimating acceleration (default: 0.9) Future work # Future work includes integrating acceleration into the EKF state.","title":"twist2accel"},{"location":"localization/twist2accel/#twist2accel","text":"","title":"twist2accel"},{"location":"localization/twist2accel/#purpose","text":"This package is responsible for estimating acceleration using the output of ekf_localizer . It uses lowpass filter to mitigate the noise.","title":"Purpose"},{"location":"localization/twist2accel/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"localization/twist2accel/#input","text":"Name Type Description input/odom nav_msgs::msg::Odometry localization odometry input/twist geometry_msgs::msg::TwistWithCovarianceStamped twist","title":"Input"},{"location":"localization/twist2accel/#output","text":"Name Type Description output/accel geometry_msgs::msg::AccelWithCovarianceStamped estimated acceleration","title":"Output"},{"location":"localization/twist2accel/#parameters","text":"Name Type Description use_odom bool use odometry if true, else use twist input (default: true) accel_lowpass_gain double lowpass gain for lowpass filter in estimating acceleration (default: 0.9)","title":"Parameters"},{"location":"localization/twist2accel/#future-work","text":"Future work includes integrating acceleration into the EKF state.","title":"Future work"},{"location":"map/lanelet2_extension/","text":"lanelet2_extension package # This package contains external library for Lanelet2 and is meant to ease the use of Lanelet2 in Autoware. Lanelet Format for Autoware # Autoware uses extended Lanelet2 Format for Autoware, which means you need to add some tags to default OSM file if you want to fully use Lanelet2 maps. For details about custom tags, please refer to this document . Code API # IO # Autoware OSM Parser # Autoware Lanelet2 Format uses .osm extension as original Lanelet2. However, there are some custom tags that is used by the parser. Currently, this includes: overwriting x,y values with local_x and local_y tags. reading <MapMetaInfo> tag which contains information about map format version and map version. The parser is registered as \"autoware_osm_handler\" as lanelet parser Projection # MGRS Projector # MGRS projector projects latitude longitude into MGRS Coordinates. Regulatory Elements # Autoware Traffic Light # Autoware Traffic Light class allows you to retrieve information about traffic lights. Autoware Traffic Light class contains following members: traffic light shape light bulbs information of traffic lights stopline associated to traffic light Utility # Message Conversion # This contains functions to convert lanelet map objects into ROS messages. Currently it contains following conversions: lanelet::LaneletMapPtr to/from autoware_auto_mapping_msgs::msg::HADMapBin lanelet::Point3d to geometry_msgs::Point lanelet::Point2d to geometry_msgs::Point lanelet::BasicPoint3d to geometry_msgs::Point Query # This module contains functions to retrieve various information from maps. e.g. crosswalks, trafficlights, stoplines Utilities # This module contains other useful functions related to Lanelet. e.g. matching waypoint with lanelets Visualization # Visualization contains functions to convert lanelet objects into visualization marker messages. Currently it contains following conversions: lanelet::Lanelet to Triangle Markers lanelet::LineString to LineStrip Markers TrafficLights to Triangle Markers Nodes # lanelet2_extension_sample # Code for this explains how this lanelet2_extension library is used. The executable is not meant to do anything. autoware_lanelet2_extension # This node checks if an .osm file follows the Autoware version of Lanelet2 format. You can check by running: ros2 run lanelet2_extension autoware_lanelet2_validation _map_file: = <path/to/map.osm>","title":"lanelet2_extension package"},{"location":"map/lanelet2_extension/#lanelet2_extension-package","text":"This package contains external library for Lanelet2 and is meant to ease the use of Lanelet2 in Autoware.","title":"lanelet2_extension package"},{"location":"map/lanelet2_extension/#lanelet-format-for-autoware","text":"Autoware uses extended Lanelet2 Format for Autoware, which means you need to add some tags to default OSM file if you want to fully use Lanelet2 maps. For details about custom tags, please refer to this document .","title":"Lanelet Format for Autoware"},{"location":"map/lanelet2_extension/#code-api","text":"","title":"Code API"},{"location":"map/lanelet2_extension/#io","text":"","title":"IO"},{"location":"map/lanelet2_extension/#autoware-osm-parser","text":"Autoware Lanelet2 Format uses .osm extension as original Lanelet2. However, there are some custom tags that is used by the parser. Currently, this includes: overwriting x,y values with local_x and local_y tags. reading <MapMetaInfo> tag which contains information about map format version and map version. The parser is registered as \"autoware_osm_handler\" as lanelet parser","title":"Autoware OSM Parser"},{"location":"map/lanelet2_extension/#projection","text":"","title":"Projection"},{"location":"map/lanelet2_extension/#mgrs-projector","text":"MGRS projector projects latitude longitude into MGRS Coordinates.","title":"MGRS Projector"},{"location":"map/lanelet2_extension/#regulatory-elements","text":"","title":"Regulatory Elements"},{"location":"map/lanelet2_extension/#autoware-traffic-light","text":"Autoware Traffic Light class allows you to retrieve information about traffic lights. Autoware Traffic Light class contains following members: traffic light shape light bulbs information of traffic lights stopline associated to traffic light","title":"Autoware Traffic Light"},{"location":"map/lanelet2_extension/#utility","text":"","title":"Utility"},{"location":"map/lanelet2_extension/#message-conversion","text":"This contains functions to convert lanelet map objects into ROS messages. Currently it contains following conversions: lanelet::LaneletMapPtr to/from autoware_auto_mapping_msgs::msg::HADMapBin lanelet::Point3d to geometry_msgs::Point lanelet::Point2d to geometry_msgs::Point lanelet::BasicPoint3d to geometry_msgs::Point","title":"Message Conversion"},{"location":"map/lanelet2_extension/#query","text":"This module contains functions to retrieve various information from maps. e.g. crosswalks, trafficlights, stoplines","title":"Query"},{"location":"map/lanelet2_extension/#utilities","text":"This module contains other useful functions related to Lanelet. e.g. matching waypoint with lanelets","title":"Utilities"},{"location":"map/lanelet2_extension/#visualization","text":"Visualization contains functions to convert lanelet objects into visualization marker messages. Currently it contains following conversions: lanelet::Lanelet to Triangle Markers lanelet::LineString to LineStrip Markers TrafficLights to Triangle Markers","title":"Visualization"},{"location":"map/lanelet2_extension/#nodes","text":"","title":"Nodes"},{"location":"map/lanelet2_extension/#lanelet2_extension_sample","text":"Code for this explains how this lanelet2_extension library is used. The executable is not meant to do anything.","title":"lanelet2_extension_sample"},{"location":"map/lanelet2_extension/#autoware_lanelet2_extension","text":"This node checks if an .osm file follows the Autoware version of Lanelet2 format. You can check by running: ros2 run lanelet2_extension autoware_lanelet2_validation _map_file: = <path/to/map.osm>","title":"autoware_lanelet2_extension"},{"location":"map/lanelet2_extension/docs/extra_lanelet_subtypes/","text":"Extra Lanelet Subtypes # Roadside Lane # The subtypes for this lanelet classify the outer lanes adjacent to the driving lane.Since the list of lanelet subtypes defined in this link cannot represent the shoulder lane and pedestrian lane described below, two new subtypes are defined.When parking on the street, it is necessary to distinguish between a shoulder lane which can be used by vehicles, and a pedestrian lane which can be used by pedestrians and bicycles.If you park in a shoulder lane, you can use the entire lane for temporary parking, but if you park in a pedestrian lane, you must leave a space of at least 75cm. Road shoulder subtype # refers: lanelet with subtype attribute. Subtype explains what the type of roadside it represents. If there is an area outside of this roadside lane that is open to traffic, such as a sidewalk or bike lane, select the road_shoulder subtype. Sample road shoulder in .osm format is shown below: <relation id= \"120700\" > <member type= \"way\" role= \"left\" ref= \"34577\" /> <member type= \"way\" role= \"right\" ref= \"120694\" /> <tag k= \"type\" v= \"lanelet\" /> <tag k= \"subtype\" v= \"road_shoulder\" /> <tag k= \"speed_limit\" v= \"10\" /> <tag k= \"location\" v= \"urban\" /> <tag k= \"one_way\" v= \"yes\" /> </relation> Pedestrian lane subtype # refers: lanelet with subtype attribute. Subtype explains what the type of roadside it represents. If there are no passable areas outside of this roadside lane, select the pedestrian_lane subtype. Sample pedestrian lane in .osm format is shown below: <relation id= \"120700\" > <member type= \"way\" role= \"left\" ref= \"34577\" /> <member type= \"way\" role= \"right\" ref= \"120694\" /> <tag k= \"type\" v= \"lanelet\" /> <tag k= \"subtype\" v= \"pedestrian_lane\" /> <tag k= \"speed_limit\" v= \"10\" /> <tag k= \"location\" v= \"urban\" /> <tag k= \"one_way\" v= \"yes\" /> </relation>","title":"Extra Lanelet Subtypes"},{"location":"map/lanelet2_extension/docs/extra_lanelet_subtypes/#extra-lanelet-subtypes","text":"","title":"Extra Lanelet Subtypes"},{"location":"map/lanelet2_extension/docs/extra_lanelet_subtypes/#roadside-lane","text":"The subtypes for this lanelet classify the outer lanes adjacent to the driving lane.Since the list of lanelet subtypes defined in this link cannot represent the shoulder lane and pedestrian lane described below, two new subtypes are defined.When parking on the street, it is necessary to distinguish between a shoulder lane which can be used by vehicles, and a pedestrian lane which can be used by pedestrians and bicycles.If you park in a shoulder lane, you can use the entire lane for temporary parking, but if you park in a pedestrian lane, you must leave a space of at least 75cm.","title":"Roadside Lane"},{"location":"map/lanelet2_extension/docs/extra_lanelet_subtypes/#road-shoulder-subtype","text":"refers: lanelet with subtype attribute. Subtype explains what the type of roadside it represents. If there is an area outside of this roadside lane that is open to traffic, such as a sidewalk or bike lane, select the road_shoulder subtype. Sample road shoulder in .osm format is shown below: <relation id= \"120700\" > <member type= \"way\" role= \"left\" ref= \"34577\" /> <member type= \"way\" role= \"right\" ref= \"120694\" /> <tag k= \"type\" v= \"lanelet\" /> <tag k= \"subtype\" v= \"road_shoulder\" /> <tag k= \"speed_limit\" v= \"10\" /> <tag k= \"location\" v= \"urban\" /> <tag k= \"one_way\" v= \"yes\" /> </relation>","title":"Road shoulder subtype"},{"location":"map/lanelet2_extension/docs/extra_lanelet_subtypes/#pedestrian-lane-subtype","text":"refers: lanelet with subtype attribute. Subtype explains what the type of roadside it represents. If there are no passable areas outside of this roadside lane, select the pedestrian_lane subtype. Sample pedestrian lane in .osm format is shown below: <relation id= \"120700\" > <member type= \"way\" role= \"left\" ref= \"34577\" /> <member type= \"way\" role= \"right\" ref= \"120694\" /> <tag k= \"type\" v= \"lanelet\" /> <tag k= \"subtype\" v= \"pedestrian_lane\" /> <tag k= \"speed_limit\" v= \"10\" /> <tag k= \"location\" v= \"urban\" /> <tag k= \"one_way\" v= \"yes\" /> </relation>","title":"Pedestrian lane subtype"},{"location":"map/lanelet2_extension/docs/extra_regulatory_elements/","text":"Extra Regulatory Elements # Detection Area # This regulatory element specifies region of interest which vehicle must pay attention whenever it is driving along the associated lanelet. When there are any obstacle in the detection area, vehicle must stop at specified stopline. refers: refers to detection area polygon. There could be multiple detection areas registered to a single regulatory element. refline: refers to stop line of the detection area Sample detection area in .osm format is shown below: <node id= 1 version= '1' lat= '49.00541994701' lon= '8.41565013855' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 2 version= '1' lat= '49.00542091657' lon= '8.4156469497' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 3 version= '1' lat= '49.00542180052' lon= '8.41564400223' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 4 version= '1' lat= '49.00541994701' lon= '8.41564400223' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 5 version= '1' lat= '49.00542180052' lon= '8.41564400223' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 6 version= '1' lat= '49.00541994701' lon= '8.41564400223' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <way id= 11 version= '1' > <nd ref= 1 /> <nd ref= 2 /> <nd ref= 3 /> <nd ref= 4 /> <nd ref= 1 /> <tag k= 'type' v= \u2019detection_area\u2019 /> <tag k= 'area' v= \u2019yes\u2019 /> </way> <way id= 12 version= \"1\" > <nd ref= 5 /> <nd ref= 6 /> <tag k= 'type' v= stop_line\u2019 /> </way> <relation id= \"13\" > <tag k= \"type\" v= \"regulatory_element\" /> <tag k= \"subtype\" v= \"detection_area\" /> <member type= \"way\" ref= \"11\" role= \"refers\" /> <member type= \"way\" ref= \"12\" role= \"ref_line\" /> </relation> Road Marking # This regulatory element specifies related road markings to a lanelet as shown below. * Note that the stopline in the image is for stoplines that are for reference, and normal stoplines should be expressed using TrafficSign regulatory element. refers: linestring with type attribute. Type explains what road marking it represents (e.g. stopline).","title":"Extra Regulatory Elements"},{"location":"map/lanelet2_extension/docs/extra_regulatory_elements/#extra-regulatory-elements","text":"","title":"Extra Regulatory Elements"},{"location":"map/lanelet2_extension/docs/extra_regulatory_elements/#detection-area","text":"This regulatory element specifies region of interest which vehicle must pay attention whenever it is driving along the associated lanelet. When there are any obstacle in the detection area, vehicle must stop at specified stopline. refers: refers to detection area polygon. There could be multiple detection areas registered to a single regulatory element. refline: refers to stop line of the detection area Sample detection area in .osm format is shown below: <node id= 1 version= '1' lat= '49.00541994701' lon= '8.41565013855' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 2 version= '1' lat= '49.00542091657' lon= '8.4156469497' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 3 version= '1' lat= '49.00542180052' lon= '8.41564400223' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 4 version= '1' lat= '49.00541994701' lon= '8.41564400223' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 5 version= '1' lat= '49.00542180052' lon= '8.41564400223' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <node id= 6 version= '1' lat= '49.00541994701' lon= '8.41564400223' > <tag k= \u2019ele\u2019 v= \u20190\u2019/ > </node> <way id= 11 version= '1' > <nd ref= 1 /> <nd ref= 2 /> <nd ref= 3 /> <nd ref= 4 /> <nd ref= 1 /> <tag k= 'type' v= \u2019detection_area\u2019 /> <tag k= 'area' v= \u2019yes\u2019 /> </way> <way id= 12 version= \"1\" > <nd ref= 5 /> <nd ref= 6 /> <tag k= 'type' v= stop_line\u2019 /> </way> <relation id= \"13\" > <tag k= \"type\" v= \"regulatory_element\" /> <tag k= \"subtype\" v= \"detection_area\" /> <member type= \"way\" ref= \"11\" role= \"refers\" /> <member type= \"way\" ref= \"12\" role= \"ref_line\" /> </relation>","title":"Detection Area"},{"location":"map/lanelet2_extension/docs/extra_regulatory_elements/#road-marking","text":"This regulatory element specifies related road markings to a lanelet as shown below. * Note that the stopline in the image is for stoplines that are for reference, and normal stoplines should be expressed using TrafficSign regulatory element. refers: linestring with type attribute. Type explains what road marking it represents (e.g. stopline).","title":"Road Marking"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/","text":"Modifying Lanelet2 format for Autoware # Overview # About the basics of the default format, please refer to main Lanelet2 repository . (see here about primitives) In addition to default Lanelet2 Format, users should add following mandatory/optional tags to their osm lanelet files as explained in reset of this document. Users may use autoware_lanelet2_validation node to check if their maps are valid. The following is the extra format added for Autoware: extra regulatory elements Detection Area Road Marking extra lanelet subtype Roadside Lane Mandatory Tags # Elevation Tags # Elevation(\"ele\") information for points( node ) is optional in default Lanelet2 format. However, some of Autoware packages(e.g. trafficlight_recognizer) need elevation to be included in HD map. Therefore, users must make sure that all points in their osm maps contain elevation tags. Here is an example osm syntax for node object. <node id= '1' visible= 'true' version= '1' lat= '49.00501435943' lon= '8.41687458512' > <tag k= 'ele' v= '3.0' /> <!-- this tag is mandatory for Autoware!! --> </node> TrafficLights # Default Lanelet2 format uses LineString( way ) or Polygon class to represent the shape of a traffic light. For Autoware, traffic light objects must be represented only by LineString to avoid confusion, where start point is at bottom left edge and end point is at bottom right edge. Also, \"height\" tag must be added in order to represent the size in vertical direction (not the position). The Following image illustrates how LineString is used to represent shape of Traffic Light in Autoware. Here is an example osm syntax for traffic light object. <way id= '13' visible= 'true' version= '1' > <nd ref= '6' /> <nd ref= '5' /> <tag k= 'type' v= 'traffic_light' /> <tag k= 'subtype' v= 'red_yellow_green' /> <tag k= 'height' v= '0.5' /> <!-- this tag is mandatory for Autoware!! --> </way> Turn Directions # Users must add \"turn_direction\" tags to lanelets within intersections to indicate vehicle's turning direction. You do not need this tags for lanelets that are not in intersections. If you do not have this tag, Autoware will not be able to light up turning indicators. This tags only take following values: left right straight Following image illustrates how lanelets should be tagged. Here is an example of osm syntax for lanelets in intersections. <relation id= '1' visible= 'true' version= '1' > <member type= 'way' ref= '2' role= 'left' /> <member type= 'way' ref= '3' role= 'right' /> <member type= 'relation' ref= '4' role= 'regulatory_element' /> <tag k= 'location' v= 'urban' /> <tag k= 'one_way' v= 'yes' /> <tag k= 'subtype' v= 'road' /> <tag k= 'type' v= 'lanelet' /> <tag k= 'turn_direction' v= 'left' /> <!-- this tag is mandatory for lanelets at intersections!! --> </relation> Optional Taggings # Following tags are optional tags that you may want to add depending on how you want to use your map in Autoware. Meta Info # Users may add the MetaInfo element to their OSM file to indicate format version and map version of their OSM file. This information is not meant to influence Autoware vehicle's behavior, but is published as ROS message so that developers could know which map was used from ROSBAG log files. MetaInfo elements exists in the same hierarchy with node , way , and relation elements, otherwise JOSM wouldn't be able to load the file correctly. Here is an example of MetaInfo in osm file: <?xml version='1.0' encoding='UTF-8'?> <osm version= '0.6' generator= 'JOSM' > <MetaInfo format_version= '1.0' map_version= '1.0' /> <node> ... </node> <way> ... </way> <relation> ... </relation> </osm> Local Coordinate Expression # Sometimes users might want to create Lanelet2 maps that are not georeferenced. In such a case, users may use \"local_x\", \"local_y\" taggings to express local positions instead of latitude and longitude. Autoware Osm Parser will overwrite x,y positions with these tags when they are present. For z values, use \"ele\" tags as default Lanelet2 Format. You would still need to fill in lat and lon attributes so that parser does not crush, but their values could be anything. Here is example node element in osm with \"local_x\", \"local_y\" taggings: <!-- lat/lon attributes are required, but their values can be anything --> <node id= '40648' visible= 'true' version= '1' lat= '0' lon= '0' > <tag k= 'local_x' v= 2.54'/ > <tag k= 'local_y' v= 4.38'/ > <tag k= 'ele' v= '3.0' /> </node> Light Bulbs in Traffic Lights # Default Lanelet format can only express shape (base + height) of traffic lights. However, region_tlr node in Autoware uses positions of each light bulbs to recognize color of traffic light. If users may wish to use this node, \"light_bulbs\"( way ) element must be registered to traffic_light regulatory_element object define position and color of each light bulb in traffic lights. If you are using other trafficlight_recognizer nodes(e.g. tlr_mxnet), which only uses bounding box of traffic light, then you do not need to add this object. \"light_bulbs\" object is defined using LineString( way ), and each node of line string is placed at the center of each light bulb. Also, each node should have \"color\" and optionally \"arrow\" tags to describe its type. Also, \"traffic_light_id\" tag is used to indicate which ID of relevant traffic_light element. \"color\" tag is used to express the color of the light bulb. Currently only three values are used: red yellow green \"arrow\" tag is used to express the direction of the arrow of light bulb: up right left up_right up_left Following image illustrates how \"light_bulbs\" LineString should be created. Here is an example of osm syntax for light_bulb object: <node id= 1 version= '1' lat= '49.00541994701' lon= '8.41565013855' > <tag k= 'ele' v= '5' /> <tag k= 'color' v= 'red' /> </node> <node id= 2 version= '1' lat= '49.00542091657' lon= '8.4156469497' > <tag k= 'ele' v= '5' /> <tag k= 'color' v= 'yellow' /> </node> <node id= 3 version= '1' lat= '49.00542180052' lon= '8.41564400223' > <tag k= 'ele' v= '5' /> <tag k= 'color' v= 'green' /> </node> <node id= 3 version= '1' lat= '49.00542180052' lon= '8.41564400223' > <tag k= 'ele' v= '4.6' /> <tag k= 'color' v= 'green' /> <tag k= arrow v= 'right' /> </node> <way id= 11 version= '1' > <nd ref= 1 /> <nd ref= 2 /> <nd ref= 3 /> <tag k= 'traffic_light_id' v= '10' /> <!-- id of linestring with type=\"traffic_light\" --> <tag k= 'type' v= 'light_bulbs' /> </way> After creating \"light_bulbs\" elements, you have to register them to traffic_light regulatory element as role \"light_bulbs\". The following illustrates how light_bulbs are registered to traffic_light regulatory elements. <relation id= '8' visible= 'true' version= '1' > <tag k= 'type' v= 'regulatory_element' /> <tag k= 'subtype' v= 'traffic_light' /> <member type= 'way' ref= '9' role= 'ref_line' /> <member type= 'way' ref= '10' role= 'refers' /> <!-- refers to the traffic light line string --> <member type= 'way' ref= '11' role= 'light_bulbs' /> <!-- refers to the light_bulb line string --> </relation>","title":"Modifying Lanelet2 format for Autoware"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#modifying-lanelet2-format-for-autoware","text":"","title":"Modifying Lanelet2 format for Autoware"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#overview","text":"About the basics of the default format, please refer to main Lanelet2 repository . (see here about primitives) In addition to default Lanelet2 Format, users should add following mandatory/optional tags to their osm lanelet files as explained in reset of this document. Users may use autoware_lanelet2_validation node to check if their maps are valid. The following is the extra format added for Autoware: extra regulatory elements Detection Area Road Marking extra lanelet subtype Roadside Lane","title":"Overview"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#mandatory-tags","text":"","title":"Mandatory Tags"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#elevation-tags","text":"Elevation(\"ele\") information for points( node ) is optional in default Lanelet2 format. However, some of Autoware packages(e.g. trafficlight_recognizer) need elevation to be included in HD map. Therefore, users must make sure that all points in their osm maps contain elevation tags. Here is an example osm syntax for node object. <node id= '1' visible= 'true' version= '1' lat= '49.00501435943' lon= '8.41687458512' > <tag k= 'ele' v= '3.0' /> <!-- this tag is mandatory for Autoware!! --> </node>","title":"Elevation Tags"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#trafficlights","text":"Default Lanelet2 format uses LineString( way ) or Polygon class to represent the shape of a traffic light. For Autoware, traffic light objects must be represented only by LineString to avoid confusion, where start point is at bottom left edge and end point is at bottom right edge. Also, \"height\" tag must be added in order to represent the size in vertical direction (not the position). The Following image illustrates how LineString is used to represent shape of Traffic Light in Autoware. Here is an example osm syntax for traffic light object. <way id= '13' visible= 'true' version= '1' > <nd ref= '6' /> <nd ref= '5' /> <tag k= 'type' v= 'traffic_light' /> <tag k= 'subtype' v= 'red_yellow_green' /> <tag k= 'height' v= '0.5' /> <!-- this tag is mandatory for Autoware!! --> </way>","title":"TrafficLights"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#turn-directions","text":"Users must add \"turn_direction\" tags to lanelets within intersections to indicate vehicle's turning direction. You do not need this tags for lanelets that are not in intersections. If you do not have this tag, Autoware will not be able to light up turning indicators. This tags only take following values: left right straight Following image illustrates how lanelets should be tagged. Here is an example of osm syntax for lanelets in intersections. <relation id= '1' visible= 'true' version= '1' > <member type= 'way' ref= '2' role= 'left' /> <member type= 'way' ref= '3' role= 'right' /> <member type= 'relation' ref= '4' role= 'regulatory_element' /> <tag k= 'location' v= 'urban' /> <tag k= 'one_way' v= 'yes' /> <tag k= 'subtype' v= 'road' /> <tag k= 'type' v= 'lanelet' /> <tag k= 'turn_direction' v= 'left' /> <!-- this tag is mandatory for lanelets at intersections!! --> </relation>","title":"Turn Directions"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#optional-taggings","text":"Following tags are optional tags that you may want to add depending on how you want to use your map in Autoware.","title":"Optional Taggings"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#meta-info","text":"Users may add the MetaInfo element to their OSM file to indicate format version and map version of their OSM file. This information is not meant to influence Autoware vehicle's behavior, but is published as ROS message so that developers could know which map was used from ROSBAG log files. MetaInfo elements exists in the same hierarchy with node , way , and relation elements, otherwise JOSM wouldn't be able to load the file correctly. Here is an example of MetaInfo in osm file: <?xml version='1.0' encoding='UTF-8'?> <osm version= '0.6' generator= 'JOSM' > <MetaInfo format_version= '1.0' map_version= '1.0' /> <node> ... </node> <way> ... </way> <relation> ... </relation> </osm>","title":"Meta Info"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#local-coordinate-expression","text":"Sometimes users might want to create Lanelet2 maps that are not georeferenced. In such a case, users may use \"local_x\", \"local_y\" taggings to express local positions instead of latitude and longitude. Autoware Osm Parser will overwrite x,y positions with these tags when they are present. For z values, use \"ele\" tags as default Lanelet2 Format. You would still need to fill in lat and lon attributes so that parser does not crush, but their values could be anything. Here is example node element in osm with \"local_x\", \"local_y\" taggings: <!-- lat/lon attributes are required, but their values can be anything --> <node id= '40648' visible= 'true' version= '1' lat= '0' lon= '0' > <tag k= 'local_x' v= 2.54'/ > <tag k= 'local_y' v= 4.38'/ > <tag k= 'ele' v= '3.0' /> </node>","title":"Local Coordinate Expression"},{"location":"map/lanelet2_extension/docs/lanelet2_format_extension/#light-bulbs-in-traffic-lights","text":"Default Lanelet format can only express shape (base + height) of traffic lights. However, region_tlr node in Autoware uses positions of each light bulbs to recognize color of traffic light. If users may wish to use this node, \"light_bulbs\"( way ) element must be registered to traffic_light regulatory_element object define position and color of each light bulb in traffic lights. If you are using other trafficlight_recognizer nodes(e.g. tlr_mxnet), which only uses bounding box of traffic light, then you do not need to add this object. \"light_bulbs\" object is defined using LineString( way ), and each node of line string is placed at the center of each light bulb. Also, each node should have \"color\" and optionally \"arrow\" tags to describe its type. Also, \"traffic_light_id\" tag is used to indicate which ID of relevant traffic_light element. \"color\" tag is used to express the color of the light bulb. Currently only three values are used: red yellow green \"arrow\" tag is used to express the direction of the arrow of light bulb: up right left up_right up_left Following image illustrates how \"light_bulbs\" LineString should be created. Here is an example of osm syntax for light_bulb object: <node id= 1 version= '1' lat= '49.00541994701' lon= '8.41565013855' > <tag k= 'ele' v= '5' /> <tag k= 'color' v= 'red' /> </node> <node id= 2 version= '1' lat= '49.00542091657' lon= '8.4156469497' > <tag k= 'ele' v= '5' /> <tag k= 'color' v= 'yellow' /> </node> <node id= 3 version= '1' lat= '49.00542180052' lon= '8.41564400223' > <tag k= 'ele' v= '5' /> <tag k= 'color' v= 'green' /> </node> <node id= 3 version= '1' lat= '49.00542180052' lon= '8.41564400223' > <tag k= 'ele' v= '4.6' /> <tag k= 'color' v= 'green' /> <tag k= arrow v= 'right' /> </node> <way id= 11 version= '1' > <nd ref= 1 /> <nd ref= 2 /> <nd ref= 3 /> <tag k= 'traffic_light_id' v= '10' /> <!-- id of linestring with type=\"traffic_light\" --> <tag k= 'type' v= 'light_bulbs' /> </way> After creating \"light_bulbs\" elements, you have to register them to traffic_light regulatory element as role \"light_bulbs\". The following illustrates how light_bulbs are registered to traffic_light regulatory elements. <relation id= '8' visible= 'true' version= '1' > <tag k= 'type' v= 'regulatory_element' /> <tag k= 'subtype' v= 'traffic_light' /> <member type= 'way' ref= '9' role= 'ref_line' /> <member type= 'way' ref= '10' role= 'refers' /> <!-- refers to the traffic light line string --> <member type= 'way' ref= '11' role= 'light_bulbs' /> <!-- refers to the light_bulb line string --> </relation>","title":"Light Bulbs in Traffic Lights"},{"location":"map/map_loader/","text":"map_loader package # This package provides the features of loading various maps. pointcloud_map_loader # Feature # pointcloud_map_loader loads PointCloud file and publishes the map data as sensor_msgs/PointCloud2 message. How to run # ros2 run map_loader pointcloud_map_loader --ros-args -p \"pcd_paths_or_directory:=[path/to/pointcloud1.pcd, path/to/pointcloud2.pcd, ...]\" Published Topics # pointcloud_map (sensor_msgs/PointCloud2) : PointCloud Map lanelet2_map_loader # Feature # lanelet2_map_loader loads Lanelet2 file and publishes the map data as autoware_auto_mapping_msgs/HADMapBin message. The node projects lan/lon coordinates into MGRS coordinates. How to run # ros2 run map_loader lanelet2_map_loader --ros-args -p lanelet2_map_path:=path/to/map.osm Published Topics # ~output/lanelet2_map (autoware_auto_mapping_msgs/HADMapBin) : Binary data of loaded Lanelet2 Map lanelet2_map_visualization # Feature # lanelet2_map_visualization visualizes autoware_auto_mapping_msgs/HADMapBin messages into visualization_msgs/MarkerArray. How to Run # ros2 run map_loader lanelet2_map_visualization Subscribed Topics # ~input/lanelet2_map (autoware_auto_mapping_msgs/HADMapBin) : binary data of Lanelet2 Map Published Topics # ~output/lanelet2_map_marker (visualization_msgs/MarkerArray) : visualization messages for RViz","title":"map_loader package"},{"location":"map/map_loader/#map_loader-package","text":"This package provides the features of loading various maps.","title":"map_loader package"},{"location":"map/map_loader/#pointcloud_map_loader","text":"","title":"pointcloud_map_loader"},{"location":"map/map_loader/#feature","text":"pointcloud_map_loader loads PointCloud file and publishes the map data as sensor_msgs/PointCloud2 message.","title":"Feature"},{"location":"map/map_loader/#how-to-run","text":"ros2 run map_loader pointcloud_map_loader --ros-args -p \"pcd_paths_or_directory:=[path/to/pointcloud1.pcd, path/to/pointcloud2.pcd, ...]\"","title":"How to run"},{"location":"map/map_loader/#published-topics","text":"pointcloud_map (sensor_msgs/PointCloud2) : PointCloud Map","title":"Published Topics"},{"location":"map/map_loader/#lanelet2_map_loader","text":"","title":"lanelet2_map_loader"},{"location":"map/map_loader/#feature_1","text":"lanelet2_map_loader loads Lanelet2 file and publishes the map data as autoware_auto_mapping_msgs/HADMapBin message. The node projects lan/lon coordinates into MGRS coordinates.","title":"Feature"},{"location":"map/map_loader/#how-to-run_1","text":"ros2 run map_loader lanelet2_map_loader --ros-args -p lanelet2_map_path:=path/to/map.osm","title":"How to run"},{"location":"map/map_loader/#published-topics_1","text":"~output/lanelet2_map (autoware_auto_mapping_msgs/HADMapBin) : Binary data of loaded Lanelet2 Map","title":"Published Topics"},{"location":"map/map_loader/#lanelet2_map_visualization","text":"","title":"lanelet2_map_visualization"},{"location":"map/map_loader/#feature_2","text":"lanelet2_map_visualization visualizes autoware_auto_mapping_msgs/HADMapBin messages into visualization_msgs/MarkerArray.","title":"Feature"},{"location":"map/map_loader/#how-to-run_2","text":"ros2 run map_loader lanelet2_map_visualization","title":"How to Run"},{"location":"map/map_loader/#subscribed-topics","text":"~input/lanelet2_map (autoware_auto_mapping_msgs/HADMapBin) : binary data of Lanelet2 Map","title":"Subscribed Topics"},{"location":"map/map_loader/#published-topics_2","text":"~output/lanelet2_map_marker (visualization_msgs/MarkerArray) : visualization messages for RViz","title":"Published Topics"},{"location":"map/map_tf_generator/Readme/","text":"map_tf_generator # Purpose # This node broadcasts viewer frames for visualization of pointcloud map in Rviz. The position of viewer frames is the geometric center of input pointclouds. Note that there is no module to need viewer frames and this is used only for visualization. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description /map/pointcloud_map sensor_msgs::msg::PointCloud2 Subscribe pointcloud map to calculate position of viewer frames Output # Name Type Description /tf_static tf2_msgs/msg/TFMessage Broadcast viewer frames Parameters # Node Parameters # None Core Parameters # Name Type Default Value Explanation viewer_frame string viewer Name of viewer frame map_frame string map The parent frame name of viewer frame Assumptions / Known limits # TBD.","title":"map_tf_generator"},{"location":"map/map_tf_generator/Readme/#map_tf_generator","text":"","title":"map_tf_generator"},{"location":"map/map_tf_generator/Readme/#purpose","text":"This node broadcasts viewer frames for visualization of pointcloud map in Rviz. The position of viewer frames is the geometric center of input pointclouds. Note that there is no module to need viewer frames and this is used only for visualization.","title":"Purpose"},{"location":"map/map_tf_generator/Readme/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"map/map_tf_generator/Readme/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"map/map_tf_generator/Readme/#input","text":"Name Type Description /map/pointcloud_map sensor_msgs::msg::PointCloud2 Subscribe pointcloud map to calculate position of viewer frames","title":"Input"},{"location":"map/map_tf_generator/Readme/#output","text":"Name Type Description /tf_static tf2_msgs/msg/TFMessage Broadcast viewer frames","title":"Output"},{"location":"map/map_tf_generator/Readme/#parameters","text":"","title":"Parameters"},{"location":"map/map_tf_generator/Readme/#node-parameters","text":"None","title":"Node Parameters"},{"location":"map/map_tf_generator/Readme/#core-parameters","text":"Name Type Default Value Explanation viewer_frame string viewer Name of viewer frame map_frame string map The parent frame name of viewer frame","title":"Core Parameters"},{"location":"map/map_tf_generator/Readme/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"perception/compare_map_segmentation/","text":"compare_map_segmentation # Purpose # The compare_map_segmentation is a node that filters the ground points from the input pointcloud by using map info (e.g. pcd, elevation map). Inner-workings / Algorithms # Compare Elevation Map Filter # Compare the z of the input points with the value of elevation_map. The height difference is calculated by the binary integration of neighboring cells. Remove points whose height difference is below the height_diff_thresh . Distance Based Compare Map Filter # WIP Voxel Based Approximate Compare Map Filter # WIP Voxel Based Compare Map Filter # WIP Voxel Distance based Compare Map Filter # WIP Inputs / Outputs # Compare Elevation Map Filter # Input # Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/elevation_map grid_map::msg::GridMap elevation map Output # Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points Other Filters # Input # Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/map grid_map::msg::GridMap map Output # Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points Parameters # Core Parameters # Name Type Description Default value map_layer_name string elevation map layer name elevation map_frame float frame_id of the map that is temporarily used before elevation_map is subscribed map height_diff_thresh float Remove points whose height difference is below this value [m] 0.15 Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"compare_map_segmentation"},{"location":"perception/compare_map_segmentation/#compare_map_segmentation","text":"","title":"compare_map_segmentation"},{"location":"perception/compare_map_segmentation/#purpose","text":"The compare_map_segmentation is a node that filters the ground points from the input pointcloud by using map info (e.g. pcd, elevation map).","title":"Purpose"},{"location":"perception/compare_map_segmentation/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"perception/compare_map_segmentation/#compare-elevation-map-filter","text":"Compare the z of the input points with the value of elevation_map. The height difference is calculated by the binary integration of neighboring cells. Remove points whose height difference is below the height_diff_thresh .","title":"Compare Elevation Map Filter"},{"location":"perception/compare_map_segmentation/#distance-based-compare-map-filter","text":"WIP","title":"Distance Based Compare Map Filter"},{"location":"perception/compare_map_segmentation/#voxel-based-approximate-compare-map-filter","text":"WIP","title":"Voxel Based Approximate Compare Map Filter"},{"location":"perception/compare_map_segmentation/#voxel-based-compare-map-filter","text":"WIP","title":"Voxel Based Compare Map Filter"},{"location":"perception/compare_map_segmentation/#voxel-distance-based-compare-map-filter","text":"WIP","title":"Voxel Distance based Compare Map Filter"},{"location":"perception/compare_map_segmentation/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/compare_map_segmentation/#compare-elevation-map-filter_1","text":"","title":"Compare Elevation Map Filter"},{"location":"perception/compare_map_segmentation/#input","text":"Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/elevation_map grid_map::msg::GridMap elevation map","title":"Input"},{"location":"perception/compare_map_segmentation/#output","text":"Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"perception/compare_map_segmentation/#other-filters","text":"","title":"Other Filters"},{"location":"perception/compare_map_segmentation/#input_1","text":"Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/map grid_map::msg::GridMap map","title":"Input"},{"location":"perception/compare_map_segmentation/#output_1","text":"Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"perception/compare_map_segmentation/#parameters","text":"","title":"Parameters"},{"location":"perception/compare_map_segmentation/#core-parameters","text":"Name Type Description Default value map_layer_name string elevation map layer name elevation map_frame float frame_id of the map that is temporarily used before elevation_map is subscribed map height_diff_thresh float Remove points whose height difference is below this value [m] 0.15","title":"Core Parameters"},{"location":"perception/compare_map_segmentation/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/compare_map_segmentation/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/compare_map_segmentation/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/compare_map_segmentation/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"perception/compare_map_segmentation/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/detected_object_feature_remover/","text":"detected_object_feature_remover # Purpose # The detected_object_feature_remover is a package to convert topic-type from DetectedObjectWithFeatureArray to DetectedObjects . Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description ~/input tier4_perception_msgs::msg::DetectedObjectWithFeatureArray detected objects with feature field Output # Name Type Description ~/output autoware_auto_perception_msgs::msg::DetectedObjects detected objects Parameters # None Assumptions / Known limits #","title":"detected_object_feature_remover"},{"location":"perception/detected_object_feature_remover/#detected_object_feature_remover","text":"","title":"detected_object_feature_remover"},{"location":"perception/detected_object_feature_remover/#purpose","text":"The detected_object_feature_remover is a package to convert topic-type from DetectedObjectWithFeatureArray to DetectedObjects .","title":"Purpose"},{"location":"perception/detected_object_feature_remover/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"perception/detected_object_feature_remover/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/detected_object_feature_remover/#input","text":"Name Type Description ~/input tier4_perception_msgs::msg::DetectedObjectWithFeatureArray detected objects with feature field","title":"Input"},{"location":"perception/detected_object_feature_remover/#output","text":"Name Type Description ~/output autoware_auto_perception_msgs::msg::DetectedObjects detected objects","title":"Output"},{"location":"perception/detected_object_feature_remover/#parameters","text":"None","title":"Parameters"},{"location":"perception/detected_object_feature_remover/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/detected_object_validation/","text":"detected_object_validation # Purpose # The purpose of this package is to eliminate obvious false positives of DetectedObjects. References/External links # Obstacle pointcloud based validator Occupancy grid based validator","title":"detected_object_validation"},{"location":"perception/detected_object_validation/#detected_object_validation","text":"","title":"detected_object_validation"},{"location":"perception/detected_object_validation/#purpose","text":"The purpose of this package is to eliminate obvious false positives of DetectedObjects.","title":"Purpose"},{"location":"perception/detected_object_validation/#referencesexternal-links","text":"Obstacle pointcloud based validator Occupancy grid based validator","title":"References/External links"},{"location":"perception/detected_object_validation/obstacle-pointcloud-based-validator/","text":"obstacle pointcloud based validator # Inner-workings / Algorithms # If the number of obstacle point groups in the DetectedObjects is small, it is considered a false positive and removed. The obstacle point cloud can be a point cloud after compare map filtering or a ground filtered point cloud. In the debug image above, the red DetectedObject is the validated object. The blue object is the deleted object. Inputs / Outputs # Input # Name Type Description ~/input/detected_objects autoware_auto_perception_msgs::msg::DetectedObjects DetectedObjects ~/input/obstacle_pointcloud sensor_msgs::msg::PointCloud2 Obstacle point cloud of dynamic objects Output # Name Type Description ~/output/objects autoware_auto_perception_msgs::msg::DetectedObjects validated DetectedObjects Parameters # Name Type Description min_pointcloud_num float Threshold for the minimum number of obstacle point clouds in DetectedObjects enable_debugger bool Whether to create debug topics or not? Assumptions / Known limits # Currently, only represented objects as BoundingBox or Cylinder are supported.","title":"obstacle pointcloud based validator"},{"location":"perception/detected_object_validation/obstacle-pointcloud-based-validator/#obstacle-pointcloud-based-validator","text":"","title":"obstacle pointcloud based validator"},{"location":"perception/detected_object_validation/obstacle-pointcloud-based-validator/#inner-workings-algorithms","text":"If the number of obstacle point groups in the DetectedObjects is small, it is considered a false positive and removed. The obstacle point cloud can be a point cloud after compare map filtering or a ground filtered point cloud. In the debug image above, the red DetectedObject is the validated object. The blue object is the deleted object.","title":"Inner-workings / Algorithms"},{"location":"perception/detected_object_validation/obstacle-pointcloud-based-validator/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/detected_object_validation/obstacle-pointcloud-based-validator/#input","text":"Name Type Description ~/input/detected_objects autoware_auto_perception_msgs::msg::DetectedObjects DetectedObjects ~/input/obstacle_pointcloud sensor_msgs::msg::PointCloud2 Obstacle point cloud of dynamic objects","title":"Input"},{"location":"perception/detected_object_validation/obstacle-pointcloud-based-validator/#output","text":"Name Type Description ~/output/objects autoware_auto_perception_msgs::msg::DetectedObjects validated DetectedObjects","title":"Output"},{"location":"perception/detected_object_validation/obstacle-pointcloud-based-validator/#parameters","text":"Name Type Description min_pointcloud_num float Threshold for the minimum number of obstacle point clouds in DetectedObjects enable_debugger bool Whether to create debug topics or not?","title":"Parameters"},{"location":"perception/detected_object_validation/obstacle-pointcloud-based-validator/#assumptions-known-limits","text":"Currently, only represented objects as BoundingBox or Cylinder are supported.","title":"Assumptions / Known limits"},{"location":"perception/detected_object_validation/occupancy-grid-based-validator/","text":"occupancy grid based validator # Inner-workings / Algorithms # Compare the occupancy grid map with the DetectedObject, and if a larger percentage of obstacles are in freespace, delete them. Basically, it takes an occupancy grid map as input and generates a binary image of freespace or other. A mask image is generated for each DetectedObject and the average value (percentage) in the mask image is calculated. If the percentage is low, it is deleted. Inputs / Outputs # Input # Name Type Description ~/input/detected_objects autoware_auto_perception_msgs::msg::DetectedObjects DetectedObjects ~/input/occupancy_grid_map nav_msgs::msg::OccupancyGrid OccupancyGrid with no time series calculation is preferred. Output # Name Type Description ~/output/objects autoware_auto_perception_msgs::msg::DetectedObjects validated DetectedObjects Parameters # Name Type Description mean_threshold float The percentage threshold of allowed non-freespace. enable_debug bool Whether to display debug images or not? Assumptions / Known limits # Currently, only vehicle represented as BoundingBox are supported.","title":"occupancy grid based validator"},{"location":"perception/detected_object_validation/occupancy-grid-based-validator/#occupancy-grid-based-validator","text":"","title":"occupancy grid based validator"},{"location":"perception/detected_object_validation/occupancy-grid-based-validator/#inner-workings-algorithms","text":"Compare the occupancy grid map with the DetectedObject, and if a larger percentage of obstacles are in freespace, delete them. Basically, it takes an occupancy grid map as input and generates a binary image of freespace or other. A mask image is generated for each DetectedObject and the average value (percentage) in the mask image is calculated. If the percentage is low, it is deleted.","title":"Inner-workings / Algorithms"},{"location":"perception/detected_object_validation/occupancy-grid-based-validator/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/detected_object_validation/occupancy-grid-based-validator/#input","text":"Name Type Description ~/input/detected_objects autoware_auto_perception_msgs::msg::DetectedObjects DetectedObjects ~/input/occupancy_grid_map nav_msgs::msg::OccupancyGrid OccupancyGrid with no time series calculation is preferred.","title":"Input"},{"location":"perception/detected_object_validation/occupancy-grid-based-validator/#output","text":"Name Type Description ~/output/objects autoware_auto_perception_msgs::msg::DetectedObjects validated DetectedObjects","title":"Output"},{"location":"perception/detected_object_validation/occupancy-grid-based-validator/#parameters","text":"Name Type Description mean_threshold float The percentage threshold of allowed non-freespace. enable_debug bool Whether to display debug images or not?","title":"Parameters"},{"location":"perception/detected_object_validation/occupancy-grid-based-validator/#assumptions-known-limits","text":"Currently, only vehicle represented as BoundingBox are supported.","title":"Assumptions / Known limits"},{"location":"perception/detection_by_tracker/","text":"detection_by_tracker # Purpose # This package feeds back the tracked objects to the detection module to keep it stable and keep detecting objects. The detection by tracker takes as input an unknown object containing a cluster of points and a tracker. The unknown object is optimized to fit the size of the tracker so that it can continue to be detected. Inner-workings / Algorithms # The detection by tracker receives an unknown object containing a point cloud and a tracker, where the unknown object is mainly shape-fitted using euclidean clustering. Shape fitting using euclidean clustering and other methods has a problem called under segmentation and over segmentation. Adapted from [3] Simply looking at the overlap between the unknown object and the tracker does not work. We need to take measures for under segmentation and over segmentation. Policy for dealing with over segmentation # Merge the unknown objects in the tracker as a single object. Shape fitting using the tracker information such as angle and size as reference information. Policy for dealing with under segmentation # Compare the tracker and unknown objects, and determine that those with large recall and small precision are under segmented objects. In order to divide the cluster of under segmented objects, it iterate the parameters to make small clusters. Adjust the parameters several times and adopt the one with the highest IoU. Inputs / Outputs # Input # Name Type Description ~/input/initial_objects tier4_perception_msgs::msg::DetectedObjectsWithFeature unknown objects ~/input/tracked_objects tier4_perception_msgs::msg::TrackedObjects trackers Output # Name Type Description ~/output autoware_auto_perception_msgs::msg::DetectedObjects objects Parameters # Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # [1] M. Himmelsbach, et al. \"Tracking and classification of arbitrary objects with bottom-up/top-down detection.\" (2012). [2] Arya Senna Abdul Rachman, Arya. \"3D-LIDAR Multi Object Tracking for Autonomous Driving: Multi-target Detection and Tracking under Urban Road Uncertainties.\" (2017). [3] David Held, et al. \"A Probabilistic Framework for Real-time 3D Segmentation using Spatial, Temporal, and Semantic Cues.\" (2016). (Optional) Future extensions / Unimplemented parts #","title":"detection_by_tracker"},{"location":"perception/detection_by_tracker/#detection_by_tracker","text":"","title":"detection_by_tracker"},{"location":"perception/detection_by_tracker/#purpose","text":"This package feeds back the tracked objects to the detection module to keep it stable and keep detecting objects. The detection by tracker takes as input an unknown object containing a cluster of points and a tracker. The unknown object is optimized to fit the size of the tracker so that it can continue to be detected.","title":"Purpose"},{"location":"perception/detection_by_tracker/#inner-workings-algorithms","text":"The detection by tracker receives an unknown object containing a point cloud and a tracker, where the unknown object is mainly shape-fitted using euclidean clustering. Shape fitting using euclidean clustering and other methods has a problem called under segmentation and over segmentation. Adapted from [3] Simply looking at the overlap between the unknown object and the tracker does not work. We need to take measures for under segmentation and over segmentation.","title":"Inner-workings / Algorithms"},{"location":"perception/detection_by_tracker/#policy-for-dealing-with-over-segmentation","text":"Merge the unknown objects in the tracker as a single object. Shape fitting using the tracker information such as angle and size as reference information.","title":"Policy for dealing with over segmentation"},{"location":"perception/detection_by_tracker/#policy-for-dealing-with-under-segmentation","text":"Compare the tracker and unknown objects, and determine that those with large recall and small precision are under segmented objects. In order to divide the cluster of under segmented objects, it iterate the parameters to make small clusters. Adjust the parameters several times and adopt the one with the highest IoU.","title":"Policy for dealing with under segmentation"},{"location":"perception/detection_by_tracker/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/detection_by_tracker/#input","text":"Name Type Description ~/input/initial_objects tier4_perception_msgs::msg::DetectedObjectsWithFeature unknown objects ~/input/tracked_objects tier4_perception_msgs::msg::TrackedObjects trackers","title":"Input"},{"location":"perception/detection_by_tracker/#output","text":"Name Type Description ~/output autoware_auto_perception_msgs::msg::DetectedObjects objects","title":"Output"},{"location":"perception/detection_by_tracker/#parameters","text":"","title":"Parameters"},{"location":"perception/detection_by_tracker/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/detection_by_tracker/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/detection_by_tracker/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/detection_by_tracker/#optional-referencesexternal-links","text":"[1] M. Himmelsbach, et al. \"Tracking and classification of arbitrary objects with bottom-up/top-down detection.\" (2012). [2] Arya Senna Abdul Rachman, Arya. \"3D-LIDAR Multi Object Tracking for Autonomous Driving: Multi-target Detection and Tracking under Urban Road Uncertainties.\" (2017). [3] David Held, et al. \"A Probabilistic Framework for Real-time 3D Segmentation using Spatial, Temporal, and Semantic Cues.\" (2016).","title":"(Optional) References/External links"},{"location":"perception/detection_by_tracker/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/elevation_map_loader/","text":"elevation_map_loader # Purpose # This package provides elevation map for compare_map_segmentation. Inner-workings / Algorithms # Generate elevation_map from subscribed pointcloud_map and vector_map and publish it. Save the generated elevation_map locally and load it from next time. The elevation value of each cell is the average value of z of the points of the lowest cluster. Cells with No elevation value can be inpainted using the values of neighboring cells. Inputs / Outputs # Input # Name Type Description input/pointcloud_map sensor_msgs::msg::PointCloud2 The point cloud map input/vector_map autoware_auto_mapping_msgs::msg::HADMapBin (Optional) The binary data of lanelet2 map Output # Name Type Description output/elevation_map grid_map_msgs::msg::GridMap The elevation map output/elevation_map_cloud sensor_msgs::msg::PointCloud2 (Optional) The point cloud generated from the value of elevation map Parameters # Node parameters # Name Type Description Default value map_layer_name std::string elevation_map layer name elevation param_file_path std::string GridMap parameters config path_default elevation_map_file_path std::string elevation_map file (bag2) path_default map_frame std::string map_frame when loading elevation_map file map use_inpaint bool Whether to inpaint empty cells true inpaint_radius float Radius of a circular neighborhood of each point inpainted that is considered by the algorithm [m] 0.3 use_elevation_map_cloud_publisher bool Whether to publish output/elevation_map_cloud false use_lane_filter bool Whether to filter elevation_map with vector_map false lane_margin float Value of how much to expand the range of vector_map [m] 0.5 lane_height_diff_thresh float Only point clouds in the height range of this value from vector_map are used to generate elevation_map [m] 1.0 lane_filter_voxel_size_x float Voxel size x for calculating point clouds in vector_map [m] 0.04 lane_filter_voxel_size_y float Voxel size y for calculating point clouds in vector_map [m] 0.04 lane_filter_voxel_size_z float Voxel size z for calculating point clouds in vector_map [m] 0.04 GridMap parameters # The parameters are described on config/elevation_map_parameters.yaml . General parameters # Name Type Description Default value pcl_grid_map_extraction/num_processing_threads int Number of threads for processing grid map cells. Filtering of the raw input point cloud is not parallelized. 12 Grid map parameters # See: https://github.com/ANYbotics/grid_map/tree/ros2/grid_map_pcl Resulting grid map parameters. Name Type Description Default value pcl_grid_map_extraction/grid_map/min_num_points_per_cell int Minimum number of points in the point cloud that have to fall within any of the grid map cells. Otherwise the cell elevation will be set to NaN. 3 pcl_grid_map_extraction/grid_map/resolution float Resolution of the grid map. Width and length are computed automatically. 0.3 pcl_grid_map_extraction/grid_map/height_type int The parameter that determine the elevation of a cell 0: Smallest value among the average values of each cluster , 1: Mean value of the cluster with the most points 1 pcl_grid_map_extraction/grid_map/height_thresh float Height range from the smallest cluster (Only for height_type 1) 1.0 Point Cloud Pre-processing Parameters # Rigid body transform parameters # Rigid body transform that is applied to the point cloud before computing elevation. Name Type Description Default value pcl_grid_map_extraction/cloud_transform/translation float Translation (xyz) that is applied to the input point cloud before computing elevation. 0.0 pcl_grid_map_extraction/cloud_transform/rotation float Rotation (intrinsic rotation, convention X-Y'-Z'') that is applied to the input point cloud before computing elevation. 0.0 Cluster extraction parameters # Cluster extraction is based on pcl algorithms. See https://pointclouds.org/documentation/tutorials/cluster_extraction.html for more details. Name Type Description Default value pcl_grid_map_extraction/cluster_extraction/cluster_tolerance float Distance between points below which they will still be considered part of one cluster. 0.2 pcl_grid_map_extraction/cluster_extraction/min_num_points int Min number of points that a cluster needs to have (otherwise it will be discarded). 3 pcl_grid_map_extraction/cluster_extraction/max_num_points int Max number of points that a cluster can have (otherwise it will be discarded). 1000000 Outlier removal parameters # See https://pointclouds.org/documentation/tutorials/statistical_outlier.html for more explanation on outlier removal. Name Type Description Default value pcl_grid_map_extraction/outlier_removal/is_remove_outliers float Whether to perform statistical outlier removal. false pcl_grid_map_extraction/outlier_removal/mean_K float Number of neighbours to analyze for estimating statistics of a point. 10 pcl_grid_map_extraction/outlier_removal/stddev_threshold float Number of standard deviations under which points are considered to be inliers. 1.0 Subsampling parameters # See https://pointclouds.org/documentation/tutorials/voxel_grid.html for more explanation on point cloud downsampling. Name Type Description Default value pcl_grid_map_extraction/downsampling/is_downsample_cloud bool Whether to perform downsampling or not. false pcl_grid_map_extraction/downsampling/voxel_size float Voxel sizes (xyz) in meters. 0.02","title":"elevation_map_loader"},{"location":"perception/elevation_map_loader/#elevation_map_loader","text":"","title":"elevation_map_loader"},{"location":"perception/elevation_map_loader/#purpose","text":"This package provides elevation map for compare_map_segmentation.","title":"Purpose"},{"location":"perception/elevation_map_loader/#inner-workings-algorithms","text":"Generate elevation_map from subscribed pointcloud_map and vector_map and publish it. Save the generated elevation_map locally and load it from next time. The elevation value of each cell is the average value of z of the points of the lowest cluster. Cells with No elevation value can be inpainted using the values of neighboring cells.","title":"Inner-workings / Algorithms"},{"location":"perception/elevation_map_loader/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/elevation_map_loader/#input","text":"Name Type Description input/pointcloud_map sensor_msgs::msg::PointCloud2 The point cloud map input/vector_map autoware_auto_mapping_msgs::msg::HADMapBin (Optional) The binary data of lanelet2 map","title":"Input"},{"location":"perception/elevation_map_loader/#output","text":"Name Type Description output/elevation_map grid_map_msgs::msg::GridMap The elevation map output/elevation_map_cloud sensor_msgs::msg::PointCloud2 (Optional) The point cloud generated from the value of elevation map","title":"Output"},{"location":"perception/elevation_map_loader/#parameters","text":"","title":"Parameters"},{"location":"perception/elevation_map_loader/#node-parameters","text":"Name Type Description Default value map_layer_name std::string elevation_map layer name elevation param_file_path std::string GridMap parameters config path_default elevation_map_file_path std::string elevation_map file (bag2) path_default map_frame std::string map_frame when loading elevation_map file map use_inpaint bool Whether to inpaint empty cells true inpaint_radius float Radius of a circular neighborhood of each point inpainted that is considered by the algorithm [m] 0.3 use_elevation_map_cloud_publisher bool Whether to publish output/elevation_map_cloud false use_lane_filter bool Whether to filter elevation_map with vector_map false lane_margin float Value of how much to expand the range of vector_map [m] 0.5 lane_height_diff_thresh float Only point clouds in the height range of this value from vector_map are used to generate elevation_map [m] 1.0 lane_filter_voxel_size_x float Voxel size x for calculating point clouds in vector_map [m] 0.04 lane_filter_voxel_size_y float Voxel size y for calculating point clouds in vector_map [m] 0.04 lane_filter_voxel_size_z float Voxel size z for calculating point clouds in vector_map [m] 0.04","title":"Node parameters"},{"location":"perception/elevation_map_loader/#gridmap-parameters","text":"The parameters are described on config/elevation_map_parameters.yaml .","title":"GridMap parameters"},{"location":"perception/elevation_map_loader/#general-parameters","text":"Name Type Description Default value pcl_grid_map_extraction/num_processing_threads int Number of threads for processing grid map cells. Filtering of the raw input point cloud is not parallelized. 12","title":"General parameters"},{"location":"perception/elevation_map_loader/#grid-map-parameters","text":"See: https://github.com/ANYbotics/grid_map/tree/ros2/grid_map_pcl Resulting grid map parameters. Name Type Description Default value pcl_grid_map_extraction/grid_map/min_num_points_per_cell int Minimum number of points in the point cloud that have to fall within any of the grid map cells. Otherwise the cell elevation will be set to NaN. 3 pcl_grid_map_extraction/grid_map/resolution float Resolution of the grid map. Width and length are computed automatically. 0.3 pcl_grid_map_extraction/grid_map/height_type int The parameter that determine the elevation of a cell 0: Smallest value among the average values of each cluster , 1: Mean value of the cluster with the most points 1 pcl_grid_map_extraction/grid_map/height_thresh float Height range from the smallest cluster (Only for height_type 1) 1.0","title":"Grid map parameters"},{"location":"perception/elevation_map_loader/#point-cloud-pre-processing-parameters","text":"","title":"Point Cloud Pre-processing Parameters"},{"location":"perception/elevation_map_loader/#rigid-body-transform-parameters","text":"Rigid body transform that is applied to the point cloud before computing elevation. Name Type Description Default value pcl_grid_map_extraction/cloud_transform/translation float Translation (xyz) that is applied to the input point cloud before computing elevation. 0.0 pcl_grid_map_extraction/cloud_transform/rotation float Rotation (intrinsic rotation, convention X-Y'-Z'') that is applied to the input point cloud before computing elevation. 0.0","title":"Rigid body transform parameters"},{"location":"perception/elevation_map_loader/#cluster-extraction-parameters","text":"Cluster extraction is based on pcl algorithms. See https://pointclouds.org/documentation/tutorials/cluster_extraction.html for more details. Name Type Description Default value pcl_grid_map_extraction/cluster_extraction/cluster_tolerance float Distance between points below which they will still be considered part of one cluster. 0.2 pcl_grid_map_extraction/cluster_extraction/min_num_points int Min number of points that a cluster needs to have (otherwise it will be discarded). 3 pcl_grid_map_extraction/cluster_extraction/max_num_points int Max number of points that a cluster can have (otherwise it will be discarded). 1000000","title":"Cluster extraction parameters"},{"location":"perception/elevation_map_loader/#outlier-removal-parameters","text":"See https://pointclouds.org/documentation/tutorials/statistical_outlier.html for more explanation on outlier removal. Name Type Description Default value pcl_grid_map_extraction/outlier_removal/is_remove_outliers float Whether to perform statistical outlier removal. false pcl_grid_map_extraction/outlier_removal/mean_K float Number of neighbours to analyze for estimating statistics of a point. 10 pcl_grid_map_extraction/outlier_removal/stddev_threshold float Number of standard deviations under which points are considered to be inliers. 1.0","title":"Outlier removal parameters"},{"location":"perception/elevation_map_loader/#subsampling-parameters","text":"See https://pointclouds.org/documentation/tutorials/voxel_grid.html for more explanation on point cloud downsampling. Name Type Description Default value pcl_grid_map_extraction/downsampling/is_downsample_cloud bool Whether to perform downsampling or not. false pcl_grid_map_extraction/downsampling/voxel_size float Voxel sizes (xyz) in meters. 0.02","title":"Subsampling parameters"},{"location":"perception/euclidean_cluster/","text":"euclidean_cluster # Purpose # euclidean_cluster is a package for clustering points into smaller parts to classify objects. This package has two clustering methods: euclidean_cluster and voxel_grid_based_euclidean_cluster . Inner-workings / Algorithms # euclidean_cluster # pcl::EuclideanClusterExtraction is applied to points. See official document for details. voxel_grid_based_euclidean_cluster # A centroid in each voxel is calculated by pcl::VoxelGrid . The centroids are clustered by pcl::EuclideanClusterExtraction . The input points are clustered based on the clustered centroids. Inputs / Outputs # Input # Name Type Description input sensor_msgs::msg::PointCloud2 input pointcloud Output # Name Type Description output tier4_perception_msgs::msg::DetectedObjectsWithFeature cluster pointcloud debug/clusters sensor_msgs::msg::PointCloud2 colored cluster pointcloud for visualization Parameters # Core Parameters # euclidean_cluster # Name Type Description use_height bool use point.z for clustering min_cluster_size int the minimum number of points that a cluster needs to contain in order to be considered valid max_cluster_size int the maximum number of points that a cluster needs to contain in order to be considered valid tolerance float the spatial cluster tolerance as a measure in the L2 Euclidean space voxel_grid_based_euclidean_cluster # Name Type Description use_height bool use point.z for clustering min_cluster_size int the minimum number of points that a cluster needs to contain in order to be considered valid max_cluster_size int the maximum number of points that a cluster needs to contain in order to be considered valid tolerance float the spatial cluster tolerance as a measure in the L2 Euclidean space voxel_leaf_size float the voxel leaf size of x and y min_points_number_per_voxel int the minimum number of points for a voxel Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts # The use_height option of voxel_grid_based_euclidean_cluster isn't implemented yet.","title":"euclidean_cluster"},{"location":"perception/euclidean_cluster/#euclidean_cluster","text":"","title":"euclidean_cluster"},{"location":"perception/euclidean_cluster/#purpose","text":"euclidean_cluster is a package for clustering points into smaller parts to classify objects. This package has two clustering methods: euclidean_cluster and voxel_grid_based_euclidean_cluster .","title":"Purpose"},{"location":"perception/euclidean_cluster/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"perception/euclidean_cluster/#euclidean_cluster_1","text":"pcl::EuclideanClusterExtraction is applied to points. See official document for details.","title":"euclidean_cluster"},{"location":"perception/euclidean_cluster/#voxel_grid_based_euclidean_cluster","text":"A centroid in each voxel is calculated by pcl::VoxelGrid . The centroids are clustered by pcl::EuclideanClusterExtraction . The input points are clustered based on the clustered centroids.","title":"voxel_grid_based_euclidean_cluster"},{"location":"perception/euclidean_cluster/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/euclidean_cluster/#input","text":"Name Type Description input sensor_msgs::msg::PointCloud2 input pointcloud","title":"Input"},{"location":"perception/euclidean_cluster/#output","text":"Name Type Description output tier4_perception_msgs::msg::DetectedObjectsWithFeature cluster pointcloud debug/clusters sensor_msgs::msg::PointCloud2 colored cluster pointcloud for visualization","title":"Output"},{"location":"perception/euclidean_cluster/#parameters","text":"","title":"Parameters"},{"location":"perception/euclidean_cluster/#core-parameters","text":"","title":"Core Parameters"},{"location":"perception/euclidean_cluster/#euclidean_cluster_2","text":"Name Type Description use_height bool use point.z for clustering min_cluster_size int the minimum number of points that a cluster needs to contain in order to be considered valid max_cluster_size int the maximum number of points that a cluster needs to contain in order to be considered valid tolerance float the spatial cluster tolerance as a measure in the L2 Euclidean space","title":"euclidean_cluster"},{"location":"perception/euclidean_cluster/#voxel_grid_based_euclidean_cluster_1","text":"Name Type Description use_height bool use point.z for clustering min_cluster_size int the minimum number of points that a cluster needs to contain in order to be considered valid max_cluster_size int the maximum number of points that a cluster needs to contain in order to be considered valid tolerance float the spatial cluster tolerance as a measure in the L2 Euclidean space voxel_leaf_size float the voxel leaf size of x and y min_points_number_per_voxel int the minimum number of points for a voxel","title":"voxel_grid_based_euclidean_cluster"},{"location":"perception/euclidean_cluster/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/euclidean_cluster/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/euclidean_cluster/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/euclidean_cluster/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"perception/euclidean_cluster/#optional-future-extensions-unimplemented-parts","text":"The use_height option of voxel_grid_based_euclidean_cluster isn't implemented yet.","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/ground_segmentation/","text":"ground_segmentation # Purpose # The ground_segmentation is a node that remove the ground points from the input pointcloud. Inner-workings / Algorithms # Detail description of each ground segmentation algorithm is in the following links. Filter Name Description Detail ray_ground_filter A method of removing the ground based on the geometrical relationship between points lined up on radiation link scan_ground_filter Almost the same method as ray_ground_filter , but with slightly improved performance link ransac_ground_filter A method of removing the ground by approximating the ground to a plane link Inputs / Outputs # Input # Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/indices pcl_msgs::msg::Indices reference indices Output # Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points Parameters # Node Parameters # Name Type Default Value Description input_frame string \" \" input frame id output_frame string \" \" output frame id max_queue_size int 5 max queue size of input/output topics use_indices bool false flag to use pointcloud indices latched_indices bool false flag to latch pointcloud indices approximate_sync bool false flag to use approximate sync option Assumptions / Known limits # pointcloud_preprocessor::Filter is implemented based on pcl_perception [1] because of this issue . References/External links # [1] https://github.com/ros-perception/perception_pcl/blob/ros2/pcl_ros/src/pcl_ros/filters/filter.cpp","title":"ground_segmentation"},{"location":"perception/ground_segmentation/#ground_segmentation","text":"","title":"ground_segmentation"},{"location":"perception/ground_segmentation/#purpose","text":"The ground_segmentation is a node that remove the ground points from the input pointcloud.","title":"Purpose"},{"location":"perception/ground_segmentation/#inner-workings-algorithms","text":"Detail description of each ground segmentation algorithm is in the following links. Filter Name Description Detail ray_ground_filter A method of removing the ground based on the geometrical relationship between points lined up on radiation link scan_ground_filter Almost the same method as ray_ground_filter , but with slightly improved performance link ransac_ground_filter A method of removing the ground by approximating the ground to a plane link","title":"Inner-workings / Algorithms"},{"location":"perception/ground_segmentation/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/ground_segmentation/#input","text":"Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/indices pcl_msgs::msg::Indices reference indices","title":"Input"},{"location":"perception/ground_segmentation/#output","text":"Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"perception/ground_segmentation/#parameters","text":"","title":"Parameters"},{"location":"perception/ground_segmentation/#node-parameters","text":"Name Type Default Value Description input_frame string \" \" input frame id output_frame string \" \" output frame id max_queue_size int 5 max queue size of input/output topics use_indices bool false flag to use pointcloud indices latched_indices bool false flag to latch pointcloud indices approximate_sync bool false flag to use approximate sync option","title":"Node Parameters"},{"location":"perception/ground_segmentation/#assumptions-known-limits","text":"pointcloud_preprocessor::Filter is implemented based on pcl_perception [1] because of this issue .","title":"Assumptions / Known limits"},{"location":"perception/ground_segmentation/#referencesexternal-links","text":"[1] https://github.com/ros-perception/perception_pcl/blob/ros2/pcl_ros/src/pcl_ros/filters/filter.cpp","title":"References/External links"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/","text":"RANSAC Ground Filter # Purpose # The purpose of this node is that remove the ground points from the input pointcloud. Inner-workings / Algorithms # Apply the input points to the plane, and set the points at a certain distance from the plane as points other than the ground. Normally, whn using this method, the input points is filtered so that it is almost flat before use. Since the drivable area is often flat, there are methods such as filtering by lane. Inputs / Outputs # This implementation inherits pointcloud_preprocessor::Filter class, please refer README . Parameters # Node Parameters # This implementation inherits pointcloud_preprocessor::Filter class, please refer README . Core Parameters # Name Type Description base_frame string base_link frame unit_axis string The axis which we need to search ground plane max_iterations int The maximum number of iterations outlier_threshold double The distance threshold to the model [m] plane_slope_threshold double The slope threshold to prevent mis-fitting [deg] voxel_size_x double voxel size x [m] voxel_size_y double voxel size y [m] voxel_size_z double voxel size z [m] height_threshold double The height threshold from ground plane for no ground points [m] debug bool whether to output debug information Assumptions / Known limits # This method can't handle slopes. The input points is filtered so that it is almost flat. (Optional) Error detection and handling # (Optional) Performance characterization # References/External links # https://pcl.readthedocs.io/projects/tutorials/en/latest/planar_segmentation.html (Optional) Future extensions / Unimplemented parts #","title":"RANSAC Ground Filter"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#ransac-ground-filter","text":"","title":"RANSAC Ground Filter"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#purpose","text":"The purpose of this node is that remove the ground points from the input pointcloud.","title":"Purpose"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#inner-workings-algorithms","text":"Apply the input points to the plane, and set the points at a certain distance from the plane as points other than the ground. Normally, whn using this method, the input points is filtered so that it is almost flat before use. Since the drivable area is often flat, there are methods such as filtering by lane.","title":"Inner-workings / Algorithms"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#inputs-outputs","text":"This implementation inherits pointcloud_preprocessor::Filter class, please refer README .","title":"Inputs / Outputs"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#parameters","text":"","title":"Parameters"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#node-parameters","text":"This implementation inherits pointcloud_preprocessor::Filter class, please refer README .","title":"Node Parameters"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#core-parameters","text":"Name Type Description base_frame string base_link frame unit_axis string The axis which we need to search ground plane max_iterations int The maximum number of iterations outlier_threshold double The distance threshold to the model [m] plane_slope_threshold double The slope threshold to prevent mis-fitting [deg] voxel_size_x double voxel size x [m] voxel_size_y double voxel size y [m] voxel_size_z double voxel size z [m] height_threshold double The height threshold from ground plane for no ground points [m] debug bool whether to output debug information","title":"Core Parameters"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#assumptions-known-limits","text":"This method can't handle slopes. The input points is filtered so that it is almost flat.","title":"Assumptions / Known limits"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#referencesexternal-links","text":"https://pcl.readthedocs.io/projects/tutorials/en/latest/planar_segmentation.html","title":"References/External links"},{"location":"perception/ground_segmentation/docs/ransac-ground-filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/","text":"Ray Ground Filter # Purpose # The purpose of this node is that remove the ground points from the input pointcloud. Inner-workings / Algorithms # The points is separated radially (Ray), and the ground is classified for each Ray sequentially from the point close to ego-vehicle based on the geometric information such as the distance and angle between the points. Inputs / Outputs # This implementation inherits pointcloud_preprocessor::Filter class, please refer README . Parameters # Node Parameters # This implementation inherits pointcloud_preprocessor::Filter class, please refer README . Core Parameters # Name Type Description base_frame string The target frame of input points general_max_slope double The triangle created by general_max_slope is called the global cone. If the point is outside the global cone, it is judged to be a point that is no on the ground initial_max_slope double Generally, the point where the object first hits is far from ego-vehicle because of sensor blind spot, so resolution is different from that point and thereafter, so this parameter exists to set a separate local_max_slope local_max_slope double The triangle created by local_max_slope is called the local cone. This parameter for classification based on the continuity of points min_height_threshold double This parameter is used instead of height_threshold because it's difficult to determine continuity in the local cone when the points are too close to each other. radial_divider_angle double The angle of ray concentric_divider_distance double Only check points which radius is larger than concentric_divider_distance reclass_distance_threshold double To check if point is to far from previous one, if so classify again min_x double The parameter to set vehicle footprint manually max_x double The parameter to set vehicle footprint manually min_y double The parameter to set vehicle footprint manually max_y double The parameter to set vehicle footprint manually Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"Ray Ground Filter"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#ray-ground-filter","text":"","title":"Ray Ground Filter"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#purpose","text":"The purpose of this node is that remove the ground points from the input pointcloud.","title":"Purpose"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#inner-workings-algorithms","text":"The points is separated radially (Ray), and the ground is classified for each Ray sequentially from the point close to ego-vehicle based on the geometric information such as the distance and angle between the points.","title":"Inner-workings / Algorithms"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#inputs-outputs","text":"This implementation inherits pointcloud_preprocessor::Filter class, please refer README .","title":"Inputs / Outputs"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#parameters","text":"","title":"Parameters"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#node-parameters","text":"This implementation inherits pointcloud_preprocessor::Filter class, please refer README .","title":"Node Parameters"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#core-parameters","text":"Name Type Description base_frame string The target frame of input points general_max_slope double The triangle created by general_max_slope is called the global cone. If the point is outside the global cone, it is judged to be a point that is no on the ground initial_max_slope double Generally, the point where the object first hits is far from ego-vehicle because of sensor blind spot, so resolution is different from that point and thereafter, so this parameter exists to set a separate local_max_slope local_max_slope double The triangle created by local_max_slope is called the local cone. This parameter for classification based on the continuity of points min_height_threshold double This parameter is used instead of height_threshold because it's difficult to determine continuity in the local cone when the points are too close to each other. radial_divider_angle double The angle of ray concentric_divider_distance double Only check points which radius is larger than concentric_divider_distance reclass_distance_threshold double To check if point is to far from previous one, if so classify again min_x double The parameter to set vehicle footprint manually max_x double The parameter to set vehicle footprint manually min_y double The parameter to set vehicle footprint manually max_y double The parameter to set vehicle footprint manually","title":"Core Parameters"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"perception/ground_segmentation/docs/ray-ground-filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/","text":"Scan Ground Filter # Purpose # The purpose of this node is that remove the ground points from the input pointcloud. Inner-workings / Algorithms # This algorithm works by following steps, Divide whole pointclouds into groups by horizontal angle and sort by xy-distance. Check the distance and vertical angle of the point one by one. Set a center of the ground contact point of the rear or front wheels as the initial point. Check vertical angle between the points. If the angle from the initial point is larger than \"global_slope_max\", the point is classified as \"no ground\". If the angle from the previous point is larger than \"local_max_slope\", the point is classified as \"no ground\". Otherwise the point is labeled as \"ground point\". If the distance from the last checked point is close, ignore any vertical angle and set current point attribute to the same as the last point. Inputs / Outputs # This implementation inherits pointcloud_preprocessor::Filter class, please refer README . Parameters # Node Parameters # This implementation inherits pointcloud_preprocessor::Filter class, please refer README . Core Parameters # Name Type Default Value Description base_frame string \"base_link\" base_link frame global_slope_max double 8.0 The global angle to classify as the ground or object [deg] local_max_slope double 6.0 The local angle to classify as the ground or object [deg] radial_divider_angle double 1.0 The angle which divide the whole pointcloud to sliced group [deg] split_points_distance_tolerance double 0.2 The xy-distance threshold to to distinguishing far and near [m] split_height_distance double 0.2 The height threshold to distinguishing far and near [m] use_virtual_ground_point bool true whether to use the ground center of front wheels as the virtual ground point. Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts # Horizontal check for classification is not implemented yet. Output ground visibility for diagnostic is not implemented yet.","title":"Scan Ground Filter"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#scan-ground-filter","text":"","title":"Scan Ground Filter"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#purpose","text":"The purpose of this node is that remove the ground points from the input pointcloud.","title":"Purpose"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#inner-workings-algorithms","text":"This algorithm works by following steps, Divide whole pointclouds into groups by horizontal angle and sort by xy-distance. Check the distance and vertical angle of the point one by one. Set a center of the ground contact point of the rear or front wheels as the initial point. Check vertical angle between the points. If the angle from the initial point is larger than \"global_slope_max\", the point is classified as \"no ground\". If the angle from the previous point is larger than \"local_max_slope\", the point is classified as \"no ground\". Otherwise the point is labeled as \"ground point\". If the distance from the last checked point is close, ignore any vertical angle and set current point attribute to the same as the last point.","title":"Inner-workings / Algorithms"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#inputs-outputs","text":"This implementation inherits pointcloud_preprocessor::Filter class, please refer README .","title":"Inputs / Outputs"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#parameters","text":"","title":"Parameters"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#node-parameters","text":"This implementation inherits pointcloud_preprocessor::Filter class, please refer README .","title":"Node Parameters"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#core-parameters","text":"Name Type Default Value Description base_frame string \"base_link\" base_link frame global_slope_max double 8.0 The global angle to classify as the ground or object [deg] local_max_slope double 6.0 The local angle to classify as the ground or object [deg] radial_divider_angle double 1.0 The angle which divide the whole pointcloud to sliced group [deg] split_points_distance_tolerance double 0.2 The xy-distance threshold to to distinguishing far and near [m] split_height_distance double 0.2 The height threshold to distinguishing far and near [m] use_virtual_ground_point bool true whether to use the ground center of front wheels as the virtual ground point.","title":"Core Parameters"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"perception/ground_segmentation/docs/scan-ground-filter/#optional-future-extensions-unimplemented-parts","text":"Horizontal check for classification is not implemented yet. Output ground visibility for diagnostic is not implemented yet.","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/heatmap_visualizer/","text":"heatmap_visualizer # Purpose # heatmap_visualizer is a package for visualizing heatmap of detected 3D objects' positions on the BEV space. This package is used for qualitative evaluation and trend analysis of the detector, it means, for instance, the heatmap shows \"This detector performs good for near around of our vehicle, but far is bad\". How to run # ros2 launch heatmap_visualizer heatmap_visualizer.launch.xml input/objects: = <DETECTED_OBJECTS_TOPIC> Inner-workings / Algorithms # In this implementation, create heatmap of the center position of detected objects for each classes, for instance, CAR, PEDESTRIAN, etc, and publish them as occupancy grid maps. In the above figure, the pink represents high detection frequency area and blue one is low, or black represents there is no detection. As inner-workings, add center positions of detected objects to index of each corresponding grid map cell in a buffer. The created heatmap will be published by each specific frame, which can be specified with frame_count . Note that the buffer to be add the positions is not reset per publishing. When publishing, firstly these values are normalized to [0, 1] using maximum and minimum values in the buffer. Secondly, they are scaled to integer in [0, 100] because nav_msgs::msg::OccupancyGrid only allow the value in [0, 100]. Inputs / Outputs # Input # Name Type Description ~/input/objects autoware_auto_perception_msgs::msg::DetectedObjects detected objects Output # Name Type Description ~/output/objects/<CLASS_NAME> nav_msgs::msg::OccupancyGrid visualized heatmap Parameters # Core Parameters # Name Type Default Value Description frame_count int 50 The number of frames to publish heatmap map_frame string base_link the frame of heatmap to be respected map_length float 200.0 the length of map in meter map_resolution float 0.8 the resolution of map use_confidence bool false the flag if use confidence score as heatmap value rename_car_to_truck_and_bus bool true the flag if rename car to truck or bus Assumptions / Known limits # The heatmap depends on the data to be used, so if the objects in data are sparse the heatmap will be sparse. (Optional) Error detection and handling # (Optional) Performance characterization # References/External links # (Optional) Future extensions / Unimplemented parts #","title":"heatmap_visualizer"},{"location":"perception/heatmap_visualizer/#heatmap_visualizer","text":"","title":"heatmap_visualizer"},{"location":"perception/heatmap_visualizer/#purpose","text":"heatmap_visualizer is a package for visualizing heatmap of detected 3D objects' positions on the BEV space. This package is used for qualitative evaluation and trend analysis of the detector, it means, for instance, the heatmap shows \"This detector performs good for near around of our vehicle, but far is bad\".","title":"Purpose"},{"location":"perception/heatmap_visualizer/#how-to-run","text":"ros2 launch heatmap_visualizer heatmap_visualizer.launch.xml input/objects: = <DETECTED_OBJECTS_TOPIC>","title":"How to run"},{"location":"perception/heatmap_visualizer/#inner-workings-algorithms","text":"In this implementation, create heatmap of the center position of detected objects for each classes, for instance, CAR, PEDESTRIAN, etc, and publish them as occupancy grid maps. In the above figure, the pink represents high detection frequency area and blue one is low, or black represents there is no detection. As inner-workings, add center positions of detected objects to index of each corresponding grid map cell in a buffer. The created heatmap will be published by each specific frame, which can be specified with frame_count . Note that the buffer to be add the positions is not reset per publishing. When publishing, firstly these values are normalized to [0, 1] using maximum and minimum values in the buffer. Secondly, they are scaled to integer in [0, 100] because nav_msgs::msg::OccupancyGrid only allow the value in [0, 100].","title":"Inner-workings / Algorithms"},{"location":"perception/heatmap_visualizer/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/heatmap_visualizer/#input","text":"Name Type Description ~/input/objects autoware_auto_perception_msgs::msg::DetectedObjects detected objects","title":"Input"},{"location":"perception/heatmap_visualizer/#output","text":"Name Type Description ~/output/objects/<CLASS_NAME> nav_msgs::msg::OccupancyGrid visualized heatmap","title":"Output"},{"location":"perception/heatmap_visualizer/#parameters","text":"","title":"Parameters"},{"location":"perception/heatmap_visualizer/#core-parameters","text":"Name Type Default Value Description frame_count int 50 The number of frames to publish heatmap map_frame string base_link the frame of heatmap to be respected map_length float 200.0 the length of map in meter map_resolution float 0.8 the resolution of map use_confidence bool false the flag if use confidence score as heatmap value rename_car_to_truck_and_bus bool true the flag if rename car to truck or bus","title":"Core Parameters"},{"location":"perception/heatmap_visualizer/#assumptions-known-limits","text":"The heatmap depends on the data to be used, so if the objects in data are sparse the heatmap will be sparse.","title":"Assumptions / Known limits"},{"location":"perception/heatmap_visualizer/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/heatmap_visualizer/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/heatmap_visualizer/#referencesexternal-links","text":"","title":"References/External links"},{"location":"perception/heatmap_visualizer/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/image_projection_based_fusion/","text":"image_projection_based_fusion # Purpose # The image_projection_based_fusion is a package to fuse detected obstacles (bounding box or segmentation) from image and 3d pointcloud or obstacles (bounding box, cluster or segmentation). Inner-workings / Algorithms # Detail description of each fusion's algorithm is in the following links. Fusion Name Description Detail roi_cluster_fusion Overwrite a classification label of clusters by that of ROIs from a 2D object detector. link roi_detected_object_fusion Overwrite a classification label of detected objects by that of ROIs from a 2D object detector. link","title":"image_projection_based_fusion"},{"location":"perception/image_projection_based_fusion/#image_projection_based_fusion","text":"","title":"image_projection_based_fusion"},{"location":"perception/image_projection_based_fusion/#purpose","text":"The image_projection_based_fusion is a package to fuse detected obstacles (bounding box or segmentation) from image and 3d pointcloud or obstacles (bounding box, cluster or segmentation).","title":"Purpose"},{"location":"perception/image_projection_based_fusion/#inner-workings-algorithms","text":"Detail description of each fusion's algorithm is in the following links. Fusion Name Description Detail roi_cluster_fusion Overwrite a classification label of clusters by that of ROIs from a 2D object detector. link roi_detected_object_fusion Overwrite a classification label of detected objects by that of ROIs from a 2D object detector. link","title":"Inner-workings / Algorithms"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/","text":"roi_cluster_fusion # Purpose # The roi_cluster_fusion is a package for filtering clusters that are less likely to be objects and overwriting labels of clusters with that of Region Of Interests (ROIs) by a 2D object detector. Inner-workings / Algorithms # The clusters are projected onto image planes, and then if the ROIs of clusters and ROIs by a detector are overlapped, the labels of clusters are overwritten with that of ROIs by detector. Intersection over Union (IoU) is used to determine if there are overlaps between them. Inputs / Outputs # Input # Name Type Description input tier4_perception_msgs::msg::DetectedObjectsWithFeature clustered pointcloud input/camera_infoID sensor_msgs::msg::CameraInfo camera information to project 3d points onto image planes, ID is between 0 and 7 input/roisID tier4_perception_msgs::msg::DetectedObjectsWithFeature ROIs from each image, ID is between 0 and 7 input/image_rawID sensor_msgs::msg::Image images for visualization, ID is between 0 and 7 Output # Name Type Description output tier4_perception_msgs::msg::DetectedObjectsWithFeature labeled cluster pointcloud output/image_rawID sensor_msgs::msg::Image images for visualization, ID is between 0 and 7 Parameters # Core Parameters # Name Type Description use_iou_x bool calculate IoU only along x-axis use_iou_y bool calculate IoU only along y-axis use_iou bool calculate IoU both along x-axis and y-axis use_cluster_semantic_type bool if false , the labels of clusters are overwritten by UNKNOWN before fusion iou_threshold float the IoU threshold to overwrite a label of clusters with a label of roi rois_number int the number of input rois debug_mode bool If true , subscribe and publish images for visualization. Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"roi_cluster_fusion"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#roi_cluster_fusion","text":"","title":"roi_cluster_fusion"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#purpose","text":"The roi_cluster_fusion is a package for filtering clusters that are less likely to be objects and overwriting labels of clusters with that of Region Of Interests (ROIs) by a 2D object detector.","title":"Purpose"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#inner-workings-algorithms","text":"The clusters are projected onto image planes, and then if the ROIs of clusters and ROIs by a detector are overlapped, the labels of clusters are overwritten with that of ROIs by detector. Intersection over Union (IoU) is used to determine if there are overlaps between them.","title":"Inner-workings / Algorithms"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#input","text":"Name Type Description input tier4_perception_msgs::msg::DetectedObjectsWithFeature clustered pointcloud input/camera_infoID sensor_msgs::msg::CameraInfo camera information to project 3d points onto image planes, ID is between 0 and 7 input/roisID tier4_perception_msgs::msg::DetectedObjectsWithFeature ROIs from each image, ID is between 0 and 7 input/image_rawID sensor_msgs::msg::Image images for visualization, ID is between 0 and 7","title":"Input"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#output","text":"Name Type Description output tier4_perception_msgs::msg::DetectedObjectsWithFeature labeled cluster pointcloud output/image_rawID sensor_msgs::msg::Image images for visualization, ID is between 0 and 7","title":"Output"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#parameters","text":"","title":"Parameters"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#core-parameters","text":"Name Type Description use_iou_x bool calculate IoU only along x-axis use_iou_y bool calculate IoU only along y-axis use_iou bool calculate IoU both along x-axis and y-axis use_cluster_semantic_type bool if false , the labels of clusters are overwritten by UNKNOWN before fusion iou_threshold float the IoU threshold to overwrite a label of clusters with a label of roi rois_number int the number of input rois debug_mode bool If true , subscribe and publish images for visualization.","title":"Core Parameters"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"perception/image_projection_based_fusion/docs/roi-cluster-fusion/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/","text":"roi_detected_object_fusion # Purpose # The roi_detected_object_fusion is a package to overwrite labels of detected objects with that of Region Of Interests (ROIs) by a 2D object detector. Inner-workings / Algorithms # The detected objects are projected onto image planes, and then if the ROIs of detected objects (3D ROIs) and from a 2D detector (2D ROIs) are overlapped, the labels of detected objects are overwritten with that of 2D ROIs. Intersection over Union (IoU) is used to determine if there are overlaps between them. The DetectedObject has three shape and the polygon vertices of a object are as below: BOUNDING_BOX : The 8 corners of a bounding box. CYLINDER : The circle is approximated by a hexagon. POLYGON : Not implemented yet. Inputs / Outputs # Input # Name Type Description input autoware_auto_perception_msgs::msg::DetectedObjects detected objects input/camera_infoID sensor_msgs::msg::CameraInfo camera information to project 3d points onto image planes, ID is between 0 and 7 input/roisID tier4_perception_msgs::msg::DetectedObjectsWithFeature ROIs from each image, ID is between 0 and 7 input/image_rawID sensor_msgs::msg::Image images for visualization, ID is between 0 and 7 Output # Name Type Description output autoware_auto_perception_msgs::msg::DetectedObjects detected objects output/image_rawID sensor_msgs::msg::Image images for visualization, ID is between 0 and 7 Parameters # Core Parameters # Name Type Description use_iou_x bool calculate IoU only along x-axis use_iou_y bool calculate IoU only along y-axis use_iou bool calculate IoU both along x-axis and y-axis iou_threshold float the IoU threshold to overwrite a label of a detected object with that of a roi rois_number int the number of input rois debug_mode bool If true , subscribe and publish images for visualization. Assumptions / Known limits # POLYGON , which is a shape of a detected object, isn't supported yet.","title":"roi_detected_object_fusion"},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/#roi_detected_object_fusion","text":"","title":"roi_detected_object_fusion"},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/#purpose","text":"The roi_detected_object_fusion is a package to overwrite labels of detected objects with that of Region Of Interests (ROIs) by a 2D object detector.","title":"Purpose"},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/#inner-workings-algorithms","text":"The detected objects are projected onto image planes, and then if the ROIs of detected objects (3D ROIs) and from a 2D detector (2D ROIs) are overlapped, the labels of detected objects are overwritten with that of 2D ROIs. Intersection over Union (IoU) is used to determine if there are overlaps between them. The DetectedObject has three shape and the polygon vertices of a object are as below: BOUNDING_BOX : The 8 corners of a bounding box. CYLINDER : The circle is approximated by a hexagon. POLYGON : Not implemented yet.","title":"Inner-workings / Algorithms"},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/#input","text":"Name Type Description input autoware_auto_perception_msgs::msg::DetectedObjects detected objects input/camera_infoID sensor_msgs::msg::CameraInfo camera information to project 3d points onto image planes, ID is between 0 and 7 input/roisID tier4_perception_msgs::msg::DetectedObjectsWithFeature ROIs from each image, ID is between 0 and 7 input/image_rawID sensor_msgs::msg::Image images for visualization, ID is between 0 and 7","title":"Input"},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/#output","text":"Name Type Description output autoware_auto_perception_msgs::msg::DetectedObjects detected objects output/image_rawID sensor_msgs::msg::Image images for visualization, ID is between 0 and 7","title":"Output"},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/#parameters","text":"","title":"Parameters"},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/#core-parameters","text":"Name Type Description use_iou_x bool calculate IoU only along x-axis use_iou_y bool calculate IoU only along y-axis use_iou bool calculate IoU both along x-axis and y-axis iou_threshold float the IoU threshold to overwrite a label of a detected object with that of a roi rois_number int the number of input rois debug_mode bool If true , subscribe and publish images for visualization.","title":"Core Parameters"},{"location":"perception/image_projection_based_fusion/docs/roi-detected-object-fusion/#assumptions-known-limits","text":"POLYGON , which is a shape of a detected object, isn't supported yet.","title":"Assumptions / Known limits"},{"location":"perception/lidar_apollo_instance_segmentation/","text":"lidar_apollo_instance_segmentation # Purpose # This node segments 3D pointcloud data from lidar sensors into obstacles, e.g., cars, trucks, bicycles, and pedestrians based on CNN based model and obstacle clustering method. Inner-workings / Algorithms # See the original design by Apollo. Inputs / Outputs # Input # Name Type Description input/pointcloud sensor_msgs/PointCloud2 Pointcloud data from lidar sensors Output # Name Type Description output/labeled_clusters tier4_perception_msgs/DetectedObjectsWithFeature Detected objects with labeled pointcloud cluster. debug/instance_pointcloud sensor_msgs/PointCloud2 Segmented pointcloud for visualization. Parameters # Node Parameters # None Core Parameters # Name Type Default Value Description score_threshold double 0.8 If the score of a detected object is lower than this value, the object is ignored. range int 60 Half of the length of feature map sides. [m] width int 640 The grid width of feature map. height int 640 The grid height of feature map. engine_file string \"vls-128.engine\" The name of TensorRT engine file for CNN model. prototxt_file string \"vls-128.prototxt\" The name of prototxt file for CNN model. caffemodel_file string \"vls-128.caffemodel\" The name of caffemodel file for CNN model. use_intensity_feature bool true The flag to use intensity feature of pointcloud. use_constant_feature bool false The flag to use direction and distance feature of pointcloud. target_frame string \"base_link\" Pointcloud data is transformed into this frame. z_offset int 2 z offset from target frame. [m] Assumptions / Known limits # There is no training code for CNN model. Note # This package makes use of three external codes. The trained files are provided by apollo. The trained files are automatically downloaded when you build. Original URL VLP-16 : https://github.com/ApolloAuto/apollo/raw/88bfa5a1acbd20092963d6057f3a922f3939a183/modules/perception/production/data/perception/lidar/models/cnnseg/velodyne16/deploy.caffemodel HDL-64 : https://github.com/ApolloAuto/apollo/raw/88bfa5a1acbd20092963d6057f3a922f3939a183/modules/perception/production/data/perception/lidar/models/cnnseg/velodyne64/deploy.caffemodel VLS-128 : https://github.com/ApolloAuto/apollo/raw/91844c80ee4bd0cc838b4de4c625852363c258b5/modules/perception/production/data/perception/lidar/models/cnnseg/velodyne128/deploy.caffemodel Supported lidars are velodyne 16, 64 and 128, but you can also use velodyne 32 and other lidars with good accuracy. apollo 3D Obstacle Perception description /****************************************************************************** * Copyright 2017 The Apollo Authors. All Rights Reserved. * * Licensed under the Apache License, Version 2.0 (the \"License\"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. *****************************************************************************/ tensorRTWrapper : It is used under the lib directory. MIT License Copyright (c) 2018 lewes6369 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. autoware_perception description /* * Copyright 2018-2019 Autoware Foundation. All rights reserved. * * Licensed under the Apache License, Version 2.0 (the \"License\"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ Special thanks # Apollo Project lewes6369 Autoware Foundation Kosuke Takeuchi (TierIV Part timer)","title":"lidar_apollo_instance_segmentation"},{"location":"perception/lidar_apollo_instance_segmentation/#lidar_apollo_instance_segmentation","text":"","title":"lidar_apollo_instance_segmentation"},{"location":"perception/lidar_apollo_instance_segmentation/#purpose","text":"This node segments 3D pointcloud data from lidar sensors into obstacles, e.g., cars, trucks, bicycles, and pedestrians based on CNN based model and obstacle clustering method.","title":"Purpose"},{"location":"perception/lidar_apollo_instance_segmentation/#inner-workings-algorithms","text":"See the original design by Apollo.","title":"Inner-workings / Algorithms"},{"location":"perception/lidar_apollo_instance_segmentation/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/lidar_apollo_instance_segmentation/#input","text":"Name Type Description input/pointcloud sensor_msgs/PointCloud2 Pointcloud data from lidar sensors","title":"Input"},{"location":"perception/lidar_apollo_instance_segmentation/#output","text":"Name Type Description output/labeled_clusters tier4_perception_msgs/DetectedObjectsWithFeature Detected objects with labeled pointcloud cluster. debug/instance_pointcloud sensor_msgs/PointCloud2 Segmented pointcloud for visualization.","title":"Output"},{"location":"perception/lidar_apollo_instance_segmentation/#parameters","text":"","title":"Parameters"},{"location":"perception/lidar_apollo_instance_segmentation/#node-parameters","text":"None","title":"Node Parameters"},{"location":"perception/lidar_apollo_instance_segmentation/#core-parameters","text":"Name Type Default Value Description score_threshold double 0.8 If the score of a detected object is lower than this value, the object is ignored. range int 60 Half of the length of feature map sides. [m] width int 640 The grid width of feature map. height int 640 The grid height of feature map. engine_file string \"vls-128.engine\" The name of TensorRT engine file for CNN model. prototxt_file string \"vls-128.prototxt\" The name of prototxt file for CNN model. caffemodel_file string \"vls-128.caffemodel\" The name of caffemodel file for CNN model. use_intensity_feature bool true The flag to use intensity feature of pointcloud. use_constant_feature bool false The flag to use direction and distance feature of pointcloud. target_frame string \"base_link\" Pointcloud data is transformed into this frame. z_offset int 2 z offset from target frame. [m]","title":"Core Parameters"},{"location":"perception/lidar_apollo_instance_segmentation/#assumptions-known-limits","text":"There is no training code for CNN model.","title":"Assumptions / Known limits"},{"location":"perception/lidar_apollo_instance_segmentation/#note","text":"This package makes use of three external codes. The trained files are provided by apollo. The trained files are automatically downloaded when you build. Original URL VLP-16 : https://github.com/ApolloAuto/apollo/raw/88bfa5a1acbd20092963d6057f3a922f3939a183/modules/perception/production/data/perception/lidar/models/cnnseg/velodyne16/deploy.caffemodel HDL-64 : https://github.com/ApolloAuto/apollo/raw/88bfa5a1acbd20092963d6057f3a922f3939a183/modules/perception/production/data/perception/lidar/models/cnnseg/velodyne64/deploy.caffemodel VLS-128 : https://github.com/ApolloAuto/apollo/raw/91844c80ee4bd0cc838b4de4c625852363c258b5/modules/perception/production/data/perception/lidar/models/cnnseg/velodyne128/deploy.caffemodel Supported lidars are velodyne 16, 64 and 128, but you can also use velodyne 32 and other lidars with good accuracy. apollo 3D Obstacle Perception description /****************************************************************************** * Copyright 2017 The Apollo Authors. All Rights Reserved. * * Licensed under the Apache License, Version 2.0 (the \"License\"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. *****************************************************************************/ tensorRTWrapper : It is used under the lib directory. MIT License Copyright (c) 2018 lewes6369 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. autoware_perception description /* * Copyright 2018-2019 Autoware Foundation. All rights reserved. * * Licensed under the Apache License, Version 2.0 (the \"License\"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */","title":"Note"},{"location":"perception/lidar_apollo_instance_segmentation/#special-thanks","text":"Apollo Project lewes6369 Autoware Foundation Kosuke Takeuchi (TierIV Part timer)","title":"Special thanks"},{"location":"perception/lidar_apollo_instance_segmentation/lib/","text":"Note # This library is customized. Original repository : https://github.com/lewes6369/tensorRTWrapper MIT License Copyright (c) 2018 lewes6369 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Index"},{"location":"perception/lidar_apollo_instance_segmentation/lib/#note","text":"This library is customized. Original repository : https://github.com/lewes6369/tensorRTWrapper MIT License Copyright (c) 2018 lewes6369 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Note"},{"location":"perception/lidar_centerpoint/","text":"lidar_centerpoint # Purpose # lidar_centerpoint is a package for detecting dynamic 3D objects. Inner-workings / Algorithms # In this implementation, CenterPoint [1] uses a PointPillars-based [2] network to inference with TensorRT. We trained the models using https://github.com/open-mmlab/mmdetection3d . Inputs / Outputs # Input # Name Type Description ~/input/pointcloud sensor_msgs::msg::PointCloud2 input pointcloud Output # Name Type Description ~/output/objects autoware_auto_perception_msgs::msg::DetectedObjects detected objects Parameters # Core Parameters # Name Type Default Value Description score_threshold float 0.4 detected objects with score less than threshold are ignored densification_world_frame_id string map the world frame id to fuse multi-frame pointcloud densification_num_past_frames int 1 the number of past frames to fuse with the current frame trt_precision string fp16 TensorRT inference precision: fp32 or fp16 encoder_onnx_path string \"\" path to VoxelFeatureEncoder ONNX file encoder_engine_path string \"\" path to VoxelFeatureEncoder TensorRT Engine file head_onnx_path string \"\" path to DetectionHead ONNX file head_engine_path string \"\" path to DetectionHead TensorRT Engine file Assumptions / Known limits # The object.existence_probability is stored the value of classification confidence of a DNN, not probability. (Optional) Error detection and handling # (Optional) Performance characterization # References/External links # [1] Yin, Tianwei, Xingyi Zhou, and Philipp Kr\u00e4henb\u00fchl. \"Center-based 3d object detection and tracking.\" arXiv preprint arXiv:2006.11275 (2020). [2] Lang, Alex H., et al. \"Pointpillars: Fast encoders for object detection from point clouds.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019. [3] https://github.com/tianweiy/CenterPoint [4] https://github.com/open-mmlab/mmdetection3d [5] https://github.com/open-mmlab/OpenPCDet [6] https://github.com/yukkysaito/autoware_perception [7] https://github.com/NVIDIA-AI-IOT/CUDA-PointPillars (Optional) Future extensions / Unimplemented parts #","title":"lidar_centerpoint"},{"location":"perception/lidar_centerpoint/#lidar_centerpoint","text":"","title":"lidar_centerpoint"},{"location":"perception/lidar_centerpoint/#purpose","text":"lidar_centerpoint is a package for detecting dynamic 3D objects.","title":"Purpose"},{"location":"perception/lidar_centerpoint/#inner-workings-algorithms","text":"In this implementation, CenterPoint [1] uses a PointPillars-based [2] network to inference with TensorRT. We trained the models using https://github.com/open-mmlab/mmdetection3d .","title":"Inner-workings / Algorithms"},{"location":"perception/lidar_centerpoint/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/lidar_centerpoint/#input","text":"Name Type Description ~/input/pointcloud sensor_msgs::msg::PointCloud2 input pointcloud","title":"Input"},{"location":"perception/lidar_centerpoint/#output","text":"Name Type Description ~/output/objects autoware_auto_perception_msgs::msg::DetectedObjects detected objects","title":"Output"},{"location":"perception/lidar_centerpoint/#parameters","text":"","title":"Parameters"},{"location":"perception/lidar_centerpoint/#core-parameters","text":"Name Type Default Value Description score_threshold float 0.4 detected objects with score less than threshold are ignored densification_world_frame_id string map the world frame id to fuse multi-frame pointcloud densification_num_past_frames int 1 the number of past frames to fuse with the current frame trt_precision string fp16 TensorRT inference precision: fp32 or fp16 encoder_onnx_path string \"\" path to VoxelFeatureEncoder ONNX file encoder_engine_path string \"\" path to VoxelFeatureEncoder TensorRT Engine file head_onnx_path string \"\" path to DetectionHead ONNX file head_engine_path string \"\" path to DetectionHead TensorRT Engine file","title":"Core Parameters"},{"location":"perception/lidar_centerpoint/#assumptions-known-limits","text":"The object.existence_probability is stored the value of classification confidence of a DNN, not probability.","title":"Assumptions / Known limits"},{"location":"perception/lidar_centerpoint/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/lidar_centerpoint/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/lidar_centerpoint/#referencesexternal-links","text":"[1] Yin, Tianwei, Xingyi Zhou, and Philipp Kr\u00e4henb\u00fchl. \"Center-based 3d object detection and tracking.\" arXiv preprint arXiv:2006.11275 (2020). [2] Lang, Alex H., et al. \"Pointpillars: Fast encoders for object detection from point clouds.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019. [3] https://github.com/tianweiy/CenterPoint [4] https://github.com/open-mmlab/mmdetection3d [5] https://github.com/open-mmlab/OpenPCDet [6] https://github.com/yukkysaito/autoware_perception [7] https://github.com/NVIDIA-AI-IOT/CUDA-PointPillars","title":"References/External links"},{"location":"perception/lidar_centerpoint/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/map_based_prediction/Readme/","text":"map_based_prediction # Role # map_based_prediction is a module to predict the future paths of other vehicles and pedestrians according to the shape of the map and the surrounding environment. Inner-workings / Algorithms # Get lanelet path The first step is to get the lanelet of the current position of the car. After that, we obtain several trajectories based on the map. Lane Change Detection After finding the current lanelet from the current position of the obstacle, our algorithm try to detect the lane change maneuver from the past positions of the obstacle. Our method uses the deviation between the obstacle's current position and its position one second ago and current position to determine if it is about to change lanes. The parameters used for the lane change decision are obtained by analyzing the data obtained from the experiment. We already confirmed that these parameters give the least number of false positives. Confidence calculation We use the following metric to compute the distance to a certain lane. d = x^T P x where x=[lateral_dist, yaw_diff] and P are covariance matrices. Therefore confidence values can be computed as confidence = 1/d Finally, we normalize the confidence value to make it as probability value. Note that the standard deviation of the lateral distance and yaw difference is given by the user. Drawing predicted trajectories From the current position and reference trajectories that we get in the step1, we create predicted trajectories by using Quintic polynomial. Note that, since this algorithm consider lateral and longitudinal motions separately, it sometimes generates dynamically-infeasible trajectories when the vehicle travels at a low speed. To deal with this problem, we only make straight line predictions when the vehicle speed is lower than a certain value (which is given as a parameter). Inputs / Outputs # Input # Name Type Description ~/perception/object_recognition/tracking/objects autoware_auto_perception_msgs::msg::TrackedObjects tracking objects without predicted path. ~/vector_map autoware_auto_mapping_msgs::msg::HADMapBin binary data of Lanelet2 Map. Output # Name Type Description ~/objects autoware_auto_perception_msgs::msg::PredictedObjects tracking objects with predicted path. ~/objects_path_markers visualization_msgs::msg::MarkerArray marker for visualization. Parameters # Parameter Type Description enable_delay_compensation bool flag to enable the time delay compensation for the position of the object prediction_time_horizon double predict time duration for predicted path [s] prediction_sampling_delta_time double sampling time for points in predicted path [s] min_velocity_for_map_based_prediction double apply map-based prediction to the objects with higher velocity than this value dist_threshold_for_searching_lanelet double The threshold of the angle used when searching for the lane to which the object belongs [rad] delta_yaw_threshold_for_searching_lanelet double The threshold of the distance used when searching for the lane to which the object belongs [m] sigma_lateral_offset double Standard deviation for lateral position of objects [m] sigma_yaw_angle double Standard deviation yaw angle of objects [rad] object_buffer_time_length double Time span of object history to store the information [s] history_time_length double Time span of object information used for prediction [s] dist_ratio_threshold_to_left_bound double Conditions for using lane change detection of objects. Distance to the left bound of lanelet. dist_ratio_threshold_to_right_bound double Conditions for using lane change detection of objects. Distance to the right bound of lanelet. diff_dist_threshold_to_left_bound double Conditions for using lane change detection of objects. Differential value of horizontal position of objects. diff_dist_threshold_to_right_bound double Conditions for using lane change detection of objects. Differential value of horizontal position of objects. Assumptions / Known limits # map_based_prediction can only predict future trajectories for cars, tracks and buses. Reference # M. Werling, J. Ziegler, S. Kammel, and S. Thrun, \u201cOptimal trajectory generation for dynamic street scenario in a frenet frame,\u201d IEEE International Conference on Robotics and Automation, Anchorage, Alaska, USA, May 2010. A. Houenou, P. Bonnifait, V. Cherfaoui, and Wen Yao, \u201cVehicle trajectory prediction based on motion model and maneuver recognition,\u201d in 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, nov 2013, pp. 4363-4369.","title":"map_based_prediction"},{"location":"perception/map_based_prediction/Readme/#map_based_prediction","text":"","title":"map_based_prediction"},{"location":"perception/map_based_prediction/Readme/#role","text":"map_based_prediction is a module to predict the future paths of other vehicles and pedestrians according to the shape of the map and the surrounding environment.","title":"Role"},{"location":"perception/map_based_prediction/Readme/#inner-workings-algorithms","text":"Get lanelet path The first step is to get the lanelet of the current position of the car. After that, we obtain several trajectories based on the map. Lane Change Detection After finding the current lanelet from the current position of the obstacle, our algorithm try to detect the lane change maneuver from the past positions of the obstacle. Our method uses the deviation between the obstacle's current position and its position one second ago and current position to determine if it is about to change lanes. The parameters used for the lane change decision are obtained by analyzing the data obtained from the experiment. We already confirmed that these parameters give the least number of false positives. Confidence calculation We use the following metric to compute the distance to a certain lane. d = x^T P x where x=[lateral_dist, yaw_diff] and P are covariance matrices. Therefore confidence values can be computed as confidence = 1/d Finally, we normalize the confidence value to make it as probability value. Note that the standard deviation of the lateral distance and yaw difference is given by the user. Drawing predicted trajectories From the current position and reference trajectories that we get in the step1, we create predicted trajectories by using Quintic polynomial. Note that, since this algorithm consider lateral and longitudinal motions separately, it sometimes generates dynamically-infeasible trajectories when the vehicle travels at a low speed. To deal with this problem, we only make straight line predictions when the vehicle speed is lower than a certain value (which is given as a parameter).","title":"Inner-workings / Algorithms"},{"location":"perception/map_based_prediction/Readme/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/map_based_prediction/Readme/#input","text":"Name Type Description ~/perception/object_recognition/tracking/objects autoware_auto_perception_msgs::msg::TrackedObjects tracking objects without predicted path. ~/vector_map autoware_auto_mapping_msgs::msg::HADMapBin binary data of Lanelet2 Map.","title":"Input"},{"location":"perception/map_based_prediction/Readme/#output","text":"Name Type Description ~/objects autoware_auto_perception_msgs::msg::PredictedObjects tracking objects with predicted path. ~/objects_path_markers visualization_msgs::msg::MarkerArray marker for visualization.","title":"Output"},{"location":"perception/map_based_prediction/Readme/#parameters","text":"Parameter Type Description enable_delay_compensation bool flag to enable the time delay compensation for the position of the object prediction_time_horizon double predict time duration for predicted path [s] prediction_sampling_delta_time double sampling time for points in predicted path [s] min_velocity_for_map_based_prediction double apply map-based prediction to the objects with higher velocity than this value dist_threshold_for_searching_lanelet double The threshold of the angle used when searching for the lane to which the object belongs [rad] delta_yaw_threshold_for_searching_lanelet double The threshold of the distance used when searching for the lane to which the object belongs [m] sigma_lateral_offset double Standard deviation for lateral position of objects [m] sigma_yaw_angle double Standard deviation yaw angle of objects [rad] object_buffer_time_length double Time span of object history to store the information [s] history_time_length double Time span of object information used for prediction [s] dist_ratio_threshold_to_left_bound double Conditions for using lane change detection of objects. Distance to the left bound of lanelet. dist_ratio_threshold_to_right_bound double Conditions for using lane change detection of objects. Distance to the right bound of lanelet. diff_dist_threshold_to_left_bound double Conditions for using lane change detection of objects. Differential value of horizontal position of objects. diff_dist_threshold_to_right_bound double Conditions for using lane change detection of objects. Differential value of horizontal position of objects.","title":"Parameters"},{"location":"perception/map_based_prediction/Readme/#assumptions-known-limits","text":"map_based_prediction can only predict future trajectories for cars, tracks and buses.","title":"Assumptions / Known limits"},{"location":"perception/map_based_prediction/Readme/#reference","text":"M. Werling, J. Ziegler, S. Kammel, and S. Thrun, \u201cOptimal trajectory generation for dynamic street scenario in a frenet frame,\u201d IEEE International Conference on Robotics and Automation, Anchorage, Alaska, USA, May 2010. A. Houenou, P. Bonnifait, V. Cherfaoui, and Wen Yao, \u201cVehicle trajectory prediction based on motion model and maneuver recognition,\u201d in 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, nov 2013, pp. 4363-4369.","title":"Reference"},{"location":"perception/multi_object_tracker/","text":"multi_object_tracker # Purpose # The results of the detection are processed by a time series. The main purpose is to give ID and estimate velocity. Inner-workings / Algorithms # This multi object tracker consists of data association and EKF. Data association # The data association performs maximum score matching, called min cost max flow problem. In this package, mussp[1] is used as solver. In addition, when associating observations to tracers, data association have gates such as the area of the object from the BEV, Mahalanobis distance, and maximum distance, depending on the class label. EKF Tracker # Models for pedestrians, bicycles (motorcycles), cars and unknown are available. The pedestrian or bicycle tracker is running at the same time as the respective EKF model in order to enable the transition between pedestrian and bicycle tracking. For big vehicles such as trucks and buses, we have separate models for passenger cars and large vehicles because they are difficult to distinguish from passenger cars and are not stable. Therefore, separate models are prepared for passenger cars and big vehicles, and these models are run at the same time as the respective EKF models to ensure stability. Inputs / Outputs # Input # Name Type Description ~/input autoware_auto_perception_msgs::msg::DetectedObjects obstacles Output # Name Type Description ~/output autoware_auto_perception_msgs::msg::TrackedObjects modified obstacles Parameters # Core Parameters # Name Type Description can_assign_matrix double Assignment table for data association max_dist_matrix double Maximum distance table for data association max_area_matrix double Maximum area table for data association min_area_matrix double Minimum area table for data association max_rad_matrix double Maximum angle table for data association world_frame_id double tracking frame enable_delay_compensation bool Estimate obstacles at current time considering detection delay publish_rate double if enable_delay_compensation is true, how many hertz to output Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # Evaluation of muSSP # According to our evaluation, muSSP is faster than normal SSP when the matrix size is more than 100. Execution time for varying matrix size at 95% sparsity. In real data, the sparsity was often around 95%. Execution time for varying the sparsity with matrix size 100. (Optional) References/External links # This package makes use of external code. Name License Original Repository muSSP Apache-2.0 https://github.com/yu-lab-vt/muSSP [1] C. Wang, Y. Wang, Y. Wang, C.-t. Wu, and G. Yu, \u201cmuSSP: Efficient Min-cost Flow Algorithm for Multi-object Tracking,\u201d NeurIPS, 2019 (Optional) Future extensions / Unimplemented parts #","title":"multi_object_tracker"},{"location":"perception/multi_object_tracker/#multi_object_tracker","text":"","title":"multi_object_tracker"},{"location":"perception/multi_object_tracker/#purpose","text":"The results of the detection are processed by a time series. The main purpose is to give ID and estimate velocity.","title":"Purpose"},{"location":"perception/multi_object_tracker/#inner-workings-algorithms","text":"This multi object tracker consists of data association and EKF.","title":"Inner-workings / Algorithms"},{"location":"perception/multi_object_tracker/#data-association","text":"The data association performs maximum score matching, called min cost max flow problem. In this package, mussp[1] is used as solver. In addition, when associating observations to tracers, data association have gates such as the area of the object from the BEV, Mahalanobis distance, and maximum distance, depending on the class label.","title":"Data association"},{"location":"perception/multi_object_tracker/#ekf-tracker","text":"Models for pedestrians, bicycles (motorcycles), cars and unknown are available. The pedestrian or bicycle tracker is running at the same time as the respective EKF model in order to enable the transition between pedestrian and bicycle tracking. For big vehicles such as trucks and buses, we have separate models for passenger cars and large vehicles because they are difficult to distinguish from passenger cars and are not stable. Therefore, separate models are prepared for passenger cars and big vehicles, and these models are run at the same time as the respective EKF models to ensure stability.","title":"EKF Tracker"},{"location":"perception/multi_object_tracker/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/multi_object_tracker/#input","text":"Name Type Description ~/input autoware_auto_perception_msgs::msg::DetectedObjects obstacles","title":"Input"},{"location":"perception/multi_object_tracker/#output","text":"Name Type Description ~/output autoware_auto_perception_msgs::msg::TrackedObjects modified obstacles","title":"Output"},{"location":"perception/multi_object_tracker/#parameters","text":"","title":"Parameters"},{"location":"perception/multi_object_tracker/#core-parameters","text":"Name Type Description can_assign_matrix double Assignment table for data association max_dist_matrix double Maximum distance table for data association max_area_matrix double Maximum area table for data association min_area_matrix double Minimum area table for data association max_rad_matrix double Maximum angle table for data association world_frame_id double tracking frame enable_delay_compensation bool Estimate obstacles at current time considering detection delay publish_rate double if enable_delay_compensation is true, how many hertz to output","title":"Core Parameters"},{"location":"perception/multi_object_tracker/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/multi_object_tracker/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/multi_object_tracker/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/multi_object_tracker/#evaluation-of-mussp","text":"According to our evaluation, muSSP is faster than normal SSP when the matrix size is more than 100. Execution time for varying matrix size at 95% sparsity. In real data, the sparsity was often around 95%. Execution time for varying the sparsity with matrix size 100.","title":"Evaluation of muSSP"},{"location":"perception/multi_object_tracker/#optional-referencesexternal-links","text":"This package makes use of external code. Name License Original Repository muSSP Apache-2.0 https://github.com/yu-lab-vt/muSSP [1] C. Wang, Y. Wang, Y. Wang, C.-t. Wu, and G. Yu, \u201cmuSSP: Efficient Min-cost Flow Algorithm for Multi-object Tracking,\u201d NeurIPS, 2019","title":"(Optional) References/External links"},{"location":"perception/multi_object_tracker/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/object_merger/","text":"object_merger # Purpose # object_merger is a package for merging detected objects from two methods by data association. Inner-workings / Algorithms # The successive shortest path algorithm is used to solve the data association problem (the minimum-cost flow problem). The cost is calculated by the distance between two objects and gate functions are applied to reset cost, s.t. the maximum distance, the maximum area and the minimum area. Inputs / Outputs # Input # Name Type Description input/object0 autoware_auto_perception_msgs::msg::DetectedObjects detection objects input/object1 autoware_auto_perception_msgs::msg::DetectedObjects detection objects Output # Name Type Description output/object autoware_auto_perception_msgs::msg::DetectedObjects modified Objects Parameters # No Parameters. Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts # Data association algorithm was the same as that of multi_object_tracker, but the algorithm of multi_object_tracker was already updated.","title":"object_merger"},{"location":"perception/object_merger/#object_merger","text":"","title":"object_merger"},{"location":"perception/object_merger/#purpose","text":"object_merger is a package for merging detected objects from two methods by data association.","title":"Purpose"},{"location":"perception/object_merger/#inner-workings-algorithms","text":"The successive shortest path algorithm is used to solve the data association problem (the minimum-cost flow problem). The cost is calculated by the distance between two objects and gate functions are applied to reset cost, s.t. the maximum distance, the maximum area and the minimum area.","title":"Inner-workings / Algorithms"},{"location":"perception/object_merger/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/object_merger/#input","text":"Name Type Description input/object0 autoware_auto_perception_msgs::msg::DetectedObjects detection objects input/object1 autoware_auto_perception_msgs::msg::DetectedObjects detection objects","title":"Input"},{"location":"perception/object_merger/#output","text":"Name Type Description output/object autoware_auto_perception_msgs::msg::DetectedObjects modified Objects","title":"Output"},{"location":"perception/object_merger/#parameters","text":"No Parameters.","title":"Parameters"},{"location":"perception/object_merger/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/object_merger/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/object_merger/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/object_merger/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"perception/object_merger/#optional-future-extensions-unimplemented-parts","text":"Data association algorithm was the same as that of multi_object_tracker, but the algorithm of multi_object_tracker was already updated.","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/object_range_splitter/","text":"object_range_splitter # Purpose # object_range_splitter is a package to divide detected objects into two messages by the distance from the origin. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description input/object autoware_auto_perception_msgs::msg::DetectedObjects detected objects Output # Name Type Description output/long_range_object autoware_auto_perception_msgs::msg::DetectedObjects long range detected objects output/short_range_object autoware_auto_perception_msgs::msg::DetectedObjects short range detected objects Parameters # Name Type Description split_range float the distance boundary to divide detected objects [m] Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"object_range_splitter"},{"location":"perception/object_range_splitter/#object_range_splitter","text":"","title":"object_range_splitter"},{"location":"perception/object_range_splitter/#purpose","text":"object_range_splitter is a package to divide detected objects into two messages by the distance from the origin.","title":"Purpose"},{"location":"perception/object_range_splitter/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"perception/object_range_splitter/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/object_range_splitter/#input","text":"Name Type Description input/object autoware_auto_perception_msgs::msg::DetectedObjects detected objects","title":"Input"},{"location":"perception/object_range_splitter/#output","text":"Name Type Description output/long_range_object autoware_auto_perception_msgs::msg::DetectedObjects long range detected objects output/short_range_object autoware_auto_perception_msgs::msg::DetectedObjects short range detected objects","title":"Output"},{"location":"perception/object_range_splitter/#parameters","text":"Name Type Description split_range float the distance boundary to divide detected objects [m]","title":"Parameters"},{"location":"perception/object_range_splitter/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/object_range_splitter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/object_range_splitter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/object_range_splitter/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"perception/object_range_splitter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/occupancy_grid_map_outlier_filter/","text":"occupancy_grid_map_outlier_filter # Purpose # This node is an outlier filter based on a occupancy grid map. Depending on the implementation of occupancy grid map, it can be called an outlier filter in time series, since the occupancy grid map expresses the occupancy probabilities in time series. Inner-workings / Algorithms # Use the occupancy grid map to separate point clouds into those with low occupancy probability and those with high occupancy probability. The point clouds that belong to the low occupancy probability are not necessarily outliers. In particular, the top of the moving object tends to belong to the low occupancy probability. Therefore, if use_radius_search_2d_filter is true, then apply an radius search 2d outlier filter to the point cloud that is determined to have a low occupancy probability. For each low occupancy probability point, determine the outlier from the radius ( radius_search_2d_filter/search_radius ) and the number of point clouds. In this case, the point cloud to be referenced is not only low occupancy probability points, but all point cloud including high occupancy probability points. The number of point clouds can be multiplied by radius_search_2d_filter/min_points_and_distance_ratio and distance from base link. However, the minimum and maximum number of point clouds is limited. The following video is a sample. Yellow points are high occupancy probability, green points are low occupancy probability which is not an outlier, and red points are outliers. At around 0:15 and 1:16 in the first video, a bird crosses the road, but it is considered as an outlier. movie1 movie2 Inputs / Outputs # Input # Name Type Description ~/input/pointcloud sensor_msgs/PointCloud2 Obstacle point cloud with ground removed. ~/input/occupancy_grid_map nav_msgs/OccupancyGrid A map in which the probability of the presence of an obstacle is occupancy probability map Output # Name Type Description ~/output/pointcloud sensor_msgs/PointCloud2 Point cloud with outliers removed. trajectory ~/output/debug/outlier/pointcloud sensor_msgs/PointCloud2 Point clouds removed as outliers. ~/output/debug/low_confidence/pointcloud sensor_msgs/PointCloud2 Point clouds that had a low probability of occupancy in the occupancy grid map. However, it is not considered as an outlier. ~/output/debug/high_confidence/pointcloud sensor_msgs/PointCloud2 Point clouds that had a high probability of occupancy in the occupancy grid map. trajectory Parameters # Name Type Description map_frame string map frame id base_link_frame string base link frame id cost_threshold int Cost threshold of occupancy grid map (0~100). 100 means 100% probability that there is an obstacle, close to 50 means that it is indistinguishable whether it is an obstacle or free space, 0 means that there is no obstacle. enable_debugger bool Whether to output the point cloud for debugging. use_radius_search_2d_filter bool Whether or not to apply density-based outlier filters to objects that are judged to have low probability of occupancy on the occupancy grid map. radius_search_2d_filter/search_radius float Radius when calculating the density radius_search_2d_filter/min_points_and_distance_ratio float Threshold value of the number of point clouds per radius when the distance from baselink is 1m, because the number of point clouds varies with the distance from baselink. radius_search_2d_filter/min_points int Minimum number of point clouds per radius radius_search_2d_filter/max_points int Maximum number of point clouds per radius Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"occupancy_grid_map_outlier_filter"},{"location":"perception/occupancy_grid_map_outlier_filter/#occupancy_grid_map_outlier_filter","text":"","title":"occupancy_grid_map_outlier_filter"},{"location":"perception/occupancy_grid_map_outlier_filter/#purpose","text":"This node is an outlier filter based on a occupancy grid map. Depending on the implementation of occupancy grid map, it can be called an outlier filter in time series, since the occupancy grid map expresses the occupancy probabilities in time series.","title":"Purpose"},{"location":"perception/occupancy_grid_map_outlier_filter/#inner-workings-algorithms","text":"Use the occupancy grid map to separate point clouds into those with low occupancy probability and those with high occupancy probability. The point clouds that belong to the low occupancy probability are not necessarily outliers. In particular, the top of the moving object tends to belong to the low occupancy probability. Therefore, if use_radius_search_2d_filter is true, then apply an radius search 2d outlier filter to the point cloud that is determined to have a low occupancy probability. For each low occupancy probability point, determine the outlier from the radius ( radius_search_2d_filter/search_radius ) and the number of point clouds. In this case, the point cloud to be referenced is not only low occupancy probability points, but all point cloud including high occupancy probability points. The number of point clouds can be multiplied by radius_search_2d_filter/min_points_and_distance_ratio and distance from base link. However, the minimum and maximum number of point clouds is limited. The following video is a sample. Yellow points are high occupancy probability, green points are low occupancy probability which is not an outlier, and red points are outliers. At around 0:15 and 1:16 in the first video, a bird crosses the road, but it is considered as an outlier. movie1 movie2","title":"Inner-workings / Algorithms"},{"location":"perception/occupancy_grid_map_outlier_filter/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/occupancy_grid_map_outlier_filter/#input","text":"Name Type Description ~/input/pointcloud sensor_msgs/PointCloud2 Obstacle point cloud with ground removed. ~/input/occupancy_grid_map nav_msgs/OccupancyGrid A map in which the probability of the presence of an obstacle is occupancy probability map","title":"Input"},{"location":"perception/occupancy_grid_map_outlier_filter/#output","text":"Name Type Description ~/output/pointcloud sensor_msgs/PointCloud2 Point cloud with outliers removed. trajectory ~/output/debug/outlier/pointcloud sensor_msgs/PointCloud2 Point clouds removed as outliers. ~/output/debug/low_confidence/pointcloud sensor_msgs/PointCloud2 Point clouds that had a low probability of occupancy in the occupancy grid map. However, it is not considered as an outlier. ~/output/debug/high_confidence/pointcloud sensor_msgs/PointCloud2 Point clouds that had a high probability of occupancy in the occupancy grid map. trajectory","title":"Output"},{"location":"perception/occupancy_grid_map_outlier_filter/#parameters","text":"Name Type Description map_frame string map frame id base_link_frame string base link frame id cost_threshold int Cost threshold of occupancy grid map (0~100). 100 means 100% probability that there is an obstacle, close to 50 means that it is indistinguishable whether it is an obstacle or free space, 0 means that there is no obstacle. enable_debugger bool Whether to output the point cloud for debugging. use_radius_search_2d_filter bool Whether or not to apply density-based outlier filters to objects that are judged to have low probability of occupancy on the occupancy grid map. radius_search_2d_filter/search_radius float Radius when calculating the density radius_search_2d_filter/min_points_and_distance_ratio float Threshold value of the number of point clouds per radius when the distance from baselink is 1m, because the number of point clouds varies with the distance from baselink. radius_search_2d_filter/min_points int Minimum number of point clouds per radius radius_search_2d_filter/max_points int Maximum number of point clouds per radius","title":"Parameters"},{"location":"perception/occupancy_grid_map_outlier_filter/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/occupancy_grid_map_outlier_filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/occupancy_grid_map_outlier_filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/occupancy_grid_map_outlier_filter/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"perception/occupancy_grid_map_outlier_filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/radar_tracks_msgs_converter/","text":"radar_tracks_msgs_converter # This package convert from radar_msgs/msg/RadarTracks into autoware_auto_perception_msgs/msg/TrackedObject . Calculation cost is O(n). n: The number of radar objects Design # Input / Output # Input ~/input/radar_objects (radar_msgs/msg/RadarTracks.msg): Converted topic ~/input/twist (geometry_msgs/msg/TwistStamped.msg): Ego vehicle twist Output ~/output/radar_objects (autoware_auto_perception_msgs/msg/TrackedObject.msg): Converted topic Parameters # update_rate_hz (double): The update rate [hz]. Default parameter is 20.0 new_frame_id (string): The header frame of output topic. Default parameter is \"base_link\" use_twist_compensation (bool): If the parameter is true, then the twist of output objects' topic is compensated by ego vehicle motion. Default parameter is \"false\" Note # This package convert the label from radar_msgs/msg/RadarTrack.msg to Autoware label. Label id is defined as below. RadarTrack Autoware UNKNOWN 32000 0 CAR 32001 1 TRUCK 32002 2 BUS 32003 3 TRAILER 32004 4 MOTORCYCLE 32005 5 BICYCLE 32006 6 PEDESTRIAN 32007 7 radar_msgs/msg/RadarTrack.msg : additional vendor-specific classifications are permitted starting from 32000. Autoware objects label","title":"radar_tracks_msgs_converter"},{"location":"perception/radar_tracks_msgs_converter/#radar_tracks_msgs_converter","text":"This package convert from radar_msgs/msg/RadarTracks into autoware_auto_perception_msgs/msg/TrackedObject . Calculation cost is O(n). n: The number of radar objects","title":"radar_tracks_msgs_converter"},{"location":"perception/radar_tracks_msgs_converter/#design","text":"","title":"Design"},{"location":"perception/radar_tracks_msgs_converter/#input-output","text":"Input ~/input/radar_objects (radar_msgs/msg/RadarTracks.msg): Converted topic ~/input/twist (geometry_msgs/msg/TwistStamped.msg): Ego vehicle twist Output ~/output/radar_objects (autoware_auto_perception_msgs/msg/TrackedObject.msg): Converted topic","title":"Input / Output"},{"location":"perception/radar_tracks_msgs_converter/#parameters","text":"update_rate_hz (double): The update rate [hz]. Default parameter is 20.0 new_frame_id (string): The header frame of output topic. Default parameter is \"base_link\" use_twist_compensation (bool): If the parameter is true, then the twist of output objects' topic is compensated by ego vehicle motion. Default parameter is \"false\"","title":"Parameters"},{"location":"perception/radar_tracks_msgs_converter/#note","text":"This package convert the label from radar_msgs/msg/RadarTrack.msg to Autoware label. Label id is defined as below. RadarTrack Autoware UNKNOWN 32000 0 CAR 32001 1 TRUCK 32002 2 BUS 32003 3 TRAILER 32004 4 MOTORCYCLE 32005 5 BICYCLE 32006 6 PEDESTRIAN 32007 7 radar_msgs/msg/RadarTrack.msg : additional vendor-specific classifications are permitted starting from 32000. Autoware objects label","title":"Note"},{"location":"perception/shape_estimation/","text":"shape_estimation # Purpose # This node calculates a refined object shape (bounding box, cylinder, convex hull) in which a pointcloud cluster fits according to a label. Inner-workings / Algorithms # Fitting algorithms # bounding box L-shape fitting. See reference below for details. cylinder cv::minEnclosingCircle convex hull cv::convexHull Inputs / Outputs # Input # Name Type Description input tier4_perception_msgs::msg::DetectedObjectsWithFeature detected objects with labeled cluster Output # Name Type Description output/objects autoware_auto_perception_msgs::msg::DetectedObjects detected objects with refined shape Parameters # Name Type Default Value Description use_corrector bool true The flag to apply rule-based filter use_filter bool true The flag to apply rule-based corrector use_vehicle_reference_yaw bool true The flag to use vehicle reference yaw for corrector Assumptions / Known limits # TBD References/External links # L-shape fitting implementation of the paper: @conference { Zhang-2017-26536 , author = {Xiao Zhang and Wenda Xu and Chiyu Dong and John M. Dolan} , title = {Efficient L-Shape Fitting for Vehicle Detection Using Laser Scanners} , booktitle = {2017 IEEE Intelligent Vehicles Symposium} , year = {2017} , month = {June} , keywords = {autonomous driving, laser scanner, perception, segmentation} , }","title":"shape_estimation"},{"location":"perception/shape_estimation/#shape_estimation","text":"","title":"shape_estimation"},{"location":"perception/shape_estimation/#purpose","text":"This node calculates a refined object shape (bounding box, cylinder, convex hull) in which a pointcloud cluster fits according to a label.","title":"Purpose"},{"location":"perception/shape_estimation/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"perception/shape_estimation/#fitting-algorithms","text":"bounding box L-shape fitting. See reference below for details. cylinder cv::minEnclosingCircle convex hull cv::convexHull","title":"Fitting algorithms"},{"location":"perception/shape_estimation/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/shape_estimation/#input","text":"Name Type Description input tier4_perception_msgs::msg::DetectedObjectsWithFeature detected objects with labeled cluster","title":"Input"},{"location":"perception/shape_estimation/#output","text":"Name Type Description output/objects autoware_auto_perception_msgs::msg::DetectedObjects detected objects with refined shape","title":"Output"},{"location":"perception/shape_estimation/#parameters","text":"Name Type Default Value Description use_corrector bool true The flag to apply rule-based filter use_filter bool true The flag to apply rule-based corrector use_vehicle_reference_yaw bool true The flag to use vehicle reference yaw for corrector","title":"Parameters"},{"location":"perception/shape_estimation/#assumptions-known-limits","text":"TBD","title":"Assumptions / Known limits"},{"location":"perception/shape_estimation/#referencesexternal-links","text":"L-shape fitting implementation of the paper: @conference { Zhang-2017-26536 , author = {Xiao Zhang and Wenda Xu and Chiyu Dong and John M. Dolan} , title = {Efficient L-Shape Fitting for Vehicle Detection Using Laser Scanners} , booktitle = {2017 IEEE Intelligent Vehicles Symposium} , year = {2017} , month = {June} , keywords = {autonomous driving, laser scanner, perception, segmentation} , }","title":"References/External links"},{"location":"perception/tensorrt_yolo/","text":"tensorrt_yolo # Purpose # This package detects 2D bounding boxes for target objects e.g., cars, trucks, bicycles, and pedestrians on a image based on YOLO(You only look once) model. Inner-workings / Algorithms # Cite # yolov3 Redmon, J., & Farhadi, A. (2018). Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767. yolov4 Bochkovskiy, A., Wang, C. Y., & Liao, H. Y. M. (2020). Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934. yolov5 Jocher, G., et al. (2021). ultralytics/yolov5: v6.0 - YOLOv5n 'Nano' models, Roboflow integration, TensorFlow export, OpenCV DNN support (v6.0). Zenodo. https://doi.org/10.5281/zenodo.5563715 Inputs / Outputs # Input # Name Type Description in/image sensor_msgs/Image The input image Output # Name Type Description out/objects tier4_perception_msgs/DetectedObjectsWithFeature The detected objects with 2D bounding boxes out/image sensor_msgs/Image The image with 2D bounding boxes for visualization Parameters # Core Parameters # Name Type Default Value Description anchors double array [10.0, 13.0, 16.0, 30.0, 33.0, 23.0, 30.0, 61.0, 62.0, 45.0, 59.0, 119.0, 116.0, 90.0, 156.0, 198.0, 373.0, 326.0] The anchors to create bounding box candidates scale_x_y double array [1.0, 1.0, 1.0] The scale parameter to eliminate grid sensitivity score_thresh double 0.1 If the objectness score is less than this value, the object is ignored in yolo layer. iou_thresh double 0.45 The iou threshold for NMS method detections_per_im int 100 The maximum detection number for one frame use_darknet_layer bool true The flag to use yolo layer in darknet ignore_thresh double 0.5 If the output score is less than this value, ths object is ignored. Node Parameters # Name Type Default Value Description onnx_file string \"\" The onnx file name for yolo model engine_file string \"\" The tensorrt engine file name for yolo model label_file string \"\" The label file with label names for detected objects written on it calib_image_directory string \"\" The directory name including calibration images for int8 inference calib_cache_file string \"\" The calibration cache file for int8 inference mode string \"FP32\" The inference mode: \"FP32\", \"FP16\", \"INT8\" Assumptions / Known limits # This package includes multiple licenses. Onnx model # YOLOv3 # YOLOv3 : Converted from darknet weight file and conf file . YOLOv4 # YOLOv4 : Converted from darknet weight file and conf file . YOLOv5 # Refer to this guide YOLOv5s YOLOv5m YOLOv5l YOLOv5x Limitations # If you want to run multiple instances of this node for multiple cameras using \"yolo.launch.xml\", first of all, create a TensorRT engine by running the \"tensorrt_yolo.launch.xml\" launch file separately for each GPU. Otherwise, multiple instances of the node trying to create the same TensorRT engine can cause potential problems. Reference repositories # https://github.com/pjreddie/darknet https://github.com/AlexeyAB/darknet https://github.com/ultralytics/yolov5 https://github.com/wang-xinyu/tensorrtx https://github.com/NVIDIA/retinanet-examples","title":"tensorrt_yolo"},{"location":"perception/tensorrt_yolo/#tensorrt_yolo","text":"","title":"tensorrt_yolo"},{"location":"perception/tensorrt_yolo/#purpose","text":"This package detects 2D bounding boxes for target objects e.g., cars, trucks, bicycles, and pedestrians on a image based on YOLO(You only look once) model.","title":"Purpose"},{"location":"perception/tensorrt_yolo/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"perception/tensorrt_yolo/#cite","text":"yolov3 Redmon, J., & Farhadi, A. (2018). Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767. yolov4 Bochkovskiy, A., Wang, C. Y., & Liao, H. Y. M. (2020). Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934. yolov5 Jocher, G., et al. (2021). ultralytics/yolov5: v6.0 - YOLOv5n 'Nano' models, Roboflow integration, TensorFlow export, OpenCV DNN support (v6.0). Zenodo. https://doi.org/10.5281/zenodo.5563715","title":"Cite"},{"location":"perception/tensorrt_yolo/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/tensorrt_yolo/#input","text":"Name Type Description in/image sensor_msgs/Image The input image","title":"Input"},{"location":"perception/tensorrt_yolo/#output","text":"Name Type Description out/objects tier4_perception_msgs/DetectedObjectsWithFeature The detected objects with 2D bounding boxes out/image sensor_msgs/Image The image with 2D bounding boxes for visualization","title":"Output"},{"location":"perception/tensorrt_yolo/#parameters","text":"","title":"Parameters"},{"location":"perception/tensorrt_yolo/#core-parameters","text":"Name Type Default Value Description anchors double array [10.0, 13.0, 16.0, 30.0, 33.0, 23.0, 30.0, 61.0, 62.0, 45.0, 59.0, 119.0, 116.0, 90.0, 156.0, 198.0, 373.0, 326.0] The anchors to create bounding box candidates scale_x_y double array [1.0, 1.0, 1.0] The scale parameter to eliminate grid sensitivity score_thresh double 0.1 If the objectness score is less than this value, the object is ignored in yolo layer. iou_thresh double 0.45 The iou threshold for NMS method detections_per_im int 100 The maximum detection number for one frame use_darknet_layer bool true The flag to use yolo layer in darknet ignore_thresh double 0.5 If the output score is less than this value, ths object is ignored.","title":"Core Parameters"},{"location":"perception/tensorrt_yolo/#node-parameters","text":"Name Type Default Value Description onnx_file string \"\" The onnx file name for yolo model engine_file string \"\" The tensorrt engine file name for yolo model label_file string \"\" The label file with label names for detected objects written on it calib_image_directory string \"\" The directory name including calibration images for int8 inference calib_cache_file string \"\" The calibration cache file for int8 inference mode string \"FP32\" The inference mode: \"FP32\", \"FP16\", \"INT8\"","title":"Node Parameters"},{"location":"perception/tensorrt_yolo/#assumptions-known-limits","text":"This package includes multiple licenses.","title":"Assumptions / Known limits"},{"location":"perception/tensorrt_yolo/#onnx-model","text":"","title":"Onnx model"},{"location":"perception/tensorrt_yolo/#yolov3","text":"YOLOv3 : Converted from darknet weight file and conf file .","title":"YOLOv3"},{"location":"perception/tensorrt_yolo/#yolov4","text":"YOLOv4 : Converted from darknet weight file and conf file .","title":"YOLOv4"},{"location":"perception/tensorrt_yolo/#yolov5","text":"Refer to this guide YOLOv5s YOLOv5m YOLOv5l YOLOv5x","title":"YOLOv5"},{"location":"perception/tensorrt_yolo/#limitations","text":"If you want to run multiple instances of this node for multiple cameras using \"yolo.launch.xml\", first of all, create a TensorRT engine by running the \"tensorrt_yolo.launch.xml\" launch file separately for each GPU. Otherwise, multiple instances of the node trying to create the same TensorRT engine can cause potential problems.","title":"Limitations"},{"location":"perception/tensorrt_yolo/#reference-repositories","text":"https://github.com/pjreddie/darknet https://github.com/AlexeyAB/darknet https://github.com/ultralytics/yolov5 https://github.com/wang-xinyu/tensorrtx https://github.com/NVIDIA/retinanet-examples","title":"Reference repositories"},{"location":"perception/traffic_light_classifier/","text":"traffic_light_classifier # Purpose # traffic_light_classifier is a package for classifying traffic light labels using cropped image around a traffic light. This package has two classifier models: cnn_classifier and hsv_classifier . Inner-workings / Algorithms # cnn_classifier # Traffic light labels are classified by MobileNetV2. hsv_classifier # Traffic light colors (green, yellow and red) are classified in HSV model. About Label # The message type is designed to comply with the unified road signs proposed at the Vienna Convention . This idea has been also proposed in Autoware.Auto . There are rules for naming labels that nodes receive. One traffic light is represented by the following character string separated by commas. color1-shape1, color2-shape2 . For example, the simple red and red cross traffic light label must be expressed as \"red-circle, red-cross\". These colors and shapes are assigned to the message as follows: Inputs / Outputs # Input # Name Type Description ~/input/image sensor_msgs::msg::Image input image ~/input/rois autoware_auto_perception_msgs::msg::TrafficLightRoiArray rois of traffic lights Output # Name Type Description ~/output/traffic_signals autoware_auto_perception_msgs::msg::TrafficSignalArray classified signals ~/output/debug/image sensor_msgs::msg::Image image for debugging Parameters # Node Parameters # Name Type Description classifier_type int if the value is 1 , cnn_classifier is used Core Parameters # cnn_classifier # Name Type Description model_file_path str path to the model file label_file_path str path to the label file precision str TensorRT precision, fp16 or int8 input_c str the channel size of an input image input_h str the height of an input image input_w str the width of an input image hsv_classifier # Name Type Description green_min_h int the minimum hue of green color green_min_s int the minimum saturation of green color green_min_v int the minimum value (brightness) of green color green_max_h int the maximum hue of green color green_max_s int the maximum saturation of green color green_max_v int the maximum value (brightness) of green color yellow_min_h int the minimum hue of yellow color yellow_min_s int the minimum saturation of yellow color yellow_min_v int the minimum value (brightness) of yellow color yellow_max_h int the maximum hue of yellow color yellow_max_s int the maximum saturation of yellow color yellow_max_v int the maximum value (brightness) of yellow color red_min_h int the minimum hue of red color red_min_s int the minimum saturation of red color red_min_v int the minimum value (brightness) of red color red_max_h int the maximum hue of red color red_max_s int the maximum saturation of red color red_max_v int the maximum value (brightness) of red color Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # References/External links # [1] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov and L. Chen, \"MobileNetV2: Inverted Residuals and Linear Bottlenecks,\" 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, 2018, pp. 4510-4520, doi: 10.1109/CVPR.2018.00474. (Optional) Future extensions / Unimplemented parts #","title":"traffic_light_classifier"},{"location":"perception/traffic_light_classifier/#traffic_light_classifier","text":"","title":"traffic_light_classifier"},{"location":"perception/traffic_light_classifier/#purpose","text":"traffic_light_classifier is a package for classifying traffic light labels using cropped image around a traffic light. This package has two classifier models: cnn_classifier and hsv_classifier .","title":"Purpose"},{"location":"perception/traffic_light_classifier/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"perception/traffic_light_classifier/#cnn_classifier","text":"Traffic light labels are classified by MobileNetV2.","title":"cnn_classifier"},{"location":"perception/traffic_light_classifier/#hsv_classifier","text":"Traffic light colors (green, yellow and red) are classified in HSV model.","title":"hsv_classifier"},{"location":"perception/traffic_light_classifier/#about-label","text":"The message type is designed to comply with the unified road signs proposed at the Vienna Convention . This idea has been also proposed in Autoware.Auto . There are rules for naming labels that nodes receive. One traffic light is represented by the following character string separated by commas. color1-shape1, color2-shape2 . For example, the simple red and red cross traffic light label must be expressed as \"red-circle, red-cross\". These colors and shapes are assigned to the message as follows:","title":"About Label"},{"location":"perception/traffic_light_classifier/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/traffic_light_classifier/#input","text":"Name Type Description ~/input/image sensor_msgs::msg::Image input image ~/input/rois autoware_auto_perception_msgs::msg::TrafficLightRoiArray rois of traffic lights","title":"Input"},{"location":"perception/traffic_light_classifier/#output","text":"Name Type Description ~/output/traffic_signals autoware_auto_perception_msgs::msg::TrafficSignalArray classified signals ~/output/debug/image sensor_msgs::msg::Image image for debugging","title":"Output"},{"location":"perception/traffic_light_classifier/#parameters","text":"","title":"Parameters"},{"location":"perception/traffic_light_classifier/#node-parameters","text":"Name Type Description classifier_type int if the value is 1 , cnn_classifier is used","title":"Node Parameters"},{"location":"perception/traffic_light_classifier/#core-parameters","text":"","title":"Core Parameters"},{"location":"perception/traffic_light_classifier/#cnn_classifier_1","text":"Name Type Description model_file_path str path to the model file label_file_path str path to the label file precision str TensorRT precision, fp16 or int8 input_c str the channel size of an input image input_h str the height of an input image input_w str the width of an input image","title":"cnn_classifier"},{"location":"perception/traffic_light_classifier/#hsv_classifier_1","text":"Name Type Description green_min_h int the minimum hue of green color green_min_s int the minimum saturation of green color green_min_v int the minimum value (brightness) of green color green_max_h int the maximum hue of green color green_max_s int the maximum saturation of green color green_max_v int the maximum value (brightness) of green color yellow_min_h int the minimum hue of yellow color yellow_min_s int the minimum saturation of yellow color yellow_min_v int the minimum value (brightness) of yellow color yellow_max_h int the maximum hue of yellow color yellow_max_s int the maximum saturation of yellow color yellow_max_v int the maximum value (brightness) of yellow color red_min_h int the minimum hue of red color red_min_s int the minimum saturation of red color red_min_v int the minimum value (brightness) of red color red_max_h int the maximum hue of red color red_max_s int the maximum saturation of red color red_max_v int the maximum value (brightness) of red color","title":"hsv_classifier"},{"location":"perception/traffic_light_classifier/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/traffic_light_classifier/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/traffic_light_classifier/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/traffic_light_classifier/#referencesexternal-links","text":"[1] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov and L. Chen, \"MobileNetV2: Inverted Residuals and Linear Bottlenecks,\" 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, 2018, pp. 4510-4520, doi: 10.1109/CVPR.2018.00474.","title":"References/External links"},{"location":"perception/traffic_light_classifier/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"perception/traffic_light_map_based_detector/","text":"The traffic_light_map_based_detector Package # Overview # traffic_light_map_based_detector calculates where the traffic lights will appear in the image based on the HD map. Calibration and vibration errors can be entered as parameters, and the size of the detected RegionOfInterest will change according to the error. If the node receives route information, it only looks at traffic lights on that route. If the node receives no route information, it looks at a radius of 200 meters and the angle between the traffic light and the camera is less than 40 degrees. Input topics # Name Type Description ~input/vector_map autoware_auto_mapping_msgs::HADMapBin vector map ~input/camera_info sensor_msgs::CameraInfo target camera parameter ~input/route autoware_auto_planning_msgs::HADMapRoute optional: route Output topics # Name Type Description ~output/rois autoware_auto_perception_msgs::TrafficLightRoiArray location of traffic lights in image corresponding to the camera info ~debug/markers visualization_msgs::MarkerArray visualization to debug Node parameters # Parameter Type Description max_vibration_pitch double Maximum error in pitch direction. If -5~+5, it will be 10. max_vibration_yaw double Maximum error in yaw direction. If -5~+5, it will be 10. max_vibration_height double Maximum error in height direction. If -5~+5, it will be 10. max_vibration_width double Maximum error in width direction. If -5~+5, it will be 10. max_vibration_depth double Maximum error in depth direction. If -5~+5, it will be 10.","title":"The `traffic_light_map_based_detector` Package"},{"location":"perception/traffic_light_map_based_detector/#the-traffic_light_map_based_detector-package","text":"","title":"The traffic_light_map_based_detector Package"},{"location":"perception/traffic_light_map_based_detector/#overview","text":"traffic_light_map_based_detector calculates where the traffic lights will appear in the image based on the HD map. Calibration and vibration errors can be entered as parameters, and the size of the detected RegionOfInterest will change according to the error. If the node receives route information, it only looks at traffic lights on that route. If the node receives no route information, it looks at a radius of 200 meters and the angle between the traffic light and the camera is less than 40 degrees.","title":"Overview"},{"location":"perception/traffic_light_map_based_detector/#input-topics","text":"Name Type Description ~input/vector_map autoware_auto_mapping_msgs::HADMapBin vector map ~input/camera_info sensor_msgs::CameraInfo target camera parameter ~input/route autoware_auto_planning_msgs::HADMapRoute optional: route","title":"Input topics"},{"location":"perception/traffic_light_map_based_detector/#output-topics","text":"Name Type Description ~output/rois autoware_auto_perception_msgs::TrafficLightRoiArray location of traffic lights in image corresponding to the camera info ~debug/markers visualization_msgs::MarkerArray visualization to debug","title":"Output topics"},{"location":"perception/traffic_light_map_based_detector/#node-parameters","text":"Parameter Type Description max_vibration_pitch double Maximum error in pitch direction. If -5~+5, it will be 10. max_vibration_yaw double Maximum error in yaw direction. If -5~+5, it will be 10. max_vibration_height double Maximum error in height direction. If -5~+5, it will be 10. max_vibration_width double Maximum error in width direction. If -5~+5, it will be 10. max_vibration_depth double Maximum error in depth direction. If -5~+5, it will be 10.","title":"Node parameters"},{"location":"perception/traffic_light_ssd_fine_detector/","text":"traffic_light_ssd_fine_detector # Purpose # It is a package for traffic light detection using MobileNetV2 and SSDLite. The trained model is based on pytorch-ssd . Inner-workings / Algorithms # Based on the camera image and the global ROI array detected by map_based_detection node, a CNN-based detection method enables highly accurate traffic light detection. Inputs / Outputs # Input # Name Type Description ~/input/image sensor_msgs/Image The full size camera image ~/input/rois autoware_auto_perception_msgs::msg::TrafficLightRoiArray The array of ROIs detected by map_based_detector Output # Name Type Description ~/output/rois autoware_auto_perception_msgs::msg::TrafficLightRoiArray The detected accurate rois ~/debug/exe_time_ms tier4_debug_msgs::msg::Float32Stamped The time taken for inference Parameters # Core Parameters # Name Type Default Value Description score_thresh double 0.7 If the objectness score is less than this value, the object is ignored mean std::vector [0.5,0.5,0.5] Average value of the normalized values of the image data used for training std std::vector [0.5,0.5,0.5] Standard deviation of the normalized values of the image data used for training Node Parameters # Name Type Default Value Description onnx_file string \"./data/mb2-ssd-lite-tlr.onnx\" The onnx file name for yolo model label_file string \"./data/voc_labels_tl.txt\" The label file with label names for detected objects written on it mode string \"FP32\" The inference mode: \"FP32\", \"FP16\", \"INT8\" max_batch_size int 8 The size of the batch processed at one time by inference by TensorRT approximate_sync bool false Flag for whether to ues approximate sync policy Assumptions / Known limits # Onnx model # https://drive.google.com/uc?id=1USFDPRH9JrVdGoqt27qHjRgittwc0kcO Reference repositories # pytorch-ssd github repository https://github.com/qfgaohao/pytorch-ssd MobileNetV2 M. Sandler, A. Howard, M. Zhu, A. Zhmoginov and L. Chen, \"MobileNetV2: Inverted Residuals and Linear Bottlenecks,\" 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, 2018, pp. 4510-4520, doi: 10.1109/CVPR.2018.00474.","title":"traffic_light_ssd_fine_detector"},{"location":"perception/traffic_light_ssd_fine_detector/#traffic_light_ssd_fine_detector","text":"","title":"traffic_light_ssd_fine_detector"},{"location":"perception/traffic_light_ssd_fine_detector/#purpose","text":"It is a package for traffic light detection using MobileNetV2 and SSDLite. The trained model is based on pytorch-ssd .","title":"Purpose"},{"location":"perception/traffic_light_ssd_fine_detector/#inner-workings-algorithms","text":"Based on the camera image and the global ROI array detected by map_based_detection node, a CNN-based detection method enables highly accurate traffic light detection.","title":"Inner-workings / Algorithms"},{"location":"perception/traffic_light_ssd_fine_detector/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/traffic_light_ssd_fine_detector/#input","text":"Name Type Description ~/input/image sensor_msgs/Image The full size camera image ~/input/rois autoware_auto_perception_msgs::msg::TrafficLightRoiArray The array of ROIs detected by map_based_detector","title":"Input"},{"location":"perception/traffic_light_ssd_fine_detector/#output","text":"Name Type Description ~/output/rois autoware_auto_perception_msgs::msg::TrafficLightRoiArray The detected accurate rois ~/debug/exe_time_ms tier4_debug_msgs::msg::Float32Stamped The time taken for inference","title":"Output"},{"location":"perception/traffic_light_ssd_fine_detector/#parameters","text":"","title":"Parameters"},{"location":"perception/traffic_light_ssd_fine_detector/#core-parameters","text":"Name Type Default Value Description score_thresh double 0.7 If the objectness score is less than this value, the object is ignored mean std::vector [0.5,0.5,0.5] Average value of the normalized values of the image data used for training std std::vector [0.5,0.5,0.5] Standard deviation of the normalized values of the image data used for training","title":"Core Parameters"},{"location":"perception/traffic_light_ssd_fine_detector/#node-parameters","text":"Name Type Default Value Description onnx_file string \"./data/mb2-ssd-lite-tlr.onnx\" The onnx file name for yolo model label_file string \"./data/voc_labels_tl.txt\" The label file with label names for detected objects written on it mode string \"FP32\" The inference mode: \"FP32\", \"FP16\", \"INT8\" max_batch_size int 8 The size of the batch processed at one time by inference by TensorRT approximate_sync bool false Flag for whether to ues approximate sync policy","title":"Node Parameters"},{"location":"perception/traffic_light_ssd_fine_detector/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/traffic_light_ssd_fine_detector/#onnx-model","text":"https://drive.google.com/uc?id=1USFDPRH9JrVdGoqt27qHjRgittwc0kcO","title":"Onnx model"},{"location":"perception/traffic_light_ssd_fine_detector/#reference-repositories","text":"pytorch-ssd github repository https://github.com/qfgaohao/pytorch-ssd MobileNetV2 M. Sandler, A. Howard, M. Zhu, A. Zhmoginov and L. Chen, \"MobileNetV2: Inverted Residuals and Linear Bottlenecks,\" 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, 2018, pp. 4510-4520, doi: 10.1109/CVPR.2018.00474.","title":"Reference repositories"},{"location":"perception/traffic_light_visualization/","text":"traffic_light_visualization # Purpose # The traffic_light_visualization is a package that includes two visualizing nodes: traffic_light_map_visualizer is a node that shows traffic lights color status and position on rviz as markers. traffic_light_roi_visualizer is a node that draws the result of traffic light recognition nodes (traffic light status, position and classification probability) on the input image as shown in the following figure and publishes it. Inner-workings / Algorithms # Inputs / Outputs # traffic_light_map_visualizer # Input # Name Type Description ~/input/tl_state autoware_auto_perception_msgs::msg::TrafficSignalArray status of traffic lights ~/input/vector_map autoware_auto_mapping_msgs::msg::HADMapBin vector map Output # Name Type Description ~/output/traffic_light visualization_msgs::msg::MarkerArray marker array that indicates status of traffic lights traffic_light_roi_visualizer # Input # Name Type Description ~/input/tl_state autoware_auto_perception_msgs::msg::TrafficSignalArray status of traffic lights ~/input/image sensor_msgs::msg::Image the image captured by perception cameras ~/input/rois autoware_auto_perception_msgs::msg::TrafficLightRoiArray the ROIs detected by traffic_light_ssd_fine_detector ~/input/rough/rois (option) autoware_auto_perception_msgs::msg::TrafficLightRoiArray the ROIs detected by traffic_light_map_based_detector Output # Name Type Description ~/output/image sensor_msgs::msg::Image output image with ROIs Parameters # traffic_light_map_visualizer # None traffic_light_roi_visualizer # Node Parameters # Name Type Default Value Description enable_fine_detection bool false whether to visualize result of the traffic light fine detection Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"traffic_light_visualization"},{"location":"perception/traffic_light_visualization/#traffic_light_visualization","text":"","title":"traffic_light_visualization"},{"location":"perception/traffic_light_visualization/#purpose","text":"The traffic_light_visualization is a package that includes two visualizing nodes: traffic_light_map_visualizer is a node that shows traffic lights color status and position on rviz as markers. traffic_light_roi_visualizer is a node that draws the result of traffic light recognition nodes (traffic light status, position and classification probability) on the input image as shown in the following figure and publishes it.","title":"Purpose"},{"location":"perception/traffic_light_visualization/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"perception/traffic_light_visualization/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"perception/traffic_light_visualization/#traffic_light_map_visualizer","text":"","title":"traffic_light_map_visualizer"},{"location":"perception/traffic_light_visualization/#input","text":"Name Type Description ~/input/tl_state autoware_auto_perception_msgs::msg::TrafficSignalArray status of traffic lights ~/input/vector_map autoware_auto_mapping_msgs::msg::HADMapBin vector map","title":"Input"},{"location":"perception/traffic_light_visualization/#output","text":"Name Type Description ~/output/traffic_light visualization_msgs::msg::MarkerArray marker array that indicates status of traffic lights","title":"Output"},{"location":"perception/traffic_light_visualization/#traffic_light_roi_visualizer","text":"","title":"traffic_light_roi_visualizer"},{"location":"perception/traffic_light_visualization/#input_1","text":"Name Type Description ~/input/tl_state autoware_auto_perception_msgs::msg::TrafficSignalArray status of traffic lights ~/input/image sensor_msgs::msg::Image the image captured by perception cameras ~/input/rois autoware_auto_perception_msgs::msg::TrafficLightRoiArray the ROIs detected by traffic_light_ssd_fine_detector ~/input/rough/rois (option) autoware_auto_perception_msgs::msg::TrafficLightRoiArray the ROIs detected by traffic_light_map_based_detector","title":"Input"},{"location":"perception/traffic_light_visualization/#output_1","text":"Name Type Description ~/output/image sensor_msgs::msg::Image output image with ROIs","title":"Output"},{"location":"perception/traffic_light_visualization/#parameters","text":"","title":"Parameters"},{"location":"perception/traffic_light_visualization/#traffic_light_map_visualizer_1","text":"None","title":"traffic_light_map_visualizer"},{"location":"perception/traffic_light_visualization/#traffic_light_roi_visualizer_1","text":"","title":"traffic_light_roi_visualizer"},{"location":"perception/traffic_light_visualization/#node-parameters","text":"Name Type Default Value Description enable_fine_detection bool false whether to visualize result of the traffic light fine detection","title":"Node Parameters"},{"location":"perception/traffic_light_visualization/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"perception/traffic_light_visualization/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"perception/traffic_light_visualization/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"perception/traffic_light_visualization/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"perception/traffic_light_visualization/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"planning/behavior_path_planner/","text":"Behavior Path Planner # Purpose / Use cases # The behavior_path_planner module is responsible to generate path based on the traffic situation, drivable area that the vehicle can move (defined in the path msg), turn signal command to be sent to the vehicle interface. Depending on the situation, a suitable module is selected and executed on the behavior tree system. The following modules are currently supported: Lane Following : Generate lane centerline from map. Lane Change : Performs a lane change. This module is performed when it is necessary and a collision check with other vehicles is cleared. Obstacle Avoidance : Perform an obstacle avoidance. This module is for avoidance of a vehicle parked on the edge of the lane or overtaking a low-speed obstacle. Pull Over : Performs a pull over. This module is performed when goal is in the shoulder lane. Ego-vehicle will stop at the goal. Pull Out : Performs a pull out. This module is performed when ego-vehicle is stationary and footprint of ego-vehicle is included in shoulder lane. This module ends when ego-vehicle merges into the road. Side Shift : (For remote control) Shift the path to left or right according to an external instruction. [WIP] Free Space : xxx. Design # Inputs / Outputs / API # output # path [ autoware_auto_planning_msgs/PathWithLaneId ] : The path generated by modules. path_candidate [ autoware_auto_planning_msgs/Path ] : The path the module is about to take. To be executed as soon as external approval is obtained. turn_indicators_cmd [ autoware_auto_vehicle_msgs/TurnIndicatorsCommand ] : Turn indicators command. hazard_lights_cmd [ autoware_auto_vehicle_msgs/HazardLightsCommand ] : Hazard lights command. force_available [ tier4_planning_msgs/PathChangeModuleArray ] : (For remote control) modules that are force-executable. ready_module [ tier4_planning_msgs/PathChangeModule ] : (For remote control) modules that are ready to be executed. running_modules [ tier4_planning_msgs/PathChangeModuleArray ] : (For remote control) Current running module. input # /planning/mission_planning/route [ autoware_auto_planning_msgs/HADMapRoute ] : Current route from start to goal. /map/vector_map [autoware_auto_mapping_msgs/HADMapBin] : Map information. /perception/object_recognition/objects [ autoware_auto_perception_msgs/PredictedObjects ] : dynamic objects from perception module. /tf [ tf2_msgs/TFMessage ] : For ego-pose. /localization/kinematic_state [`nav_msgs/Odometry] : For ego-velocity. path_change_approval [ std_msgs::Bool ] : (For remote control) path_change_force [ tier4_planning_msgs::PathChangeModule ] : (For remote control) Inner-workings / Algorithms # Drivable Area Generation # Drivable lanes are quantized and drawn on an image as a drivable area, whose resolution is drivable_area_resolution . To prevent the quantization from causing instability to the planning modules, drivable area's pose follows the rules below. Drivable area is generated in the map coordinate. Its position is quantized with drivable_area_resolution . Its orientation is 0. The size of the drivable area changes dynamically to realize both decreasing the computation cost and covering enough lanes to follow. For the second purpose, the drivable area covers a certain length forward and backward lanes with some margins defined by parameters. Parameters for drivable area generation # Name Unit Type Description Default value drivable_area_resolution [m] double resolution of the image of the drivable area 0.1 drivable_lane_forward_length [m] double length of the forward lane from the ego covered by the drivable area 50.0 drivable_lane_backward_length [m] double length of the backward lane from the ego covered by the drivable area 5.0 drivable_lane_margin [m] double forward and backward lane margin from the ego covered by the drivable area 3.0 drivable_area_margin [m] double margin of width and height of the drivable area 6.0 Behavior Tree # In the behavior path planner, the behavior tree mechanism is used to manage which modules are activated in which situations. In general, this \"behavior manager\" like function is expected to become bigger as more and more modules are added in the future. To improve maintainability, we adopted the behavior tree. The behavior tree has the following advantages: easy visualization, easy configuration management (behaviors can be changed by replacing configuration files), and high scalability compared to the state machine. The current behavior tree structure is shown below. Each modules (LaneChange, Avoidance, etc) have Request , Ready , and Plan nodes as a common function. Request : Check if there is a request from the module (e.g. LaneChange has a request when there are multi-lanes and the vehicle is not on the preferred lane), Ready : Check if it is safe to execute the plan (e.g. LaneChange is ready when the lane_change path does not have any conflicts with other dynamic objects on S-T space). Plan : Calculates path and set it to the output of the BehaviorTree. Until the internal status returns SUCCESS, it will be in running state and will not transit to another module. ForceApproval : A lane change-specific node that overrides the result of Ready when a forced lane change command is given externally. Lane Following # Generate path from center line of the route. special case # In the case of a route that requires a lane change, the path is generated with a specific distance margin (default: 12.0 m ) from the end of the lane to ensure the minimum distance for lane change. (This function works not only for lane following but also for all modules.) Lane Change # The Lane Change module is activated when lane change is needed and can be safely executed. start lane change condition (need to meet all of the conditions below) # lane change request condition The ego-vehicle isn\u2019t on a preferred_lane . There is neither intersection nor crosswalk on the path of the lane change lane change ready condition Path of the lane change doesn\u2019t collide with other objects (see the figure below) Lane change is allowed by an operator finish lane change condition (need to meet any of the conditions below) # Certain distance (default: 3.0 m ) have passed after the vehicle move to the target lane. Before the base_link exceeds white dotted line, a collision with the object was predicted (only if enable_abort_lane_change is true.) However, when current velocity is lower than 10km/h and the ego-vehicle is near the lane end, the lane change isn\u2019t aborted and the ego-vehicle plans to stop. Then, after no collision is predicted, the ego-vehicle resume the lane change. Collision prediction with obstacles # Predict each position of the ego-vehicle and other vehicle on the target lane of the lane change at t1, t2,...tn If a distance between the ego-vehicle and other one is lower than the threshold ( ego_velocity * stop_time (2s) ) at each time, that is judged as a collision Path Generation # Path to complete the lane change in n + m seconds under an assumption that a velocity of the ego-vehicle is constant. Once the lane change is executed, the path won\u2019t be updated until the \"finish-lane-change-condition\" is satisfied. Avoidance # The Avoidance module is activated when dynamic objects to be avoided exist and can be safely avoided. Target objects # Dynamic objects that satisfy the following conditions are considered to be avoidance targets. Semantics type is CAR , TRUCK , or BUS low speed (default: < 1.0 m/s ) Not being around center line (default: deviation from center > 0.5 m ) Any footprint of the object in on the detection area (driving lane + 1 m margin for lateral direction). How to generate avoidance path # To prevent sudden changes in the vicinity of the ego-position, an avoidance path is generated after a certain distance of straight lane driving. The process of generating the avoidance path is as follows: detect the target object and calculate the lateral shift distance (default: 2.0 m from closest footprint point) calculate the avoidance distance within the constraint of lateral jerk. (default: 0.3 ~ 2.0 m/s3 ) If the maximum jerk constraint is exceeded to keep the straight margin, the avoidance path generation is aborted. generates the smooth path with given avoiding distance and lateral shift length. generate \"return to center\" path if there is no next target within a certain distance (default: 50 m ) after the current target. single objects case # multiple objects case # If there are multiple avoidance targets and the lateral distances of these are close (default: < 0.5m ), those objects are considered as a single avoidance target and avoidance is performed simultaneously with a single steering operation. If the lateral distances of the avoidance targets differ greatly than threshold, multiple steering operations are used to avoid them. Smooth path generation # The path generation is computed in Frenet coordinates. The shift length profile for avoidance is generated by four segmental constant jerk polynomials, and added to the original path. Since the lateral jerk can be approximately seen as a steering maneuver, this calculation yields a result similar to a Clothoid curve. Unimplemented parts / limitations for avoidance # collision check is not implemented shift distance should be variable depending on the situation (left/right is free or occupied). Now it is a fixed value. collaboration with \"avoidance-by-lane-change\". specific rules for traffic condition (need to back to the center line before entering an intersection). Pull Over # The Pull Over module is activated when goal is in the shoulder lane. Ego-vehicle will stop at the goal. start pull over condition (need to meet all of the conditions below) # Pull over request condition The goal is in shoulder lane The distance from ego-vehicle to the destination is long enough for pull over The distance required for a pull over is determined by the sum of the straight distance before pull over, the straight distance after pull over, and the minimum distance required for pull over Pull over ready condition The end of generated path is located before the end of the road Distance required for pull over of generated path is shorter than distance from ego-pose to goal-pose Generated Path of the pull over doesn\u2019t collide with other objects Pull over is allowed by an operator If pull over path is not allowed by an operator, leave distance required for pull over and stop. This case is defined as UC ID: UC-F-11-00004 at ODD Use Case Slide Pullover draft finish pull over condition (need to meet any of the conditions below) # The distance to the goal from your vehicle is lower than threshold (default: < 1m ) The speed of the vehicle is 0. Collision prediction with obstacles # Predict each position of the ego-vehicle and other vehicle on the target lane of the pull over at t1, t2,...tn If a distance between the ego-vehicle and other one is lower than the threshold ( ego_velocity * stop_time (2s) ) at each time, that is judged as a collision Path Generation # The path is generated with a certain margin (default: 0.5 m ) from left boundary of shoulder lane. Pull over distance is calculated by the speed, lateral deviation, and the lateral jerk. The lateral jerk is searched for among the predetermined minimum and maximum values, and the one satisfies ready conditions described above is output. Apply uniform offset to centerline of shoulder lane for ensuring margin In the section between merge start and end, path is shifted by a method that is used to generate avoidance path (four segmental constant jerk polynomials) Combine this path with center line of road lane Parameters for path generation # Name Unit Type Description Default value straight_distance [m] double straight distance after pull over. 5.0 minimum_lateral_jerk [m/s3] double minimum lateral jerk to calculate shifting distance. 0.5 maximum_lateral_jerk [m/s3] double maximum lateral jerk to calculate shifting distance. 2.0 pull_over_velocity [m/s] double Accelerate/decelerate to this speed before the start of the pullover. 3.0 margin [m] double distance from ego-vehicle's footprint to the edge of the shoulder lane. This value determines shift length. 0.5 Unimplemented parts / limitations for pull over # When parking on the shoulder of the road, 3.5m space on the right side should be secured Pull Out # The Pull Out module is activated when ego-vehicle is stationary and footprint of ego-vehicle is included in shoulder lane. This module ends when ego-vehicle merges into the road. start pull out condition (need to meet all of the conditions below) # Pull out request condition The speed of the vehicle is 0. The footprint of ego-vehicle is included in shoulder lane The distance from ego-vehicle to the destination is long enough for pull out (same with pull over) The distance required for a pull out is determined by the sum of the straight distance before pull out, the straight distance after pull out, and the minimum distance required for pull out Pull out ready condition The end of generated path is located before the end of the road Distance required for pull out of generated path is shorter than distance from ego-pose to goal-pose Generated Path is safe (This condition is temporarily invalid) It is possible to enable or disable this condition. (default: disabled ) If safe path cannot be generated from the current position, safe path from a receding position is searched. Pull out is allowed by an operator finish pull out condition (need to meet any of the conditions below) # The distance to the goal of your vehicle is lower than threshold (default: < 1m ) The speed of the vehicle is 0. Safe check with obstacles in shoulder lane # Calculate ego-vehicle's footprint on pull out path between from current position to merge end point. (Illustrated by blue frame) Calculate object's polygon which is located in shoulder lane If a distance between the footprint and the polygon is lower than the threshold (default: 1.5 m ), that is judged as a unsafe path Path Generation # The path is generated with a certain margin from the left edge of the shoulder lane. Unimplemented parts / limitations for pull put # Note that the current module's start judgment condition may incorrectly judge that the vehicle is parked on the shoulder, such as when stopping in front of an intersection. Side Shift # The role of the Side Shift module is to shift the reference path laterally in response to external instructions (such as remote operation). Parameters for path generation # In the figure, straight margin distance is to avoid sudden shifting, that is calculated by max(min_distance_to_start_shifting, ego_speed * time_to_start_shifting) . The shifting distance is calculated by jerk, with minimum speed and minimum distance parameter, described below. The minimum speed is used to prevent sharp shift when ego vehicle is stopped. Name Unit Type Description Default value min_distance_to_start_shifting [m] double minimum straight distance before shift start. 5.0 time_to_start_shifting [s] double time of minimum straight distance before shift start. 1.0 shifting_lateral_jerk [m/s3] double lateral jerk to calculate shifting distance. 0.2 min_shifting_distance [m] double the shifting distance is longer than this length. 5.0 min_shifting_speed [m/s] double lateral jerk is calculated with the greater of current_speed or this speed. 5.56 Smooth goal connection # If the target path contains a goal, modify the points of the path so that the path and the goal are connected smoothly. This process will change the shape of the path by the distance of refine_goal_search_radius_range from the goal. Note that this logic depends on the interpolation algorithm that will be executed in a later module (at the moment it uses spline interpolation), so it needs to be updated in the future. Turn signal # Turn on signal when the planned path crosses lanes or when a right or left turn is required. The turn signal information includes the direction of the turn signal and the distance to the point where the turn signal is needed. From planned path # turn signal direction Calculate the lateral movement distance from the shift start and end point and judges whether the line is crossed or not. If the vehicle straddles the line during lateral movement, the system turns on the blinker. The timing to start lighting is when the time required to reach the shift start point is 3 seconds or less, or when the distance becomes smaller than threshold (default: 10.0 m ). The Japanese Road Traffic Law requires turn signal to be turned on 3.0 sec before the vehicle starts moving sideways, but it also gives a condition based on distance because the turn signal may be turned off by slowing down before the shift start point. distance The distance from the top of ego vehicle to the shift end point. From map information # turn signal direction Determine right or left turns based on the \"turn_direction\" information embedded in the lanelet. distance The distance to the lane where you start to turn. Conciliation # When multiple turn signal conditions are met, the turn signal with the smaller distance is selected. References / External links # This module depends on the external BehaviorTreeCpp library. Future extensions / Unimplemented parts # - Related issues # -","title":"Behavior Path Planner"},{"location":"planning/behavior_path_planner/#behavior-path-planner","text":"","title":"Behavior Path Planner"},{"location":"planning/behavior_path_planner/#purpose-use-cases","text":"The behavior_path_planner module is responsible to generate path based on the traffic situation, drivable area that the vehicle can move (defined in the path msg), turn signal command to be sent to the vehicle interface. Depending on the situation, a suitable module is selected and executed on the behavior tree system. The following modules are currently supported: Lane Following : Generate lane centerline from map. Lane Change : Performs a lane change. This module is performed when it is necessary and a collision check with other vehicles is cleared. Obstacle Avoidance : Perform an obstacle avoidance. This module is for avoidance of a vehicle parked on the edge of the lane or overtaking a low-speed obstacle. Pull Over : Performs a pull over. This module is performed when goal is in the shoulder lane. Ego-vehicle will stop at the goal. Pull Out : Performs a pull out. This module is performed when ego-vehicle is stationary and footprint of ego-vehicle is included in shoulder lane. This module ends when ego-vehicle merges into the road. Side Shift : (For remote control) Shift the path to left or right according to an external instruction. [WIP] Free Space : xxx.","title":"Purpose / Use cases"},{"location":"planning/behavior_path_planner/#design","text":"","title":"Design"},{"location":"planning/behavior_path_planner/#inputs-outputs-api","text":"","title":"Inputs / Outputs / API"},{"location":"planning/behavior_path_planner/#output","text":"path [ autoware_auto_planning_msgs/PathWithLaneId ] : The path generated by modules. path_candidate [ autoware_auto_planning_msgs/Path ] : The path the module is about to take. To be executed as soon as external approval is obtained. turn_indicators_cmd [ autoware_auto_vehicle_msgs/TurnIndicatorsCommand ] : Turn indicators command. hazard_lights_cmd [ autoware_auto_vehicle_msgs/HazardLightsCommand ] : Hazard lights command. force_available [ tier4_planning_msgs/PathChangeModuleArray ] : (For remote control) modules that are force-executable. ready_module [ tier4_planning_msgs/PathChangeModule ] : (For remote control) modules that are ready to be executed. running_modules [ tier4_planning_msgs/PathChangeModuleArray ] : (For remote control) Current running module.","title":"output"},{"location":"planning/behavior_path_planner/#input","text":"/planning/mission_planning/route [ autoware_auto_planning_msgs/HADMapRoute ] : Current route from start to goal. /map/vector_map [autoware_auto_mapping_msgs/HADMapBin] : Map information. /perception/object_recognition/objects [ autoware_auto_perception_msgs/PredictedObjects ] : dynamic objects from perception module. /tf [ tf2_msgs/TFMessage ] : For ego-pose. /localization/kinematic_state [`nav_msgs/Odometry] : For ego-velocity. path_change_approval [ std_msgs::Bool ] : (For remote control) path_change_force [ tier4_planning_msgs::PathChangeModule ] : (For remote control)","title":"input"},{"location":"planning/behavior_path_planner/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"planning/behavior_path_planner/#drivable-area-generation","text":"Drivable lanes are quantized and drawn on an image as a drivable area, whose resolution is drivable_area_resolution . To prevent the quantization from causing instability to the planning modules, drivable area's pose follows the rules below. Drivable area is generated in the map coordinate. Its position is quantized with drivable_area_resolution . Its orientation is 0. The size of the drivable area changes dynamically to realize both decreasing the computation cost and covering enough lanes to follow. For the second purpose, the drivable area covers a certain length forward and backward lanes with some margins defined by parameters.","title":"Drivable Area Generation"},{"location":"planning/behavior_path_planner/#parameters-for-drivable-area-generation","text":"Name Unit Type Description Default value drivable_area_resolution [m] double resolution of the image of the drivable area 0.1 drivable_lane_forward_length [m] double length of the forward lane from the ego covered by the drivable area 50.0 drivable_lane_backward_length [m] double length of the backward lane from the ego covered by the drivable area 5.0 drivable_lane_margin [m] double forward and backward lane margin from the ego covered by the drivable area 3.0 drivable_area_margin [m] double margin of width and height of the drivable area 6.0","title":"Parameters for drivable area generation"},{"location":"planning/behavior_path_planner/#behavior-tree","text":"In the behavior path planner, the behavior tree mechanism is used to manage which modules are activated in which situations. In general, this \"behavior manager\" like function is expected to become bigger as more and more modules are added in the future. To improve maintainability, we adopted the behavior tree. The behavior tree has the following advantages: easy visualization, easy configuration management (behaviors can be changed by replacing configuration files), and high scalability compared to the state machine. The current behavior tree structure is shown below. Each modules (LaneChange, Avoidance, etc) have Request , Ready , and Plan nodes as a common function. Request : Check if there is a request from the module (e.g. LaneChange has a request when there are multi-lanes and the vehicle is not on the preferred lane), Ready : Check if it is safe to execute the plan (e.g. LaneChange is ready when the lane_change path does not have any conflicts with other dynamic objects on S-T space). Plan : Calculates path and set it to the output of the BehaviorTree. Until the internal status returns SUCCESS, it will be in running state and will not transit to another module. ForceApproval : A lane change-specific node that overrides the result of Ready when a forced lane change command is given externally.","title":"Behavior Tree"},{"location":"planning/behavior_path_planner/#lane-following","text":"Generate path from center line of the route.","title":"Lane Following"},{"location":"planning/behavior_path_planner/#special-case","text":"In the case of a route that requires a lane change, the path is generated with a specific distance margin (default: 12.0 m ) from the end of the lane to ensure the minimum distance for lane change. (This function works not only for lane following but also for all modules.)","title":"special case"},{"location":"planning/behavior_path_planner/#lane-change","text":"The Lane Change module is activated when lane change is needed and can be safely executed.","title":"Lane Change"},{"location":"planning/behavior_path_planner/#start-lane-change-condition-need-to-meet-all-of-the-conditions-below","text":"lane change request condition The ego-vehicle isn\u2019t on a preferred_lane . There is neither intersection nor crosswalk on the path of the lane change lane change ready condition Path of the lane change doesn\u2019t collide with other objects (see the figure below) Lane change is allowed by an operator","title":"start lane change condition (need to meet all of the conditions below)"},{"location":"planning/behavior_path_planner/#finish-lane-change-condition-need-to-meet-any-of-the-conditions-below","text":"Certain distance (default: 3.0 m ) have passed after the vehicle move to the target lane. Before the base_link exceeds white dotted line, a collision with the object was predicted (only if enable_abort_lane_change is true.) However, when current velocity is lower than 10km/h and the ego-vehicle is near the lane end, the lane change isn\u2019t aborted and the ego-vehicle plans to stop. Then, after no collision is predicted, the ego-vehicle resume the lane change.","title":"finish lane change condition (need to meet any of the conditions below)"},{"location":"planning/behavior_path_planner/#collision-prediction-with-obstacles","text":"Predict each position of the ego-vehicle and other vehicle on the target lane of the lane change at t1, t2,...tn If a distance between the ego-vehicle and other one is lower than the threshold ( ego_velocity * stop_time (2s) ) at each time, that is judged as a collision","title":"Collision prediction with obstacles"},{"location":"planning/behavior_path_planner/#path-generation","text":"Path to complete the lane change in n + m seconds under an assumption that a velocity of the ego-vehicle is constant. Once the lane change is executed, the path won\u2019t be updated until the \"finish-lane-change-condition\" is satisfied.","title":"Path Generation"},{"location":"planning/behavior_path_planner/#avoidance","text":"The Avoidance module is activated when dynamic objects to be avoided exist and can be safely avoided.","title":"Avoidance"},{"location":"planning/behavior_path_planner/#target-objects","text":"Dynamic objects that satisfy the following conditions are considered to be avoidance targets. Semantics type is CAR , TRUCK , or BUS low speed (default: < 1.0 m/s ) Not being around center line (default: deviation from center > 0.5 m ) Any footprint of the object in on the detection area (driving lane + 1 m margin for lateral direction).","title":"Target objects"},{"location":"planning/behavior_path_planner/#how-to-generate-avoidance-path","text":"To prevent sudden changes in the vicinity of the ego-position, an avoidance path is generated after a certain distance of straight lane driving. The process of generating the avoidance path is as follows: detect the target object and calculate the lateral shift distance (default: 2.0 m from closest footprint point) calculate the avoidance distance within the constraint of lateral jerk. (default: 0.3 ~ 2.0 m/s3 ) If the maximum jerk constraint is exceeded to keep the straight margin, the avoidance path generation is aborted. generates the smooth path with given avoiding distance and lateral shift length. generate \"return to center\" path if there is no next target within a certain distance (default: 50 m ) after the current target.","title":"How to generate avoidance path"},{"location":"planning/behavior_path_planner/#single-objects-case","text":"","title":"single objects case"},{"location":"planning/behavior_path_planner/#multiple-objects-case","text":"If there are multiple avoidance targets and the lateral distances of these are close (default: < 0.5m ), those objects are considered as a single avoidance target and avoidance is performed simultaneously with a single steering operation. If the lateral distances of the avoidance targets differ greatly than threshold, multiple steering operations are used to avoid them.","title":"multiple objects case"},{"location":"planning/behavior_path_planner/#smooth-path-generation","text":"The path generation is computed in Frenet coordinates. The shift length profile for avoidance is generated by four segmental constant jerk polynomials, and added to the original path. Since the lateral jerk can be approximately seen as a steering maneuver, this calculation yields a result similar to a Clothoid curve.","title":"Smooth path generation"},{"location":"planning/behavior_path_planner/#unimplemented-parts-limitations-for-avoidance","text":"collision check is not implemented shift distance should be variable depending on the situation (left/right is free or occupied). Now it is a fixed value. collaboration with \"avoidance-by-lane-change\". specific rules for traffic condition (need to back to the center line before entering an intersection).","title":"Unimplemented parts / limitations for avoidance"},{"location":"planning/behavior_path_planner/#pull-over","text":"The Pull Over module is activated when goal is in the shoulder lane. Ego-vehicle will stop at the goal.","title":"Pull Over"},{"location":"planning/behavior_path_planner/#start-pull-over-condition-need-to-meet-all-of-the-conditions-below","text":"Pull over request condition The goal is in shoulder lane The distance from ego-vehicle to the destination is long enough for pull over The distance required for a pull over is determined by the sum of the straight distance before pull over, the straight distance after pull over, and the minimum distance required for pull over Pull over ready condition The end of generated path is located before the end of the road Distance required for pull over of generated path is shorter than distance from ego-pose to goal-pose Generated Path of the pull over doesn\u2019t collide with other objects Pull over is allowed by an operator If pull over path is not allowed by an operator, leave distance required for pull over and stop. This case is defined as UC ID: UC-F-11-00004 at ODD Use Case Slide Pullover draft","title":"start pull over condition (need to meet all of the conditions below)"},{"location":"planning/behavior_path_planner/#finish-pull-over-condition-need-to-meet-any-of-the-conditions-below","text":"The distance to the goal from your vehicle is lower than threshold (default: < 1m ) The speed of the vehicle is 0.","title":"finish pull over condition (need to meet any of the conditions below)"},{"location":"planning/behavior_path_planner/#collision-prediction-with-obstacles_1","text":"Predict each position of the ego-vehicle and other vehicle on the target lane of the pull over at t1, t2,...tn If a distance between the ego-vehicle and other one is lower than the threshold ( ego_velocity * stop_time (2s) ) at each time, that is judged as a collision","title":"Collision prediction with obstacles"},{"location":"planning/behavior_path_planner/#path-generation_1","text":"The path is generated with a certain margin (default: 0.5 m ) from left boundary of shoulder lane. Pull over distance is calculated by the speed, lateral deviation, and the lateral jerk. The lateral jerk is searched for among the predetermined minimum and maximum values, and the one satisfies ready conditions described above is output. Apply uniform offset to centerline of shoulder lane for ensuring margin In the section between merge start and end, path is shifted by a method that is used to generate avoidance path (four segmental constant jerk polynomials) Combine this path with center line of road lane","title":"Path Generation"},{"location":"planning/behavior_path_planner/#parameters-for-path-generation","text":"Name Unit Type Description Default value straight_distance [m] double straight distance after pull over. 5.0 minimum_lateral_jerk [m/s3] double minimum lateral jerk to calculate shifting distance. 0.5 maximum_lateral_jerk [m/s3] double maximum lateral jerk to calculate shifting distance. 2.0 pull_over_velocity [m/s] double Accelerate/decelerate to this speed before the start of the pullover. 3.0 margin [m] double distance from ego-vehicle's footprint to the edge of the shoulder lane. This value determines shift length. 0.5","title":"Parameters for path generation"},{"location":"planning/behavior_path_planner/#unimplemented-parts-limitations-for-pull-over","text":"When parking on the shoulder of the road, 3.5m space on the right side should be secured","title":"Unimplemented parts / limitations for pull over"},{"location":"planning/behavior_path_planner/#pull-out","text":"The Pull Out module is activated when ego-vehicle is stationary and footprint of ego-vehicle is included in shoulder lane. This module ends when ego-vehicle merges into the road.","title":"Pull Out"},{"location":"planning/behavior_path_planner/#start-pull-out-condition-need-to-meet-all-of-the-conditions-below","text":"Pull out request condition The speed of the vehicle is 0. The footprint of ego-vehicle is included in shoulder lane The distance from ego-vehicle to the destination is long enough for pull out (same with pull over) The distance required for a pull out is determined by the sum of the straight distance before pull out, the straight distance after pull out, and the minimum distance required for pull out Pull out ready condition The end of generated path is located before the end of the road Distance required for pull out of generated path is shorter than distance from ego-pose to goal-pose Generated Path is safe (This condition is temporarily invalid) It is possible to enable or disable this condition. (default: disabled ) If safe path cannot be generated from the current position, safe path from a receding position is searched. Pull out is allowed by an operator","title":"start pull out condition (need to meet all of the conditions below)"},{"location":"planning/behavior_path_planner/#finish-pull-out-condition-need-to-meet-any-of-the-conditions-below","text":"The distance to the goal of your vehicle is lower than threshold (default: < 1m ) The speed of the vehicle is 0.","title":"finish pull out condition (need to meet any of the conditions below)"},{"location":"planning/behavior_path_planner/#safe-check-with-obstacles-in-shoulder-lane","text":"Calculate ego-vehicle's footprint on pull out path between from current position to merge end point. (Illustrated by blue frame) Calculate object's polygon which is located in shoulder lane If a distance between the footprint and the polygon is lower than the threshold (default: 1.5 m ), that is judged as a unsafe path","title":"Safe check with obstacles in shoulder lane"},{"location":"planning/behavior_path_planner/#path-generation_2","text":"The path is generated with a certain margin from the left edge of the shoulder lane.","title":"Path Generation"},{"location":"planning/behavior_path_planner/#unimplemented-parts-limitations-for-pull-put","text":"Note that the current module's start judgment condition may incorrectly judge that the vehicle is parked on the shoulder, such as when stopping in front of an intersection.","title":"Unimplemented parts / limitations for pull put"},{"location":"planning/behavior_path_planner/#side-shift","text":"The role of the Side Shift module is to shift the reference path laterally in response to external instructions (such as remote operation).","title":"Side Shift"},{"location":"planning/behavior_path_planner/#parameters-for-path-generation_1","text":"In the figure, straight margin distance is to avoid sudden shifting, that is calculated by max(min_distance_to_start_shifting, ego_speed * time_to_start_shifting) . The shifting distance is calculated by jerk, with minimum speed and minimum distance parameter, described below. The minimum speed is used to prevent sharp shift when ego vehicle is stopped. Name Unit Type Description Default value min_distance_to_start_shifting [m] double minimum straight distance before shift start. 5.0 time_to_start_shifting [s] double time of minimum straight distance before shift start. 1.0 shifting_lateral_jerk [m/s3] double lateral jerk to calculate shifting distance. 0.2 min_shifting_distance [m] double the shifting distance is longer than this length. 5.0 min_shifting_speed [m/s] double lateral jerk is calculated with the greater of current_speed or this speed. 5.56","title":"Parameters for path generation"},{"location":"planning/behavior_path_planner/#smooth-goal-connection","text":"If the target path contains a goal, modify the points of the path so that the path and the goal are connected smoothly. This process will change the shape of the path by the distance of refine_goal_search_radius_range from the goal. Note that this logic depends on the interpolation algorithm that will be executed in a later module (at the moment it uses spline interpolation), so it needs to be updated in the future.","title":"Smooth goal connection"},{"location":"planning/behavior_path_planner/#turn-signal","text":"Turn on signal when the planned path crosses lanes or when a right or left turn is required. The turn signal information includes the direction of the turn signal and the distance to the point where the turn signal is needed.","title":"Turn signal"},{"location":"planning/behavior_path_planner/#from-planned-path","text":"turn signal direction Calculate the lateral movement distance from the shift start and end point and judges whether the line is crossed or not. If the vehicle straddles the line during lateral movement, the system turns on the blinker. The timing to start lighting is when the time required to reach the shift start point is 3 seconds or less, or when the distance becomes smaller than threshold (default: 10.0 m ). The Japanese Road Traffic Law requires turn signal to be turned on 3.0 sec before the vehicle starts moving sideways, but it also gives a condition based on distance because the turn signal may be turned off by slowing down before the shift start point. distance The distance from the top of ego vehicle to the shift end point.","title":"From planned path"},{"location":"planning/behavior_path_planner/#from-map-information","text":"turn signal direction Determine right or left turns based on the \"turn_direction\" information embedded in the lanelet. distance The distance to the lane where you start to turn.","title":"From map information"},{"location":"planning/behavior_path_planner/#conciliation","text":"When multiple turn signal conditions are met, the turn signal with the smaller distance is selected.","title":"Conciliation"},{"location":"planning/behavior_path_planner/#references-external-links","text":"This module depends on the external BehaviorTreeCpp library.","title":"References / External links"},{"location":"planning/behavior_path_planner/#future-extensions-unimplemented-parts","text":"-","title":"Future extensions / Unimplemented parts"},{"location":"planning/behavior_path_planner/#related-issues","text":"-","title":"Related issues"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/","text":"Avoidance Module # This is a rule-based path planning module designed for obstacle avoidance. Purpose / Role # This module is designed for rule-based avoidance that is easy for developers to design its behavior. It generates avoidance path parameterized by intuitive parameters such as lateral jerk and avoidance distance margin. This makes it possible to pre-define avoidance behavior. In addition, the approval interface of behavior_path_planner allows external users / modules (e.g. remote operation) to intervene the decision of the vehicle behavior.\u3000 This function is expected to be used, for example, for remote intervention in emergency situations or gathering information on operator decisions during development. Limitations # This module allows developers to design vehicle behavior in avoidance planning using specific rules. Due to the property of rule-based planning, the algorithm can not compensate for not colliding with obstacles in complex cases. This is a trade-off between \"be intuitive and easy to design\" and \"be hard to tune but can handle many cases\". This module adopts the former policy and therefore this output should be checked more strictly in the later stage. In the .iv reference implementation, there is another avoidance module in motion planning module that uses optimization to handle the avoidance in complex cases. (Note that, the motion planner needs to be adjusted so that the behavior result will not be changed much in the simple case and this is a typical challenge for the behavior-motion hierarchical architecture.) Why is avoidance in behavior module? # This module executes avoidance over lanes, and the decision requires the lane structure information to take care of traffic rules (e.g. it needs to send an indicator signal when the vehicle crosses a lane). The difference between motion and behavior module in the planning stack is whether the planner takes traffic rules into account, which is why this avoidance module exists in the behavior module. Inner-workings / Algorithms # The following figure shows a simple explanation of the logic for avoidance path generation. First, target objects are picked up, and shift requests are generated for each object. These shift requests are generated by taking into account the lateral jerk required for avoidance (red lines). Then these requests are merged and the shift points are created on the reference path (blue line). Filtering operations are performed on the shift points such as removing unnecessary shift points (yellow line), and finally a smooth avoidance path is generated by combining Clothoid-like curve primitives (green line). Flowchart # Details of Functions # How to decide the target obstacles # The avoidance target should be limited to stationary objects (you should not avoid a vehicle waiting at a traffic light even if it blocks your path). Therefore, target vehicles for avoidance should meet the following specific conditions. It is in the vicinity of your lane (parametrized) It is stopped (estimated speed is lower than threshold) This means that overtaking is not supported (will be supported in the future). It is a specific class. User can limit avoidance targets (e.g. do not avoid unknown-class targets). It is not being in the center of the route This means that the vehicle is parked on the edge of the lane. This prevents the vehicle from avoiding a vehicle waiting at a traffic light in the middle of the lane. However, this is not an appropriate implementation for the purpose. Even if a vehicle is in the center of the lane, it should be avoided if it has its hazard lights on, and this is a point that should be improved in the future as the recognition performance improves. Compensation for detection lost # In order to prevent chattering of recognition results, once an obstacle is targeted, it is hold for a while even if it disappears. This is effective when recognition is unstable. However, since it will result in over-detection (increase a number of false-positive), it is necessary to adjust parameters according to the recognition accuracy (if object_hold_max_count = 0 , the recognition result is 100% trusted). How to decide the path shape # Generate shift points for obstacles with given lateral jerk. These points are integrated to generate an avoidance path. The detailed process flow for each case corresponding to the obstacle placement are described below. The actual implementation is not separated for each case, but the function corresponding to multiple obstacle case (both directions) is always running. One obstacle case # The lateral shift distance to the obstacle is calculated, and then the shift point is generated from the ego vehicle speed and the given lateral jerk as shown in the figure below. A smooth avoidance path is then calculated based on the shift point. Additionally, the following processes are executed in special cases. Lateral jerk relaxation conditions # If the ego vehicle is close to the avoidance target, the lateral jerk will be relaxed up to the maximum jerk When returning to the center line after avoidance, if there is not enough distance left to the goal (end of path), the jerk condition will be relaxed as above. Minimum velocity relaxation conditions # There is a problem that we can not know the actual speed during avoidance in advance. This is especially critical when the ego vehicle speed is 0. To solve that, this module provides a parameter for the minimum avoidance speed, which is used for the lateral jerk calculation when the vehicle speed is low. If the ego vehicle speed is lower than \"nominal\" minimum speed, use the minimum speed in the calculation of the jerk. If the ego vehicle speed is lower than \"sharp\" minimum speed and a nominal lateral jerk is not enough for avoidance (the case where the ego vehicle is stopped close to the obstacle), use the \"sharp\" minimum speed in the calculation of the jerk (it should be lower than \"nominal\" speed). Multiple obstacle case (one direction) # Generate shift points for multiple obstacles. All of them are merged to generate new shift points along the reference path. The new points are filtered (e.g. remove small-impact shift points), and the avoidance path is computed for the filtered shift points. Merge process of raw shift points : check the shift length on each path points. If the shift points are overlapped, the maximum shift value is selected for the same direction. For the details of the shift point filtering, see filtering for shift points . Multiple obstacle case (both direction) # Generate shift points for multiple obstacles. All of them are merged to generate new shift points. If there are areas where the desired shifts conflict in different directions, the sum of the maximum shift amounts of these areas is used as the final shift amount. The rest of the process is the same as in the case of one direction. Filtering for shift points # The shift points are modified by a filtering process in order to get the expected shape of the avoidance path. It contains the following filters. Quantization: Quantize the avoidance width in order to ignore small shifts. Small shifts removal: Shifts with small changes with respect to the previous shift point are unified in the previous shift width. Similar gradient removal: Connect two shift points with a straight line, and remove the shift points in between if their shift amount is in the vicinity of the straight line. Remove momentary returns: For shift points that reduce the avoidance width (for going back to the center line), if there is enough long distance in the longitudinal direction, remove them. Computing shift length (available as of develop/v0.25.0) # Note : This feature is available as of develop/ v0.25.0 . The shift length is set as a constant value before the feature is implemented (develop/ v0.24.0 and below). Setting the shift length like this will cause the module to generate an avoidance path regardless of actual environmental properties. For example, the path might exceed the actual road boundary or go towards a wall. Therefore, to address this limitation, in addition to how to decide the target obstacle , the upgraded module also takes into account the following additional element The obstacles' current lane and position. The road shoulder with reference to the direction to avoid. Note: Lane direction is disregarded. These elements are used to compute the distance from the object to the road's shoulder ( (class ObjectData.to_road_shoulder_distance) ). Computing shift length # To compute the shift length, in addition to the vehicle's width and the parameterized lateral_collision_margin , the upgraded feature also adds two new parameters; lateral_collision_safety_buffer and road_shoulder_safety_margin . The lateral_collision_safety_buffer parameter is used to set a safety gap that will act as the final line of defense when computing avoidance path. The rationale behind having this parameter is that the parameter lateral_collision_margin might be changed according to the situation for various reasons. Therefore, lateral_collision_safety_buffer will act as the final line of defense in case of the usage of lateral_collision_margin fails. It is recommended to set the value to more than half of the ego vehicle's width. The road_shoulder_safety_margin will prevent the module from generating a path that might cause the vehicle to go too near the road shoulder. The shift length is subjected to the following constraints. \\text{shift_length}=\\begin{cases}d_{lcsb}+d_{lcm}+\\frac{1}{2}(W_{ego})&\\text{if}&(d_{lcsb}+d_{lcm}+W_{ego}+d_{rssm})\\lt d_{trsd}\\\\0 &\\textrm{if}&\\left(d_{lcsb}+d_{lcm}+W_{ego}+d_{rssm}\\right)\\geq d_{trsd}\\end{cases} \\text{shift_length}=\\begin{cases}d_{lcsb}+d_{lcm}+\\frac{1}{2}(W_{ego})&\\text{if}&(d_{lcsb}+d_{lcm}+W_{ego}+d_{rssm})\\lt d_{trsd}\\\\0 &\\textrm{if}&\\left(d_{lcsb}+d_{lcm}+W_{ego}+d_{rssm}\\right)\\geq d_{trsd}\\end{cases} where = lateral_collision_safety_buffer = lateral_collision_margin = ego vehicle's width = road_shoulder_safety_margin = (class ObjectData).to_road_shoulder_distance How to keep the consistency of the planning # TODO Parameters # Avoidance path generation # Name Unit Type Description Default value resample_interval_for_planning [m] double Path resample interval for avoidance planning path. 0.3 resample_interval_for_output [m] double Path resample interval for output path. Too short interval increases computational cost for latter modules. 3.0 lateral_collision_margin [m] double The lateral distance between ego and avoidance targets. 1.5 lateral_collision_safety_buffer [m] double Creates an additional gap that will prevent the vehicle from getting to near to the obstacle 0.5 longitudinal_collision_margin_min_distance [m] double when complete avoidance motion, there is a distance margin with the object for longitudinal direction. 0.0 longitudinal_collision_margin_time [s] double when complete avoidance motion, there is a time margin with the object for longitudinal direction. 0.0 prepare_time [s] double Avoidance shift starts from point ahead of this time x ego_speed to avoid sudden path change. 1.0 min_prepare_distance [m] double Minimum distance for \"prepare_time\" x \"ego_speed\". 1.0 nominal_lateral_jerk [m/s3] double Avoidance path is generated with this jerk when there is enough distance from ego. 0.3 max_lateral_jerk [m/s3] double Avoidance path gets sharp up to this jerk limit when there is not enough distance from ego. 2.0 min_avoidance_distance [m] double Minimum distance of avoidance path (i.e. this distance is needed even if its lateral jerk is very low) 10.0 min_nominal_avoidance_speed [m/s] double Minimum speed for jerk calculation in a nominal situation (*1). 5.0 min_sharp_avoidance_speed [m/s] double Minimum speed for jerk calculation in a sharp situation (*1). 1.0 max_right_shift_length [m] double Maximum shift length for right direction 5.0 max_left_shift_length [m] double Maximum shift length for left direction 5.0 road_shoulder_safety_margin [m] double Prevents the generated path to come too close to the road shoulders. 0.5 avoidance_execution_lateral_threshold [m] double The lateral distance deviation threshold between the current path and suggested avoidance point to execute avoidance. (*2) 0.5 (*2) If there are multiple vehicles in a row to be avoided, no new avoidance path will be generated unless their lateral margin difference exceeds this value. Speed limit modification # Name Unit Type Description Default value min_avoidance_speed_for_acc_prevention [m] double Minimum speed limit to be applied to prevent acceleration during avoidance. 3.0 max_avoidance_acceleration [m/ss] double Maximum acceleration during avoidance. 0.5 Object selection # Name Unit Type Description Default value object_check_forward_distance [m] double Forward distance to search the avoidance target. 150.0 object_check_backward_distance [m] double Backward distance to search the avoidance target. 2.0 threshold_distance_object_is_on_center [m] double Vehicles around the center line within this distance will be excluded from avoidance target. 1.0 threshold_speed_object_is_stopped [m/s] double Vehicles with speed greater than this will be excluded from avoidance target. 1.0 detection_area_right_expand_dist [m] double Lanelet expand length for right side to find avoidance target vehicles. 0.0 detection_area_left_expand_dist [m] double Lanelet expand length for left side to find avoidance target vehicles. 1.0 object_hold_max_count [-] int For the compensation of the detection lost. The object is registered once it is observed as an avoidance target. When the detection loses, it counts up the lost_count and the object will be un-registered when the count exceeds this limit. 20 System # Name Unit Type Description Default value publish_debug_marker [-] double Flag to publish debug marker (set false as default since it takes considerable cost). false print_debug_info [-] double Flag to print debug info (set false as default since it takes considerable cost). false Future extensions / Unimplemented parts # Planning on the intersection If it is known that the ego vehicle is going to stop in the middle of avoidance execution (for example, at a red traffic light), sometimes the avoidance should not be executed until the vehicle is ready to move. This is because it is impossible to predict how the environment will change during the stop.\u3000 This is especially important at intersections. Safety Check In the current implementation, it is only the jerk limit that permits the avoidance execution. It is needed to consider the collision with other vehicles when change the path shape. Consideration of the speed of the avoidance target In the current implementation, only stopped vehicle is targeted as an avoidance target. It is needed to support the overtaking function for low-speed vehicles, such as a bicycle. (It is actually possible to overtake the low-speed objects by changing the parameter, but the logic is not supported and thus the safety cannot be guaranteed.) The overtaking (e.g., to overtake a vehicle running in front at 20 km/h at 40 km/h) may need to be handled outside the avoidance module. It should be discussed which module should handle it. Cancel avoidance when target disappears In the current implementation, even if the avoidance target disappears, the avoidance path will remain. If there is no longer a need to avoid, it must be canceled. Improved performance of avoidance target selection Essentially, avoidance targets are judged based on whether they are static objects or not. For example, a vehicle waiting at a traffic light should not be avoided because we know that it will start moving in the future. However this decision cannot be made in the current Autoware due to the lack of the perception functions. Therefore, the current avoidance module limits the avoidance target to vehicles parked on the shoulder of the road, and executes avoidance only for vehicles that are stopped away from the center of the lane. However, this logic cannot avoid a vehicle that has broken down and is stopped in the center of the lane, which should be recognized as a static object by the perception module. There is room for improvement in the performance of this decision. Resampling path Now the rough resolution resampling is processed to the output path in order to reduce the computational cost for the later modules. This resolution is set to a uniformly large value \u3000(e.g. 5m ), but small resolution should be applied for complex paths. How to debug # The behavior_path_planner will publish debug marker when publish_debug_marker parameter is true . It will visualize all outputs of the each avoidance planning process such as target vehicles, shift points for each object, shift points after each filtering process, etc., so that developer can see what is going on in the each process one by one.","title":"Avoidance Module"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#avoidance-module","text":"This is a rule-based path planning module designed for obstacle avoidance.","title":"Avoidance Module"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#purpose-role","text":"This module is designed for rule-based avoidance that is easy for developers to design its behavior. It generates avoidance path parameterized by intuitive parameters such as lateral jerk and avoidance distance margin. This makes it possible to pre-define avoidance behavior. In addition, the approval interface of behavior_path_planner allows external users / modules (e.g. remote operation) to intervene the decision of the vehicle behavior.\u3000 This function is expected to be used, for example, for remote intervention in emergency situations or gathering information on operator decisions during development.","title":"Purpose / Role"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#limitations","text":"This module allows developers to design vehicle behavior in avoidance planning using specific rules. Due to the property of rule-based planning, the algorithm can not compensate for not colliding with obstacles in complex cases. This is a trade-off between \"be intuitive and easy to design\" and \"be hard to tune but can handle many cases\". This module adopts the former policy and therefore this output should be checked more strictly in the later stage. In the .iv reference implementation, there is another avoidance module in motion planning module that uses optimization to handle the avoidance in complex cases. (Note that, the motion planner needs to be adjusted so that the behavior result will not be changed much in the simple case and this is a typical challenge for the behavior-motion hierarchical architecture.)","title":"Limitations"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#why-is-avoidance-in-behavior-module","text":"This module executes avoidance over lanes, and the decision requires the lane structure information to take care of traffic rules (e.g. it needs to send an indicator signal when the vehicle crosses a lane). The difference between motion and behavior module in the planning stack is whether the planner takes traffic rules into account, which is why this avoidance module exists in the behavior module.","title":"Why is avoidance in behavior module?"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#inner-workings-algorithms","text":"The following figure shows a simple explanation of the logic for avoidance path generation. First, target objects are picked up, and shift requests are generated for each object. These shift requests are generated by taking into account the lateral jerk required for avoidance (red lines). Then these requests are merged and the shift points are created on the reference path (blue line). Filtering operations are performed on the shift points such as removing unnecessary shift points (yellow line), and finally a smooth avoidance path is generated by combining Clothoid-like curve primitives (green line).","title":"Inner-workings / Algorithms"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#flowchart","text":"","title":"Flowchart"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#details-of-functions","text":"","title":"Details of Functions"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#how-to-decide-the-target-obstacles","text":"The avoidance target should be limited to stationary objects (you should not avoid a vehicle waiting at a traffic light even if it blocks your path). Therefore, target vehicles for avoidance should meet the following specific conditions. It is in the vicinity of your lane (parametrized) It is stopped (estimated speed is lower than threshold) This means that overtaking is not supported (will be supported in the future). It is a specific class. User can limit avoidance targets (e.g. do not avoid unknown-class targets). It is not being in the center of the route This means that the vehicle is parked on the edge of the lane. This prevents the vehicle from avoiding a vehicle waiting at a traffic light in the middle of the lane. However, this is not an appropriate implementation for the purpose. Even if a vehicle is in the center of the lane, it should be avoided if it has its hazard lights on, and this is a point that should be improved in the future as the recognition performance improves.","title":"How to decide the target obstacles"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#compensation-for-detection-lost","text":"In order to prevent chattering of recognition results, once an obstacle is targeted, it is hold for a while even if it disappears. This is effective when recognition is unstable. However, since it will result in over-detection (increase a number of false-positive), it is necessary to adjust parameters according to the recognition accuracy (if object_hold_max_count = 0 , the recognition result is 100% trusted).","title":"Compensation for detection lost"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#how-to-decide-the-path-shape","text":"Generate shift points for obstacles with given lateral jerk. These points are integrated to generate an avoidance path. The detailed process flow for each case corresponding to the obstacle placement are described below. The actual implementation is not separated for each case, but the function corresponding to multiple obstacle case (both directions) is always running.","title":"How to decide the path shape"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#one-obstacle-case","text":"The lateral shift distance to the obstacle is calculated, and then the shift point is generated from the ego vehicle speed and the given lateral jerk as shown in the figure below. A smooth avoidance path is then calculated based on the shift point. Additionally, the following processes are executed in special cases.","title":"One obstacle case"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#lateral-jerk-relaxation-conditions","text":"If the ego vehicle is close to the avoidance target, the lateral jerk will be relaxed up to the maximum jerk When returning to the center line after avoidance, if there is not enough distance left to the goal (end of path), the jerk condition will be relaxed as above.","title":"Lateral jerk relaxation conditions"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#minimum-velocity-relaxation-conditions","text":"There is a problem that we can not know the actual speed during avoidance in advance. This is especially critical when the ego vehicle speed is 0. To solve that, this module provides a parameter for the minimum avoidance speed, which is used for the lateral jerk calculation when the vehicle speed is low. If the ego vehicle speed is lower than \"nominal\" minimum speed, use the minimum speed in the calculation of the jerk. If the ego vehicle speed is lower than \"sharp\" minimum speed and a nominal lateral jerk is not enough for avoidance (the case where the ego vehicle is stopped close to the obstacle), use the \"sharp\" minimum speed in the calculation of the jerk (it should be lower than \"nominal\" speed).","title":"Minimum velocity relaxation conditions"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#multiple-obstacle-case-one-direction","text":"Generate shift points for multiple obstacles. All of them are merged to generate new shift points along the reference path. The new points are filtered (e.g. remove small-impact shift points), and the avoidance path is computed for the filtered shift points. Merge process of raw shift points : check the shift length on each path points. If the shift points are overlapped, the maximum shift value is selected for the same direction. For the details of the shift point filtering, see filtering for shift points .","title":"Multiple obstacle case (one direction)"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#multiple-obstacle-case-both-direction","text":"Generate shift points for multiple obstacles. All of them are merged to generate new shift points. If there are areas where the desired shifts conflict in different directions, the sum of the maximum shift amounts of these areas is used as the final shift amount. The rest of the process is the same as in the case of one direction.","title":"Multiple obstacle case (both direction)"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#filtering-for-shift-points","text":"The shift points are modified by a filtering process in order to get the expected shape of the avoidance path. It contains the following filters. Quantization: Quantize the avoidance width in order to ignore small shifts. Small shifts removal: Shifts with small changes with respect to the previous shift point are unified in the previous shift width. Similar gradient removal: Connect two shift points with a straight line, and remove the shift points in between if their shift amount is in the vicinity of the straight line. Remove momentary returns: For shift points that reduce the avoidance width (for going back to the center line), if there is enough long distance in the longitudinal direction, remove them.","title":"Filtering for shift points"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#computing-shift-length-available-as-of-developv0250","text":"Note : This feature is available as of develop/ v0.25.0 . The shift length is set as a constant value before the feature is implemented (develop/ v0.24.0 and below). Setting the shift length like this will cause the module to generate an avoidance path regardless of actual environmental properties. For example, the path might exceed the actual road boundary or go towards a wall. Therefore, to address this limitation, in addition to how to decide the target obstacle , the upgraded module also takes into account the following additional element The obstacles' current lane and position. The road shoulder with reference to the direction to avoid. Note: Lane direction is disregarded. These elements are used to compute the distance from the object to the road's shoulder ( (class ObjectData.to_road_shoulder_distance) ).","title":"Computing shift length (available as of develop/v0.25.0)"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#computing-shift-length","text":"To compute the shift length, in addition to the vehicle's width and the parameterized lateral_collision_margin , the upgraded feature also adds two new parameters; lateral_collision_safety_buffer and road_shoulder_safety_margin . The lateral_collision_safety_buffer parameter is used to set a safety gap that will act as the final line of defense when computing avoidance path. The rationale behind having this parameter is that the parameter lateral_collision_margin might be changed according to the situation for various reasons. Therefore, lateral_collision_safety_buffer will act as the final line of defense in case of the usage of lateral_collision_margin fails. It is recommended to set the value to more than half of the ego vehicle's width. The road_shoulder_safety_margin will prevent the module from generating a path that might cause the vehicle to go too near the road shoulder. The shift length is subjected to the following constraints. \\text{shift_length}=\\begin{cases}d_{lcsb}+d_{lcm}+\\frac{1}{2}(W_{ego})&\\text{if}&(d_{lcsb}+d_{lcm}+W_{ego}+d_{rssm})\\lt d_{trsd}\\\\0 &\\textrm{if}&\\left(d_{lcsb}+d_{lcm}+W_{ego}+d_{rssm}\\right)\\geq d_{trsd}\\end{cases} \\text{shift_length}=\\begin{cases}d_{lcsb}+d_{lcm}+\\frac{1}{2}(W_{ego})&\\text{if}&(d_{lcsb}+d_{lcm}+W_{ego}+d_{rssm})\\lt d_{trsd}\\\\0 &\\textrm{if}&\\left(d_{lcsb}+d_{lcm}+W_{ego}+d_{rssm}\\right)\\geq d_{trsd}\\end{cases} where = lateral_collision_safety_buffer = lateral_collision_margin = ego vehicle's width = road_shoulder_safety_margin = (class ObjectData).to_road_shoulder_distance","title":"Computing shift length"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#how-to-keep-the-consistency-of-the-planning","text":"TODO","title":"How to keep the consistency of the planning"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#parameters","text":"","title":"Parameters"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#avoidance-path-generation","text":"Name Unit Type Description Default value resample_interval_for_planning [m] double Path resample interval for avoidance planning path. 0.3 resample_interval_for_output [m] double Path resample interval for output path. Too short interval increases computational cost for latter modules. 3.0 lateral_collision_margin [m] double The lateral distance between ego and avoidance targets. 1.5 lateral_collision_safety_buffer [m] double Creates an additional gap that will prevent the vehicle from getting to near to the obstacle 0.5 longitudinal_collision_margin_min_distance [m] double when complete avoidance motion, there is a distance margin with the object for longitudinal direction. 0.0 longitudinal_collision_margin_time [s] double when complete avoidance motion, there is a time margin with the object for longitudinal direction. 0.0 prepare_time [s] double Avoidance shift starts from point ahead of this time x ego_speed to avoid sudden path change. 1.0 min_prepare_distance [m] double Minimum distance for \"prepare_time\" x \"ego_speed\". 1.0 nominal_lateral_jerk [m/s3] double Avoidance path is generated with this jerk when there is enough distance from ego. 0.3 max_lateral_jerk [m/s3] double Avoidance path gets sharp up to this jerk limit when there is not enough distance from ego. 2.0 min_avoidance_distance [m] double Minimum distance of avoidance path (i.e. this distance is needed even if its lateral jerk is very low) 10.0 min_nominal_avoidance_speed [m/s] double Minimum speed for jerk calculation in a nominal situation (*1). 5.0 min_sharp_avoidance_speed [m/s] double Minimum speed for jerk calculation in a sharp situation (*1). 1.0 max_right_shift_length [m] double Maximum shift length for right direction 5.0 max_left_shift_length [m] double Maximum shift length for left direction 5.0 road_shoulder_safety_margin [m] double Prevents the generated path to come too close to the road shoulders. 0.5 avoidance_execution_lateral_threshold [m] double The lateral distance deviation threshold between the current path and suggested avoidance point to execute avoidance. (*2) 0.5 (*2) If there are multiple vehicles in a row to be avoided, no new avoidance path will be generated unless their lateral margin difference exceeds this value.","title":"Avoidance path generation"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#speed-limit-modification","text":"Name Unit Type Description Default value min_avoidance_speed_for_acc_prevention [m] double Minimum speed limit to be applied to prevent acceleration during avoidance. 3.0 max_avoidance_acceleration [m/ss] double Maximum acceleration during avoidance. 0.5","title":"Speed limit modification"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#object-selection","text":"Name Unit Type Description Default value object_check_forward_distance [m] double Forward distance to search the avoidance target. 150.0 object_check_backward_distance [m] double Backward distance to search the avoidance target. 2.0 threshold_distance_object_is_on_center [m] double Vehicles around the center line within this distance will be excluded from avoidance target. 1.0 threshold_speed_object_is_stopped [m/s] double Vehicles with speed greater than this will be excluded from avoidance target. 1.0 detection_area_right_expand_dist [m] double Lanelet expand length for right side to find avoidance target vehicles. 0.0 detection_area_left_expand_dist [m] double Lanelet expand length for left side to find avoidance target vehicles. 1.0 object_hold_max_count [-] int For the compensation of the detection lost. The object is registered once it is observed as an avoidance target. When the detection loses, it counts up the lost_count and the object will be un-registered when the count exceeds this limit. 20","title":"Object selection"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#system","text":"Name Unit Type Description Default value publish_debug_marker [-] double Flag to publish debug marker (set false as default since it takes considerable cost). false print_debug_info [-] double Flag to print debug info (set false as default since it takes considerable cost). false","title":"System"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#future-extensions-unimplemented-parts","text":"Planning on the intersection If it is known that the ego vehicle is going to stop in the middle of avoidance execution (for example, at a red traffic light), sometimes the avoidance should not be executed until the vehicle is ready to move. This is because it is impossible to predict how the environment will change during the stop.\u3000 This is especially important at intersections. Safety Check In the current implementation, it is only the jerk limit that permits the avoidance execution. It is needed to consider the collision with other vehicles when change the path shape. Consideration of the speed of the avoidance target In the current implementation, only stopped vehicle is targeted as an avoidance target. It is needed to support the overtaking function for low-speed vehicles, such as a bicycle. (It is actually possible to overtake the low-speed objects by changing the parameter, but the logic is not supported and thus the safety cannot be guaranteed.) The overtaking (e.g., to overtake a vehicle running in front at 20 km/h at 40 km/h) may need to be handled outside the avoidance module. It should be discussed which module should handle it. Cancel avoidance when target disappears In the current implementation, even if the avoidance target disappears, the avoidance path will remain. If there is no longer a need to avoid, it must be canceled. Improved performance of avoidance target selection Essentially, avoidance targets are judged based on whether they are static objects or not. For example, a vehicle waiting at a traffic light should not be avoided because we know that it will start moving in the future. However this decision cannot be made in the current Autoware due to the lack of the perception functions. Therefore, the current avoidance module limits the avoidance target to vehicles parked on the shoulder of the road, and executes avoidance only for vehicles that are stopped away from the center of the lane. However, this logic cannot avoid a vehicle that has broken down and is stopped in the center of the lane, which should be recognized as a static object by the perception module. There is room for improvement in the performance of this decision. Resampling path Now the rough resolution resampling is processed to the output path in order to reduce the computational cost for the later modules. This resolution is set to a uniformly large value \u3000(e.g. 5m ), but small resolution should be applied for complex paths.","title":"Future extensions / Unimplemented parts"},{"location":"planning/behavior_path_planner/behavior_path_planner_avoidance-design/#how-to-debug","text":"The behavior_path_planner will publish debug marker when publish_debug_marker parameter is true . It will visualize all outputs of the each avoidance planning process such as target vehicles, shift points for each object, shift points after each filtering process, etc., so that developer can see what is going on in the each process one by one.","title":"How to debug"},{"location":"planning/behavior_path_planner/behavior_path_planner_design_limitations/","text":"Design limitation # The document describes the limitations that are currently present in the behavior_path_planner module. The following items (but not limited to) fall in the scope of limitation: limitations due to the third-party API design and requirement limitations due to any shortcoming out of the developer's control. Limitation: Multiple connected opposite lanes require Linestring with shared ID # To fully utilize the Lanelet2 's API , the design of the vector map ( .osm ) needs to follow all the criteria described in Lanelet2 documentation. Specifically, in the case of 2 or more lanes, the Linestrings that divide the current lane with the opposite/adjacent lane need to have a matching Linestring ID . Assume the following ideal case . In the image, Linestring ID51 is shared by Lanelet A and Lanelet B . Hence we can directly use the available left , adjacentLeft , right , adjacentRight and findUsages method within Lanelet2 's API to directly query the direction and opposite lane availability. const auto right_lane = routing_graph_ptr_ -> right ( lanelet ); const auto adjacent_right_lane = routing_graph_ptr_ -> adjacentRight ( lanelet ); const auto opposite_right_lane = lanelet_map_ptr_ -> laneletLayer . findUsages ( lanelet . rightBound (). invert ()); The following images show the situation where these API does not work directly . This means that we cannot use them straight away, and several assumptions and logical instruction are needed to make these APIs work. In this example (multiple linestring issues), Lanelet C contains Linestring ID61 and ID62 , while Lanelet D contains Linestring ID63 and ID 64 . Although the Linestring ID62 and ID64 have identical point IDs and seem visually connected, the API will treat these Linestring as though they are separated. When it searches for any Lanelet that is connected via Linestring ID62 , it will return NULL , since ID62 only connects to Lanelet C and not other Lanelet . Although, in this case, it is possible to forcefully search the lanelet availability by checking the lanelet that contains the points, using getLaneletFromPoint method. But, the implementation requires complex rules for it to work. Take the following images as an example. Assume Object X is in Lanelet F . We can forcefully search Lanelet E via Point 7 , and it will work if Point 7 is utilized by only 2 lanelet . However, the complexity increases when we want to start searching for the direction of the opposite lane. We can infer the direction of the lanelet by using mathematical operations (dot product of vector V_ID72 ( Point 6 minus Point 9 ), and V_ID74 ( Point 7 minus Point 8 ). But, notice that we did not use Point 7 in V_ID72. This is because searching it requires an iteration, adding additional non-beneficial computation. Suppose the points are used by more than 2 lanelets . In that case, we have to find the differences for all lanelet, and the result might be undefined. The reason is that the differences between the coordinates do not reflect the actual shape of the lanelet. The following image demonstrates this point. There are many other available solutions to try. However, further attempt to solve this might cause issues in the future, especially for maintaining or scaling up the software. In conclusion, the multiple Linestring issues will not be supported. Covering these scenarios might give the user an \"everything is possible\" impression. This is dangerous since any attempt to create a non-standardized vector map is not compliant with safety regulations. Limitation: Avoidance at Corners and Intersections # Currently, the implementation doesn't cover avoidance at corners and intersections. The reason is similar to here. However, this case can still be supported in the future (assuming the vector map is defined correctly).","title":"Design limitation"},{"location":"planning/behavior_path_planner/behavior_path_planner_design_limitations/#design-limitation","text":"The document describes the limitations that are currently present in the behavior_path_planner module. The following items (but not limited to) fall in the scope of limitation: limitations due to the third-party API design and requirement limitations due to any shortcoming out of the developer's control.","title":"Design limitation"},{"location":"planning/behavior_path_planner/behavior_path_planner_design_limitations/#limitation-multiple-connected-opposite-lanes-require-linestring-with-shared-id","text":"To fully utilize the Lanelet2 's API , the design of the vector map ( .osm ) needs to follow all the criteria described in Lanelet2 documentation. Specifically, in the case of 2 or more lanes, the Linestrings that divide the current lane with the opposite/adjacent lane need to have a matching Linestring ID . Assume the following ideal case . In the image, Linestring ID51 is shared by Lanelet A and Lanelet B . Hence we can directly use the available left , adjacentLeft , right , adjacentRight and findUsages method within Lanelet2 's API to directly query the direction and opposite lane availability. const auto right_lane = routing_graph_ptr_ -> right ( lanelet ); const auto adjacent_right_lane = routing_graph_ptr_ -> adjacentRight ( lanelet ); const auto opposite_right_lane = lanelet_map_ptr_ -> laneletLayer . findUsages ( lanelet . rightBound (). invert ()); The following images show the situation where these API does not work directly . This means that we cannot use them straight away, and several assumptions and logical instruction are needed to make these APIs work. In this example (multiple linestring issues), Lanelet C contains Linestring ID61 and ID62 , while Lanelet D contains Linestring ID63 and ID 64 . Although the Linestring ID62 and ID64 have identical point IDs and seem visually connected, the API will treat these Linestring as though they are separated. When it searches for any Lanelet that is connected via Linestring ID62 , it will return NULL , since ID62 only connects to Lanelet C and not other Lanelet . Although, in this case, it is possible to forcefully search the lanelet availability by checking the lanelet that contains the points, using getLaneletFromPoint method. But, the implementation requires complex rules for it to work. Take the following images as an example. Assume Object X is in Lanelet F . We can forcefully search Lanelet E via Point 7 , and it will work if Point 7 is utilized by only 2 lanelet . However, the complexity increases when we want to start searching for the direction of the opposite lane. We can infer the direction of the lanelet by using mathematical operations (dot product of vector V_ID72 ( Point 6 minus Point 9 ), and V_ID74 ( Point 7 minus Point 8 ). But, notice that we did not use Point 7 in V_ID72. This is because searching it requires an iteration, adding additional non-beneficial computation. Suppose the points are used by more than 2 lanelets . In that case, we have to find the differences for all lanelet, and the result might be undefined. The reason is that the differences between the coordinates do not reflect the actual shape of the lanelet. The following image demonstrates this point. There are many other available solutions to try. However, further attempt to solve this might cause issues in the future, especially for maintaining or scaling up the software. In conclusion, the multiple Linestring issues will not be supported. Covering these scenarios might give the user an \"everything is possible\" impression. This is dangerous since any attempt to create a non-standardized vector map is not compliant with safety regulations.","title":"Limitation: Multiple connected opposite lanes require Linestring with shared ID"},{"location":"planning/behavior_path_planner/behavior_path_planner_design_limitations/#limitation-avoidance-at-corners-and-intersections","text":"Currently, the implementation doesn't cover avoidance at corners and intersections. The reason is similar to here. However, this case can still be supported in the future (assuming the vector map is defined correctly).","title":"Limitation: Avoidance at Corners and Intersections"},{"location":"planning/behavior_velocity_planner/","text":"Behavior Velocity Planner # Overview # behavior_velocity_planner is a planner that adjust velocity based on the traffic rules. It consists of several modules. Please refer to the links listed below for detail on each module. Blind Spot Crosswalk Detection Area Intersection MergeFromPrivate Stop Line Virtual Traffic Light Traffic Light Occlusion Spot No Stopping Area Run Out When each module plans velocity, it considers based on base_link (center of rear-wheel axis) pose. So for example, in order to stop at a stop line with the vehicles' front on the stop line, it calculates base_link position from the distance between base_link to front and modifies path velocity from the base_link position. Input topics # Name Type Description ~input/path_with_lane_id autoware_auto_planning_msgs::msg::PathWithLaneId path with lane_id ~input/vector_map autoware_auto_mapping_msgs::msg::HADMapBin vector map ~input/vehicle_odometry nav_msgs::msg::Odometry vehicle velocity ~input/dynamic_objects autoware_auto_perception_msgs::msg::PredictedObjects dynamic objects ~input/no_ground_pointcloud sensor_msgs::msg::PointCloud2 obstacle pointcloud ~/input/compare_map_filtered_pointcloud sensor_msgs::msg::PointCloud2 obstacle pointcloud filtered by compare map. Note that this is used only when the detection method of run out module is Points. ~input/traffic_signals autoware_auto_perception_msgs::msg::TrafficSignalArray traffic light states Output topics # Name Type Description ~output/path autoware_auto_planning_msgs::msg::Path path to be followed ~output/stop_reasons tier4_planning_msgs::msg::StopReasonArray reasons that cause the vehicle to stop Node parameters # Parameter Type Description launch_blind_spot bool whether to launch blind_spot module launch_crosswalk bool whether to launch crosswalk module launch_detection_area bool whether to launch detection_area module launch_intersection bool whether to launch intersection module launch_traffic_light bool whether to launch traffic light module launch_stop_line bool whether to launch stop_line module launch_occlusion_spot bool whether to launch occlusion_spot module launch_run_out bool whether to launch run_out module forward_path_length double forward path length backward_path_length double backward path length max_accel double (to be a global parameter) max acceleration of the vehicle system_delay double (to be a global parameter) delay time until output control command delay_response_time double (to be a global parameter) delay time of the vehicle's response to control commands","title":"Behavior Velocity Planner"},{"location":"planning/behavior_velocity_planner/#behavior-velocity-planner","text":"","title":"Behavior Velocity Planner"},{"location":"planning/behavior_velocity_planner/#overview","text":"behavior_velocity_planner is a planner that adjust velocity based on the traffic rules. It consists of several modules. Please refer to the links listed below for detail on each module. Blind Spot Crosswalk Detection Area Intersection MergeFromPrivate Stop Line Virtual Traffic Light Traffic Light Occlusion Spot No Stopping Area Run Out When each module plans velocity, it considers based on base_link (center of rear-wheel axis) pose. So for example, in order to stop at a stop line with the vehicles' front on the stop line, it calculates base_link position from the distance between base_link to front and modifies path velocity from the base_link position.","title":"Overview"},{"location":"planning/behavior_velocity_planner/#input-topics","text":"Name Type Description ~input/path_with_lane_id autoware_auto_planning_msgs::msg::PathWithLaneId path with lane_id ~input/vector_map autoware_auto_mapping_msgs::msg::HADMapBin vector map ~input/vehicle_odometry nav_msgs::msg::Odometry vehicle velocity ~input/dynamic_objects autoware_auto_perception_msgs::msg::PredictedObjects dynamic objects ~input/no_ground_pointcloud sensor_msgs::msg::PointCloud2 obstacle pointcloud ~/input/compare_map_filtered_pointcloud sensor_msgs::msg::PointCloud2 obstacle pointcloud filtered by compare map. Note that this is used only when the detection method of run out module is Points. ~input/traffic_signals autoware_auto_perception_msgs::msg::TrafficSignalArray traffic light states","title":"Input topics"},{"location":"planning/behavior_velocity_planner/#output-topics","text":"Name Type Description ~output/path autoware_auto_planning_msgs::msg::Path path to be followed ~output/stop_reasons tier4_planning_msgs::msg::StopReasonArray reasons that cause the vehicle to stop","title":"Output topics"},{"location":"planning/behavior_velocity_planner/#node-parameters","text":"Parameter Type Description launch_blind_spot bool whether to launch blind_spot module launch_crosswalk bool whether to launch crosswalk module launch_detection_area bool whether to launch detection_area module launch_intersection bool whether to launch intersection module launch_traffic_light bool whether to launch traffic light module launch_stop_line bool whether to launch stop_line module launch_occlusion_spot bool whether to launch occlusion_spot module launch_run_out bool whether to launch run_out module forward_path_length double forward path length backward_path_length double backward path length max_accel double (to be a global parameter) max acceleration of the vehicle system_delay double (to be a global parameter) delay time until output control command delay_response_time double (to be a global parameter) delay time of the vehicle's response to control commands","title":"Node parameters"},{"location":"planning/behavior_velocity_planner/blind-spot-design/","text":"Blind Spot # Role # Blind spot check while turning right/left by a dynamic object information and planning a velocity of the start/stop. Activation Timing # This function is activated when the lane id of the target path has an intersection label (i.e. the turn_direction attribute is left or right ). Inner-workings / Algorithms # Sets a stop line, a pass judge line, a detection area and conflict area based on a map information and a self position. Stop line : Automatically created based on crossing lane information. Pass judge line : A position to judge if stop or not to avoid a rapid brake. Detection area : Right/left side area of the self position. Conflict area : Right/left side area from the self position to the stop line. Stop/Go state: When both conditions are met for any of each object, this module state is transited to the \"stop\" state and insert zero velocity to stop the vehicle. Object is on the detection area Object\u2019s predicted path is on the conflict area In order to avoid a rapid stop, the \u201cstop\u201d judgement is not executed after the judgment line is passed. Once a \"stop\" is judged, it will not transit to the \"go\" state until the \"go\" judgment continues for a certain period in order to prevent chattering of the state (e.g. 2 seconds). Module Parameters # Parameter Type Description stop_line_margin double [m] a margin that the vehicle tries to stop before stop_line backward_length double [m] distance from closest path point to the edge of beginning point. ignore_width_from_center_line double [m] ignore threshold that vehicle behind is collide with ego vehicle or not max_future_movement_time double [s] maximum time for considering future movement of object Flowchart #","title":"Blind spot design"},{"location":"planning/behavior_velocity_planner/blind-spot-design/#blind-spot","text":"","title":"Blind Spot"},{"location":"planning/behavior_velocity_planner/blind-spot-design/#role","text":"Blind spot check while turning right/left by a dynamic object information and planning a velocity of the start/stop.","title":"Role"},{"location":"planning/behavior_velocity_planner/blind-spot-design/#activation-timing","text":"This function is activated when the lane id of the target path has an intersection label (i.e. the turn_direction attribute is left or right ).","title":"Activation Timing"},{"location":"planning/behavior_velocity_planner/blind-spot-design/#inner-workings-algorithms","text":"Sets a stop line, a pass judge line, a detection area and conflict area based on a map information and a self position. Stop line : Automatically created based on crossing lane information. Pass judge line : A position to judge if stop or not to avoid a rapid brake. Detection area : Right/left side area of the self position. Conflict area : Right/left side area from the self position to the stop line. Stop/Go state: When both conditions are met for any of each object, this module state is transited to the \"stop\" state and insert zero velocity to stop the vehicle. Object is on the detection area Object\u2019s predicted path is on the conflict area In order to avoid a rapid stop, the \u201cstop\u201d judgement is not executed after the judgment line is passed. Once a \"stop\" is judged, it will not transit to the \"go\" state until the \"go\" judgment continues for a certain period in order to prevent chattering of the state (e.g. 2 seconds).","title":"Inner-workings / Algorithms"},{"location":"planning/behavior_velocity_planner/blind-spot-design/#module-parameters","text":"Parameter Type Description stop_line_margin double [m] a margin that the vehicle tries to stop before stop_line backward_length double [m] distance from closest path point to the edge of beginning point. ignore_width_from_center_line double [m] ignore threshold that vehicle behind is collide with ego vehicle or not max_future_movement_time double [s] maximum time for considering future movement of object","title":"Module Parameters"},{"location":"planning/behavior_velocity_planner/blind-spot-design/#flowchart","text":"","title":"Flowchart"},{"location":"planning/behavior_velocity_planner/crosswalk-design/","text":"CrossWalk # Role For Crosswalk # Judgement whether a vehicle can go into a crosswalk and plan a velocity of the start/stop. Role For Walkway # tbd. Activation Timing # Launches when there is a crosswalk on the target lane. Limitations # For Crosswalk # In order to prevent perception failure, the detection area is limited by using pre-defined areas (deceleration area and stop area). Therefore this module does not respond to pedestrians or bicycles outside these areas. For Walkway # tbd. Inner-workings / Algorithms # Scene Crosswalk # The crosswalk module considers the following objects as target objects. pedestrian cyclist Stop condition # If any of conditions below is met, the vehicle will stop at stop point. A target object exists in the stop area . A target object in the crosswalk area is predicted to enter the stop area within 3 seconds based on its prediction path. Decelerate condition # If any of conditions below is met, the vehicle will decelerate to be 10 km/h at slow point. A target object exists in the deceleration area . Scene Walkway # TBD Module Parameters # Parameter Type Description crosswalk/stop_line_distance double [m] make stop line away from crosswalk when no explicit stop line exists crosswalk/stop_margin double [m] a margin that the vehicle tries to stop before stop_line crosswalk/slow_margin bool [m] a margin that the vehicle tries to slow down before stop_line crosswalk/slow_velocity double [m] a slow down velocity crosswalk/stop_predicted_object_prediction_time_margin double [s] time margin for decision of ego vehicle to stop or not walkway/stop_line_distance double [m] make stop line away from crosswalk when no explicit stop line exists walkway/stop_margin double [m] a margin that the vehicle tries to stop before walkway walkway/stop_duration_sec double [s] time margin for decision of ego vehicle to stop Flowchart # flow chart is almost the same as stop line. Known Issues # Crosswalk # The logic for determining speed should be set more strictly from safety reasons. The deceleration speed from the deceleration area logic is set to a constant value (10 [km/h]), which does not take into account the safety distance from obstacles. Walkway # If the vehicle exceeds the stop line more than the threshold distance, this module will get stuck in STOP state and will not start moving.","title":"Crosswalk design"},{"location":"planning/behavior_velocity_planner/crosswalk-design/#crosswalk","text":"","title":"CrossWalk"},{"location":"planning/behavior_velocity_planner/crosswalk-design/#role-for-crosswalk","text":"Judgement whether a vehicle can go into a crosswalk and plan a velocity of the start/stop.","title":"Role For Crosswalk"},{"location":"planning/behavior_velocity_planner/crosswalk-design/#role-for-walkway","text":"tbd.","title":"Role For Walkway"},{"location":"planning/behavior_velocity_planner/crosswalk-design/#activation-timing","text":"Launches when there is a crosswalk on the target lane.","title":"Activation Timing"},{"location":"planning/behavior_velocity_planner/crosswalk-design/#limitations","text":"","title":"Limitations"},{"location":"planning/behavior_velocity_planner/crosswalk-design/#for-crosswalk","text":"In order to prevent perception failure, the detection area is limited by using pre-defined areas (deceleration area and stop area). Therefore this module does not respond to pedestrians or bicycles outside these areas.","title":"For Crosswalk"},{"location":"planning/behavior_velocity_planner/crosswalk-design/#for-walkway","text":"tbd.","title":"For Walkway"},{"location":"planning/behavior_velocity_planner/crosswalk-design/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"planning/behavior_velocity_planner/crosswalk-design/#scene-crosswalk","text":"The crosswalk module considers the following objects as target objects. pedestrian cyclist","title":"Scene Crosswalk"},{"location":"planning/behavior_velocity_planner/crosswalk-design/#stop-condition","text":"If any of conditions below is met, the vehicle will stop at stop point. A target object exists in the stop area . A target object in the crosswalk area is predicted to enter the stop area within 3 seconds based on its prediction path.","title":"Stop condition"},{"location":"planning/behavior_velocity_planner/crosswalk-design/#decelerate-condition","text":"If any of conditions below is met, the vehicle will decelerate to be 10 km/h at slow point. A target object exists in the deceleration area .","title":"Decelerate condition"},{"location":"planning/behavior_velocity_planner/crosswalk-design/#scene-walkway","text":"TBD","title":"Scene Walkway"},{"location":"planning/behavior_velocity_planner/crosswalk-design/#module-parameters","text":"Parameter Type Description crosswalk/stop_line_distance double [m] make stop line away from crosswalk when no explicit stop line exists crosswalk/stop_margin double [m] a margin that the vehicle tries to stop before stop_line crosswalk/slow_margin bool [m] a margin that the vehicle tries to slow down before stop_line crosswalk/slow_velocity double [m] a slow down velocity crosswalk/stop_predicted_object_prediction_time_margin double [s] time margin for decision of ego vehicle to stop or not walkway/stop_line_distance double [m] make stop line away from crosswalk when no explicit stop line exists walkway/stop_margin double [m] a margin that the vehicle tries to stop before walkway walkway/stop_duration_sec double [s] time margin for decision of ego vehicle to stop","title":"Module Parameters"},{"location":"planning/behavior_velocity_planner/crosswalk-design/#flowchart","text":"flow chart is almost the same as stop line.","title":"Flowchart"},{"location":"planning/behavior_velocity_planner/crosswalk-design/#known-issues","text":"","title":"Known Issues"},{"location":"planning/behavior_velocity_planner/crosswalk-design/#crosswalk_1","text":"The logic for determining speed should be set more strictly from safety reasons. The deceleration speed from the deceleration area logic is set to a constant value (10 [km/h]), which does not take into account the safety distance from obstacles.","title":"Crosswalk"},{"location":"planning/behavior_velocity_planner/crosswalk-design/#walkway","text":"If the vehicle exceeds the stop line more than the threshold distance, this module will get stuck in STOP state and will not start moving.","title":"Walkway"},{"location":"planning/behavior_velocity_planner/detection-area-design/","text":"Detection Area # Role # If pointcloud is detected in a detection area defined on a map, the stop planning will be executed at the predetermined point. Activation Timing # This module is activated when there is a detection area on the target lane. Algorithm # Gets a detection area and stop line from map information and confirms if there is pointcloud in the detection area Inserts stop point l[m] in front of the stop line Inserts a pass judge point to a point where the vehicle can stop with a max deceleration Sets velocity as zero behind the stop line when the ego-vehicle is in front of the pass judge point If the ego vehicle has passed the pass judge point already, it doesn\u2019t stop and pass through. Module Parameters # Parameter Type Description stop_margin double [m] a margin that the vehicle tries to stop before stop_line use_dead_line bool [-] weather to use dead line or not dead_line_margin double [m] ignore threshold that vehicle behind is collide with ego vehicle or not use_pass_judge_line bool [-] weather to use pass judge line or not state_clear_time double [s] when the vehicle is stopping for certain time without incoming obstacle, move to STOPPED state Flowchart #","title":"Detection area design"},{"location":"planning/behavior_velocity_planner/detection-area-design/#detection-area","text":"","title":"Detection Area"},{"location":"planning/behavior_velocity_planner/detection-area-design/#role","text":"If pointcloud is detected in a detection area defined on a map, the stop planning will be executed at the predetermined point.","title":"Role"},{"location":"planning/behavior_velocity_planner/detection-area-design/#activation-timing","text":"This module is activated when there is a detection area on the target lane.","title":"Activation Timing"},{"location":"planning/behavior_velocity_planner/detection-area-design/#algorithm","text":"Gets a detection area and stop line from map information and confirms if there is pointcloud in the detection area Inserts stop point l[m] in front of the stop line Inserts a pass judge point to a point where the vehicle can stop with a max deceleration Sets velocity as zero behind the stop line when the ego-vehicle is in front of the pass judge point If the ego vehicle has passed the pass judge point already, it doesn\u2019t stop and pass through.","title":"Algorithm"},{"location":"planning/behavior_velocity_planner/detection-area-design/#module-parameters","text":"Parameter Type Description stop_margin double [m] a margin that the vehicle tries to stop before stop_line use_dead_line bool [-] weather to use dead line or not dead_line_margin double [m] ignore threshold that vehicle behind is collide with ego vehicle or not use_pass_judge_line bool [-] weather to use pass judge line or not state_clear_time double [s] when the vehicle is stopping for certain time without incoming obstacle, move to STOPPED state","title":"Module Parameters"},{"location":"planning/behavior_velocity_planner/detection-area-design/#flowchart","text":"","title":"Flowchart"},{"location":"planning/behavior_velocity_planner/intersection-design/","text":"Intersection # Role # Judgement whether a vehicle can go into an intersection or not by a dynamic object information, and planning a velocity of the low-down/stop. This module is designed for rule-based intersection velocity decision that is easy for developers to design its behavior. It generates proper velocity for intersection scene. In addition, the external users / modules (e.g. remote operation) to can intervene the STOP/GO decision for the vehicle behavior. The override interface is expected to be used, for example, for remote intervention in emergency situations or gathering information on operator decisions during development. Activation Timing # This function is activated when the attention lane conflicts with the ego vehicle's lane. Limitations # This module allows developers to design vehicle velocity in intersection module using specific rules. This module is affected by object detection and prediction accuracy considering as stuck vehicle in this intersection module. Inner-workings / Algorithms # How To Select Attention Target Objects # Objects that satisfy all of the following conditions are considered as target objects (possible collision objects): The type of object type is car , truck , bus or motorbike . (Bicycle, pedestrian, animal, unknown are not.) The center of gravity of object is located within a certain distance from the attention lane (threshold = detection_area_margin ) . In the past, the decision was made based on whether any points of the object's polygon exists in the attention lane, but since there were many false positives, the logic has changed to the current one. The posture of object is the same direction as the attention lane (threshold = detection_area_angle_threshold ). The orientation of the target is recalculated in this module according to the orientation_reliable and the sign of the velocity of the target. Not being in the same lane as the ego vehicle . To avoid judging the vehicle ahead as a collision target. This logic needs to be improved. How to Define Attention Lanes # Target objects are limited by lanelets to prevent unexpected behavior. A lane that satisfies the following conditions is defined as an \"Attention Lane\" and used to define the target object. The lane crosses with the driving lane of the ego-vehicle The lane has high priority for the driving lane of the ego-vehicle (priority tags are needed to be configured in Lanelet-map according to the situation). See the following figures to know how to create an attention area and its rationale. Note: the case traffic light, turn right only is not currently implemented. Collision Check and Crossing Judgement # The following process is performed for the attention targets to determine whether the ego vehicle can cross the intersection safely. If it is judged that the ego vehicle cannot pass through the intersection with enough margin, it will insert the stopping speed on the stop line of the intersection. calculate the passing time and the time that the ego vehicle is in the intersection. This time is set as t_s t_s ~ t_e t_e extract the predicted path of the target object whose confidence is greater than min_predicted_path_confidence . detect collision between the extracted predicted path and ego's predicted path in the following process. obtain the passing area of the ego vehicle A_{ego} A_{ego} in t_s t_s ~ t_e t_e . calculate the passing area of the target object A_{target} A_{target} at t_s t_s - collision_start_margin_time ~ t_e t_e + collision_end_margin_time for each predicted path (*1). check if A_{ego} A_{ego} and A_{target} A_{target} regions are overlapped (has collision). when a collision is detected, the module inserts a stop velocity in front of the intersection. Note that there is a time margin for the stop release (*2). (*1) The parameters collision_start_margin_time and collision_end_margin_time can be interpreted as follows: If the ego vehicle passes through the intersection earlier than the target object, the collision is detected if the time difference between the two is less than collision_start_margin_time . If the ego vehicle passes through the intersection later than the target object, the collision is detected if the time difference between the two is less than collision_end_margin_time . (*2) If the collision is detected, the state transits to \"stop\" immediately. On the other hand, the collision judgment must be clear for a certain period (default : 2.0[s]) to transit from \"stop\" to \"go\" to prevent to prevent chattering of decisions. Stop Line Automatic Generation # The driving lane is complemented at a certain intervals (default : 20 [cm]), and the line which is a margin distance (default : 100cm) in front of the attention lane is defined as a stop line. (Also the length of the vehicle is considered and the stop point is set at the base_link point in front of the stop lane.) Pass Judge Line # To avoid a rapid braking, in case that a deceleration more than a threshold (default : 0.5[G]) is needed, the ego vehicle doesn\u2019t stop. In order to judge this condition, pass judge line is set a certain distance (default : 0.5 * v_current^2 / a_max) in front of the stop line. To prevent a chattering, once the ego vehicle passes this line, \u201cstop\u201d decision in the intersection won\u2019t be done any more. To prevent going over the pass judge line before the traffic light stop line, the distance between stop line and pass judge line become 0m in case that there is a stop line between the ego vehicle and an intersection stop line. Stuck Vehicle Detection # If there is any object in a certain distance (default : 5[m]) from the end point of the intersection lane on the driving lane and the object velocity is less than a threshold (default 3.0[km/h]), the object is regarded as a stuck vehicle. If the stuck vehicle exists, the ego vehicle cannot enter the intersection. Module Parameters # Parameter Type Description intersection/state_transit_margin_time double [m] time margin to change state intersection/decel_velocity double [m] deceleration velocity in intersection intersection/path_expand_width bool [m] path area to see with expansion intersection/stop_line_margin double [m] margin before stop line intersection/stuck_vehicle_detect_dist double [m] this should be the length between cars when they are stopped. intersection/stuck_vehicle_ignore_dist double [m] obstacle stop max distance(5.0[m]) + stuck vehicle size / 2.0[m]) intersection/stuck_vehicle_vel_thr double [m/s] velocity below 3[km/h] is ignored by default intersection/intersection_velocity double [m/s] velocity to pass intersection. 10[km/h] is by default intersection/intersection_max_accel double [m/s^2] acceleration in intersection intersection/detection_area_margin double [m] range for expanding detection area intersection/detection_area_length double [m] range for lidar detection 200[m] is by default intersection/detection_area_angle_threshold double [rad] threshold of angle difference between the detection object and lane intersection/min_predicted_path_confidence double [-] minimum confidence value of predicted path to use for collision detection merge_from_private_road/stop_duration_sec double [s] duration to stop How To Tune Parameters # The time to change state form Stop to GO is too long. Change state_transit_margin_time to lower value. Be careful if this margin is too small then vehicle is going to change state many times and cause chattering. The distance to stuck vehicle is too long. Change stuck_vehicle_detect_dist to lower value. Note this module consider obstacle stop max distance as detection distance. The speed in intersection is too slow Change intersection_velocity to higher value. Flowchart # NOTE current state is treated as STOP if is_entry_prohibited = true else GO Known Limits # This module generate intersection stop line and ignoring lanelet automatically form lanelet map , however if you want to set intersection stop line and ignoring lanelet manually you need to tag right_of_way and yield to all conflicting lanes properly. How to Set Lanelet Map fot Intersection # Set a turn_direction tag (Fig. 1) # IntersectionModule will be activated by this tag. If this tag is not set, ego-vehicle don\u2019t recognize the lane as an intersection. Even if it\u2019s a straight lane, this tag is mandatory if it is located within intersection. Set a value in turn_direction tag to light up turn signals. Values of turn_direction must be one of \u201cstraight\u201d(no turn signal), \u201cright\u201d or \u201cleft\u201d. Autoware will light up respective turn signals 30[m] before entering the specified lane. You may also set optional tag \u201cturn_signal_distance\u201d to modify the distance to start lighting up turn signals. Lanes within intersections must be defined as a single Lanelet. For example, blue lane in Fig.3 cannot be split into 2 Lanelets. Explicitly describe a stop position [RoadMarking] (Optional) (Fig. 2) # As a default, IntersectionModule estimates a stop position by the crossing point of driving lane and attention lane. But there are some cases like Fig.2 in which we would like to set a stop position explicitly. When a stop_line is defined as a RoadMarking item in the intersection lane, it overwrites the stop position. (Not only creating stop_line , but also defining as a RoadMaking item are needed.) Exclusion setting of attention lanes [RightOfWay] (Fig.3) # By default, IntersectionModule treats all lanes crossing with the registered lane as attention targets (yellow and green lanelets). But in some cases (e.g. when driving lane is priority lane or traffic light is green for the driving lane), we want to ignore some of the yield lanes. By setting RightOfWay of the RegulatoryElement item, we can define lanes to be ignored. Register ignored lanes as \u201cyield\u201d and register the attention lanes and driving lane as \u201cright_of_way\u201d lanelets in RightOfWay RegulatoryElement (For an intersection with traffic lights, we need to create items for each lane in the intersection. Please note that it needs a lot of man-hours.)","title":"Intersection design"},{"location":"planning/behavior_velocity_planner/intersection-design/#intersection","text":"","title":"Intersection"},{"location":"planning/behavior_velocity_planner/intersection-design/#role","text":"Judgement whether a vehicle can go into an intersection or not by a dynamic object information, and planning a velocity of the low-down/stop. This module is designed for rule-based intersection velocity decision that is easy for developers to design its behavior. It generates proper velocity for intersection scene. In addition, the external users / modules (e.g. remote operation) to can intervene the STOP/GO decision for the vehicle behavior. The override interface is expected to be used, for example, for remote intervention in emergency situations or gathering information on operator decisions during development.","title":"Role"},{"location":"planning/behavior_velocity_planner/intersection-design/#activation-timing","text":"This function is activated when the attention lane conflicts with the ego vehicle's lane.","title":"Activation Timing"},{"location":"planning/behavior_velocity_planner/intersection-design/#limitations","text":"This module allows developers to design vehicle velocity in intersection module using specific rules. This module is affected by object detection and prediction accuracy considering as stuck vehicle in this intersection module.","title":"Limitations"},{"location":"planning/behavior_velocity_planner/intersection-design/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"planning/behavior_velocity_planner/intersection-design/#how-to-select-attention-target-objects","text":"Objects that satisfy all of the following conditions are considered as target objects (possible collision objects): The type of object type is car , truck , bus or motorbike . (Bicycle, pedestrian, animal, unknown are not.) The center of gravity of object is located within a certain distance from the attention lane (threshold = detection_area_margin ) . In the past, the decision was made based on whether any points of the object's polygon exists in the attention lane, but since there were many false positives, the logic has changed to the current one. The posture of object is the same direction as the attention lane (threshold = detection_area_angle_threshold ). The orientation of the target is recalculated in this module according to the orientation_reliable and the sign of the velocity of the target. Not being in the same lane as the ego vehicle . To avoid judging the vehicle ahead as a collision target. This logic needs to be improved.","title":"How To Select Attention Target Objects"},{"location":"planning/behavior_velocity_planner/intersection-design/#how-to-define-attention-lanes","text":"Target objects are limited by lanelets to prevent unexpected behavior. A lane that satisfies the following conditions is defined as an \"Attention Lane\" and used to define the target object. The lane crosses with the driving lane of the ego-vehicle The lane has high priority for the driving lane of the ego-vehicle (priority tags are needed to be configured in Lanelet-map according to the situation). See the following figures to know how to create an attention area and its rationale. Note: the case traffic light, turn right only is not currently implemented.","title":"How to Define Attention Lanes"},{"location":"planning/behavior_velocity_planner/intersection-design/#collision-check-and-crossing-judgement","text":"The following process is performed for the attention targets to determine whether the ego vehicle can cross the intersection safely. If it is judged that the ego vehicle cannot pass through the intersection with enough margin, it will insert the stopping speed on the stop line of the intersection. calculate the passing time and the time that the ego vehicle is in the intersection. This time is set as t_s t_s ~ t_e t_e extract the predicted path of the target object whose confidence is greater than min_predicted_path_confidence . detect collision between the extracted predicted path and ego's predicted path in the following process. obtain the passing area of the ego vehicle A_{ego} A_{ego} in t_s t_s ~ t_e t_e . calculate the passing area of the target object A_{target} A_{target} at t_s t_s - collision_start_margin_time ~ t_e t_e + collision_end_margin_time for each predicted path (*1). check if A_{ego} A_{ego} and A_{target} A_{target} regions are overlapped (has collision). when a collision is detected, the module inserts a stop velocity in front of the intersection. Note that there is a time margin for the stop release (*2). (*1) The parameters collision_start_margin_time and collision_end_margin_time can be interpreted as follows: If the ego vehicle passes through the intersection earlier than the target object, the collision is detected if the time difference between the two is less than collision_start_margin_time . If the ego vehicle passes through the intersection later than the target object, the collision is detected if the time difference between the two is less than collision_end_margin_time . (*2) If the collision is detected, the state transits to \"stop\" immediately. On the other hand, the collision judgment must be clear for a certain period (default : 2.0[s]) to transit from \"stop\" to \"go\" to prevent to prevent chattering of decisions.","title":"Collision Check and Crossing Judgement"},{"location":"planning/behavior_velocity_planner/intersection-design/#stop-line-automatic-generation","text":"The driving lane is complemented at a certain intervals (default : 20 [cm]), and the line which is a margin distance (default : 100cm) in front of the attention lane is defined as a stop line. (Also the length of the vehicle is considered and the stop point is set at the base_link point in front of the stop lane.)","title":"Stop Line Automatic Generation"},{"location":"planning/behavior_velocity_planner/intersection-design/#pass-judge-line","text":"To avoid a rapid braking, in case that a deceleration more than a threshold (default : 0.5[G]) is needed, the ego vehicle doesn\u2019t stop. In order to judge this condition, pass judge line is set a certain distance (default : 0.5 * v_current^2 / a_max) in front of the stop line. To prevent a chattering, once the ego vehicle passes this line, \u201cstop\u201d decision in the intersection won\u2019t be done any more. To prevent going over the pass judge line before the traffic light stop line, the distance between stop line and pass judge line become 0m in case that there is a stop line between the ego vehicle and an intersection stop line.","title":"Pass Judge Line"},{"location":"planning/behavior_velocity_planner/intersection-design/#stuck-vehicle-detection","text":"If there is any object in a certain distance (default : 5[m]) from the end point of the intersection lane on the driving lane and the object velocity is less than a threshold (default 3.0[km/h]), the object is regarded as a stuck vehicle. If the stuck vehicle exists, the ego vehicle cannot enter the intersection.","title":"Stuck Vehicle Detection"},{"location":"planning/behavior_velocity_planner/intersection-design/#module-parameters","text":"Parameter Type Description intersection/state_transit_margin_time double [m] time margin to change state intersection/decel_velocity double [m] deceleration velocity in intersection intersection/path_expand_width bool [m] path area to see with expansion intersection/stop_line_margin double [m] margin before stop line intersection/stuck_vehicle_detect_dist double [m] this should be the length between cars when they are stopped. intersection/stuck_vehicle_ignore_dist double [m] obstacle stop max distance(5.0[m]) + stuck vehicle size / 2.0[m]) intersection/stuck_vehicle_vel_thr double [m/s] velocity below 3[km/h] is ignored by default intersection/intersection_velocity double [m/s] velocity to pass intersection. 10[km/h] is by default intersection/intersection_max_accel double [m/s^2] acceleration in intersection intersection/detection_area_margin double [m] range for expanding detection area intersection/detection_area_length double [m] range for lidar detection 200[m] is by default intersection/detection_area_angle_threshold double [rad] threshold of angle difference between the detection object and lane intersection/min_predicted_path_confidence double [-] minimum confidence value of predicted path to use for collision detection merge_from_private_road/stop_duration_sec double [s] duration to stop","title":"Module Parameters"},{"location":"planning/behavior_velocity_planner/intersection-design/#how-to-tune-parameters","text":"The time to change state form Stop to GO is too long. Change state_transit_margin_time to lower value. Be careful if this margin is too small then vehicle is going to change state many times and cause chattering. The distance to stuck vehicle is too long. Change stuck_vehicle_detect_dist to lower value. Note this module consider obstacle stop max distance as detection distance. The speed in intersection is too slow Change intersection_velocity to higher value.","title":"How To Tune Parameters"},{"location":"planning/behavior_velocity_planner/intersection-design/#flowchart","text":"NOTE current state is treated as STOP if is_entry_prohibited = true else GO","title":"Flowchart"},{"location":"planning/behavior_velocity_planner/intersection-design/#known-limits","text":"This module generate intersection stop line and ignoring lanelet automatically form lanelet map , however if you want to set intersection stop line and ignoring lanelet manually you need to tag right_of_way and yield to all conflicting lanes properly.","title":"Known Limits"},{"location":"planning/behavior_velocity_planner/intersection-design/#how-to-set-lanelet-map-fot-intersection","text":"","title":"How to Set Lanelet Map fot Intersection"},{"location":"planning/behavior_velocity_planner/intersection-design/#set-a-turn_direction-tag-fig-1","text":"IntersectionModule will be activated by this tag. If this tag is not set, ego-vehicle don\u2019t recognize the lane as an intersection. Even if it\u2019s a straight lane, this tag is mandatory if it is located within intersection. Set a value in turn_direction tag to light up turn signals. Values of turn_direction must be one of \u201cstraight\u201d(no turn signal), \u201cright\u201d or \u201cleft\u201d. Autoware will light up respective turn signals 30[m] before entering the specified lane. You may also set optional tag \u201cturn_signal_distance\u201d to modify the distance to start lighting up turn signals. Lanes within intersections must be defined as a single Lanelet. For example, blue lane in Fig.3 cannot be split into 2 Lanelets.","title":"Set a turn_direction tag (Fig. 1)"},{"location":"planning/behavior_velocity_planner/intersection-design/#explicitly-describe-a-stop-position-roadmarking-optional-fig-2","text":"As a default, IntersectionModule estimates a stop position by the crossing point of driving lane and attention lane. But there are some cases like Fig.2 in which we would like to set a stop position explicitly. When a stop_line is defined as a RoadMarking item in the intersection lane, it overwrites the stop position. (Not only creating stop_line , but also defining as a RoadMaking item are needed.)","title":"Explicitly describe a stop position [RoadMarking] (Optional) (Fig. 2)"},{"location":"planning/behavior_velocity_planner/intersection-design/#exclusion-setting-of-attention-lanes-rightofway-fig3","text":"By default, IntersectionModule treats all lanes crossing with the registered lane as attention targets (yellow and green lanelets). But in some cases (e.g. when driving lane is priority lane or traffic light is green for the driving lane), we want to ignore some of the yield lanes. By setting RightOfWay of the RegulatoryElement item, we can define lanes to be ignored. Register ignored lanes as \u201cyield\u201d and register the attention lanes and driving lane as \u201cright_of_way\u201d lanelets in RightOfWay RegulatoryElement (For an intersection with traffic lights, we need to create items for each lane in the intersection. Please note that it needs a lot of man-hours.)","title":"Exclusion setting of attention lanes [RightOfWay] (Fig.3)"},{"location":"planning/behavior_velocity_planner/merge-from-private-design/","text":"Merge From Private # Role # When an ego vehicle enters a public road from a private road (e.g. a parking lot), it needs to face and stop before entering the public road to make sure it is safe. This module is activated when there is an intersection at the location where the vehicle enters the public road from the private road. The basic behavior is the same as the intersection module, but the ego vehicle must stop once at the stop line. Activation Timing # This module is activated when the following conditions are met: ego-lane has a private tag ego-lane has a conflict with other no-private lanelets Module Parameters # Parameter Type Description merge_from_private_road/stop_duration_sec double [m] time margin to change state Known Issue # If ego vehicle go over the stop line for a certain distance, then ego vehicle will not transit from STOP.","title":"Merge from private design"},{"location":"planning/behavior_velocity_planner/merge-from-private-design/#merge-from-private","text":"","title":"Merge From Private"},{"location":"planning/behavior_velocity_planner/merge-from-private-design/#role","text":"When an ego vehicle enters a public road from a private road (e.g. a parking lot), it needs to face and stop before entering the public road to make sure it is safe. This module is activated when there is an intersection at the location where the vehicle enters the public road from the private road. The basic behavior is the same as the intersection module, but the ego vehicle must stop once at the stop line.","title":"Role"},{"location":"planning/behavior_velocity_planner/merge-from-private-design/#activation-timing","text":"This module is activated when the following conditions are met: ego-lane has a private tag ego-lane has a conflict with other no-private lanelets","title":"Activation Timing"},{"location":"planning/behavior_velocity_planner/merge-from-private-design/#module-parameters","text":"Parameter Type Description merge_from_private_road/stop_duration_sec double [m] time margin to change state","title":"Module Parameters"},{"location":"planning/behavior_velocity_planner/merge-from-private-design/#known-issue","text":"If ego vehicle go over the stop line for a certain distance, then ego vehicle will not transit from STOP.","title":"Known Issue"},{"location":"planning/behavior_velocity_planner/no-stopping-area-design/","text":"No Stopping Area # Role # This module plans to avoid stop in 'no stopping area`. PassThrough case if ego vehicle go through pass judge point, then ego vehicle can't stop with maximum jerk and acceleration, so this module doesn't insert stop velocity. In this case override or external operation is necessary. STOP case If there is a stuck vehicle or stop velocity around no_stopping_area , then vehicle stops inside no_stopping_area so this module makes stop velocity in front of no_stopping_area GO case else Limitation # This module allows developers to design vehicle velocity in no_stopping_area module using specific rules. Once ego vehicle go through pass through point, ego vehicle does't insert stop velocity and does't change decision from GO. Also this module only considers dynamic object in order to avoid unnecessarily stop. ModelParameter # Parameter Type Description state_clear_time double [s] time to clear stop state stuck_vehicle_vel_thr double [m/s] vehicles below this velocity are considered as stuck vehicle. stop_margin double [m] margin to stop line at no stopping area dead_line_margin double [m] if ego pass this position GO stop_line_margin double [m] margin to auto-gen stop line at no stopping area detection_area_length double [m] length of searching polygon stuck_vehicle_front_margin double [m] obstacle stop max distance Flowchart #","title":"No stopping area design"},{"location":"planning/behavior_velocity_planner/no-stopping-area-design/#no-stopping-area","text":"","title":"No Stopping Area"},{"location":"planning/behavior_velocity_planner/no-stopping-area-design/#role","text":"This module plans to avoid stop in 'no stopping area`. PassThrough case if ego vehicle go through pass judge point, then ego vehicle can't stop with maximum jerk and acceleration, so this module doesn't insert stop velocity. In this case override or external operation is necessary. STOP case If there is a stuck vehicle or stop velocity around no_stopping_area , then vehicle stops inside no_stopping_area so this module makes stop velocity in front of no_stopping_area GO case else","title":"Role"},{"location":"planning/behavior_velocity_planner/no-stopping-area-design/#limitation","text":"This module allows developers to design vehicle velocity in no_stopping_area module using specific rules. Once ego vehicle go through pass through point, ego vehicle does't insert stop velocity and does't change decision from GO. Also this module only considers dynamic object in order to avoid unnecessarily stop.","title":"Limitation"},{"location":"planning/behavior_velocity_planner/no-stopping-area-design/#modelparameter","text":"Parameter Type Description state_clear_time double [s] time to clear stop state stuck_vehicle_vel_thr double [m/s] vehicles below this velocity are considered as stuck vehicle. stop_margin double [m] margin to stop line at no stopping area dead_line_margin double [m] if ego pass this position GO stop_line_margin double [m] margin to auto-gen stop line at no stopping area detection_area_length double [m] length of searching polygon stuck_vehicle_front_margin double [m] obstacle stop max distance","title":"ModelParameter"},{"location":"planning/behavior_velocity_planner/no-stopping-area-design/#flowchart","text":"","title":"Flowchart"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/","text":"Occlusion Spot # Role # This module plans safe velocity to slow down before reaching collision point that hidden object is darting out from occlusion spot where driver can't see clearly because of obstacles. Activation Timing # This module is activated if launch_occlusion_spot becomes true Limitation # To solve the excessive deceleration due to false positive of the perception, the logic of detection method can be selectable. This point has not been discussed in detail and needs to be improved (see the description below). Inner-workings / Algorithms # Logics Working # There are several types of occlusions, such as \"occlusions generated by parked vehicles\" and \"occlusions caused by obstructions\". In situations such as driving on road with obstacles , where people jump out of the way frequently, all possible occlusion spots must be taken into account. This module considers all occlusion spots calculated from the occupancy grid , but it is not reasonable to take into account all occlusion spots for example, people jumping out from behind a guardrail, or behind cruising vehicle. Therefore currently detection area will be limited to to use dynamic object information. Note that this decision logic is still under development and needs to be improved. Occlusion Spot Public # This module inserts safe velocity at the collision point estimated from the associated occlusion spot under assumption that the pedestrian possibly coming out of the occlusion spot. This module consider 3 policies that are very important for risk predicting system for occlusion spot. \"Passable\" without deceleration If ego vehicle speed is high enough to pass the occlusion spot and expected to have no collision with any objects coming out of the occlusion spot, then it's possible for ego vehicle to pass the spot without deceleration. \"Predictable\" with enough distance to occlusion If ego vehicle has enough distance to the occlusion spot, then ego vehicle is going to slow down to the speed that is slow enough to stop before collision with full brake. If ego vehicle pass the possible collision point, then ego vehicle is going to drive normally. \"Unavoidable\" without enough distance to occlusion spot This module assumes the occlusion spot is detected stably far from the ego vehicle. Therefore this module can not guarantee the safety behavior for the occlusion spot detected suddenly in front of the ego vehicle. In this case, slow velocity that does not cause the strong deceleration is only applied. Occlusion Spot Private # This module considers any occlusion spot around ego path computed from the occupancy grid. Occlusion Spot Common # The Concept of Safe Velocity # The safe slowdown velocity is calculated from the below parameters of ego emergency braking system and time to collision. jerk limit[m/s^3] deceleration limit[m/s2] delay response time[s] time to collision of pedestrian[s] with these parameters we can briefly define safe motion before occlusion spot for ideal environment. Maximum Slowdown Velocity # The maximum slowdown velocity is calculated from the below parameters of ego current velocity and acceleration with maximum slowdown jerk and maximum slowdown acceleration in order not to slowdown too much. j_{max} j_{max} slowdown jerk limit[m/s^3] a_{max} a_{max} slowdown deceleration limit[m/s2] v_{0} v_{0} current velocity[m/s] a_{0} a_{0} current acceleration[m/s] Safe Behavior After Passing Safe Margin Point # This module defines safe margin to consider ego distance to stop and collision path point geometrically. While ego is cruising from safe margin to collision path point, ego vehicle keeps the same velocity as occlusion spot safe velocity. DetectionArea Polygon # Occlusion spot computation: searching occlusion spots for all cells in the occupancy_grid inside \"max lateral distance\" requires a lot of computational cost, so this module use only one most notable occlusion spot for each partition. (currently offset is from baselink to front for safety) The maximum length of detection area depends on ego current vehicle velocity and acceleration. Partition Lanelet # By using lanelet information of \"guard_rail\", \"fence\", \"wall\" tag, it's possible to remove unwanted occlusion spot. Use Object Info # use object info to make occupancy grid more accurate Collision Free Judgement # obstacle that can run out from occlusion should have free space until intersection from ego vehicle Possible Collision # obstacle that can run out from occlusion is interrupted by moving vehicle. Module Parameters # Parameter Type Description pedestrian_vel double [m/s] maximum velocity assumed pedestrian coming out from occlusion point. pedestrian_radius double [m] assumed pedestrian radius which fits in occlusion spot. Parameter Type Description use_object_info bool [-] whether to reflect object info to occupancy grid map or not. use_partition_lanelet bool [-] whether to use partition lanelet map data. Parameter /debug Type Description is_show_occlusion bool [-] whether to show occlusion point markers.\u3000 is_show_cv_window bool [-] whether to show open_cv debug window. is_show_processing_time bool [-] whether to show processing time. Parameter /threshold Type Description detection_area_length double [m] the length of path to consider occlusion spot stuck_vehicle_vel double [m/s] velocity below this value is assumed to stop lateral_distance double [m] maximum lateral distance to consider hidden collision Parameter /motion Type Description safety_ratio double [-] safety ratio for jerk and acceleration max_slow_down_jerk double [m/s^3] jerk for safe brake max_slow_down_accel double [m/s^2] deceleration for safe brake non_effective_jerk double [m/s^3] weak jerk for velocity planning. non_effective_acceleration double [m/s^2] weak deceleration for velocity planning. min_allowed_velocity double [m/s] minimum velocity allowed safe_margin double [m] maximum error to stop with emergency braking system. Parameter /detection_area Type Description min_occlusion_spot_size double [m] the length of path to consider occlusion spot slice_length double [m] the distance of divided detection area max_lateral_distance double [m] buffer around the ego path used to build the detection_area area. Parameter /grid Type Description free_space_max double [-] maximum value of a free space cell in the occupancy grid occupied_min double [-] buffer around the ego path used to build the detection_area area. Flowchart # Rough overview of the whole process # Detail process for predicted object # Detail process for private road #","title":"Occlusion spot design"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#occlusion-spot","text":"","title":"Occlusion Spot"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#role","text":"This module plans safe velocity to slow down before reaching collision point that hidden object is darting out from occlusion spot where driver can't see clearly because of obstacles.","title":"Role"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#activation-timing","text":"This module is activated if launch_occlusion_spot becomes true","title":"Activation Timing"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#limitation","text":"To solve the excessive deceleration due to false positive of the perception, the logic of detection method can be selectable. This point has not been discussed in detail and needs to be improved (see the description below).","title":"Limitation"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#logics-working","text":"There are several types of occlusions, such as \"occlusions generated by parked vehicles\" and \"occlusions caused by obstructions\". In situations such as driving on road with obstacles , where people jump out of the way frequently, all possible occlusion spots must be taken into account. This module considers all occlusion spots calculated from the occupancy grid , but it is not reasonable to take into account all occlusion spots for example, people jumping out from behind a guardrail, or behind cruising vehicle. Therefore currently detection area will be limited to to use dynamic object information. Note that this decision logic is still under development and needs to be improved.","title":"Logics Working"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#occlusion-spot-public","text":"This module inserts safe velocity at the collision point estimated from the associated occlusion spot under assumption that the pedestrian possibly coming out of the occlusion spot. This module consider 3 policies that are very important for risk predicting system for occlusion spot. \"Passable\" without deceleration If ego vehicle speed is high enough to pass the occlusion spot and expected to have no collision with any objects coming out of the occlusion spot, then it's possible for ego vehicle to pass the spot without deceleration. \"Predictable\" with enough distance to occlusion If ego vehicle has enough distance to the occlusion spot, then ego vehicle is going to slow down to the speed that is slow enough to stop before collision with full brake. If ego vehicle pass the possible collision point, then ego vehicle is going to drive normally. \"Unavoidable\" without enough distance to occlusion spot This module assumes the occlusion spot is detected stably far from the ego vehicle. Therefore this module can not guarantee the safety behavior for the occlusion spot detected suddenly in front of the ego vehicle. In this case, slow velocity that does not cause the strong deceleration is only applied.","title":"Occlusion Spot Public"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#occlusion-spot-private","text":"This module considers any occlusion spot around ego path computed from the occupancy grid.","title":"Occlusion Spot Private"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#occlusion-spot-common","text":"","title":"Occlusion Spot Common"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#the-concept-of-safe-velocity","text":"The safe slowdown velocity is calculated from the below parameters of ego emergency braking system and time to collision. jerk limit[m/s^3] deceleration limit[m/s2] delay response time[s] time to collision of pedestrian[s] with these parameters we can briefly define safe motion before occlusion spot for ideal environment.","title":"The Concept of Safe Velocity"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#maximum-slowdown-velocity","text":"The maximum slowdown velocity is calculated from the below parameters of ego current velocity and acceleration with maximum slowdown jerk and maximum slowdown acceleration in order not to slowdown too much. j_{max} j_{max} slowdown jerk limit[m/s^3] a_{max} a_{max} slowdown deceleration limit[m/s2] v_{0} v_{0} current velocity[m/s] a_{0} a_{0} current acceleration[m/s]","title":"Maximum Slowdown Velocity"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#safe-behavior-after-passing-safe-margin-point","text":"This module defines safe margin to consider ego distance to stop and collision path point geometrically. While ego is cruising from safe margin to collision path point, ego vehicle keeps the same velocity as occlusion spot safe velocity.","title":"Safe Behavior After Passing Safe Margin Point"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#detectionarea-polygon","text":"Occlusion spot computation: searching occlusion spots for all cells in the occupancy_grid inside \"max lateral distance\" requires a lot of computational cost, so this module use only one most notable occlusion spot for each partition. (currently offset is from baselink to front for safety) The maximum length of detection area depends on ego current vehicle velocity and acceleration.","title":"DetectionArea Polygon"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#partition-lanelet","text":"By using lanelet information of \"guard_rail\", \"fence\", \"wall\" tag, it's possible to remove unwanted occlusion spot.","title":"Partition Lanelet"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#use-object-info","text":"use object info to make occupancy grid more accurate","title":"Use Object Info"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#collision-free-judgement","text":"obstacle that can run out from occlusion should have free space until intersection from ego vehicle","title":"Collision Free Judgement"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#possible-collision","text":"obstacle that can run out from occlusion is interrupted by moving vehicle.","title":"Possible Collision"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#module-parameters","text":"Parameter Type Description pedestrian_vel double [m/s] maximum velocity assumed pedestrian coming out from occlusion point. pedestrian_radius double [m] assumed pedestrian radius which fits in occlusion spot. Parameter Type Description use_object_info bool [-] whether to reflect object info to occupancy grid map or not. use_partition_lanelet bool [-] whether to use partition lanelet map data. Parameter /debug Type Description is_show_occlusion bool [-] whether to show occlusion point markers.\u3000 is_show_cv_window bool [-] whether to show open_cv debug window. is_show_processing_time bool [-] whether to show processing time. Parameter /threshold Type Description detection_area_length double [m] the length of path to consider occlusion spot stuck_vehicle_vel double [m/s] velocity below this value is assumed to stop lateral_distance double [m] maximum lateral distance to consider hidden collision Parameter /motion Type Description safety_ratio double [-] safety ratio for jerk and acceleration max_slow_down_jerk double [m/s^3] jerk for safe brake max_slow_down_accel double [m/s^2] deceleration for safe brake non_effective_jerk double [m/s^3] weak jerk for velocity planning. non_effective_acceleration double [m/s^2] weak deceleration for velocity planning. min_allowed_velocity double [m/s] minimum velocity allowed safe_margin double [m] maximum error to stop with emergency braking system. Parameter /detection_area Type Description min_occlusion_spot_size double [m] the length of path to consider occlusion spot slice_length double [m] the distance of divided detection area max_lateral_distance double [m] buffer around the ego path used to build the detection_area area. Parameter /grid Type Description free_space_max double [-] maximum value of a free space cell in the occupancy grid occupied_min double [-] buffer around the ego path used to build the detection_area area.","title":"Module Parameters"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#flowchart","text":"","title":"Flowchart"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#rough-overview-of-the-whole-process","text":"","title":"Rough overview of the whole process"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#detail-process-for-predicted-object","text":"","title":"Detail process for predicted object"},{"location":"planning/behavior_velocity_planner/occlusion-spot-design/#detail-process-for-private-road","text":"","title":"Detail process for private road"},{"location":"planning/behavior_velocity_planner/run-out-design/","text":"Run Out # Role # run_out is the module that decelerates and stops for dynamic obstacles such as pedestrians and bicycles. Activation Timing # This module is activated if launch_run_out becomes true Inner-workings / Algorithms # Flow chart # Preprocess path # Calculate the expected target velocity for ego vehicle # Calculate the expected target velocity for the ego vehicle path to calculate time to collision with obstacles more precisely. The expected target velocity is calculated with motion velocity smoother module by using current velocity, current acceleration and velocity limits directed by the map and external API. Extend the path # The path is extended by the length of base link to front to consider obstacles after the goal. Trim path from ego position # The path is trimmed from ego position to a certain distance to reduce calculation time. Trimmed distance is specified by parameter of detection_distance . Preprocess obstacles # Create data of abstracted dynamic obstacle # This module can handle multiple types of obstacles by creating abstracted dynamic obstacle data layer. Currently we have 3 types of detection method (Object, ObjectWithoutPath, Points) to create abstracted obstacle data. Abstracted dynamic obstacle # Abstracted obstacle data has following information. Name Type Description pose geometry_msgs::msg::Pose pose of the obstacle classifications std::vector<autoware_auto_perception_msgs::msg::ObjectClassification> classifications with probability shape autoware_auto_perception_msgs::msg::Shape shape of the obstacle predicted_paths std::vector<DynamicObstacle::PredictedPath> predicted paths with confidence. this data doesn't have time step because we use minimum and maximum velocity instead. min_velocity_mps float minimum velocity of the obstacle. specified by parameter of dynamic_obstacle.min_vel_kmph max_velocity_mps float maximum velocity of the obstacle. specified by parameter of dynamic_obstacle.max_vel_kmph Enter the maximum/minimum velocity of the object as a parameter, adding enough margin to the expected velocity. This parameter is used to create polygons for collision detection . Future work: Determine the maximum/minimum velocity from the estimated velocity with covariance of the object 3 types of detection method # We have 3 types of detection method to meet different safety and availability requirements. The characteristics of them are shown in the table below. Method of Object has high availability (less false positive) because it detects only objects whose predicted path is on the lane. However, sometimes it is not safe because perception may fail to detect obstacles or generate incorrect predicted paths. On the other hand, method of Points has high safety (less false negative) because it uses pointcloud as input. Since points don't have a predicted path, the path that moves in the direction normal to the path of ego vehicle is considered to be the predicted path of abstracted dynamic obstacle data. However, without proper adjustment of filter of points, it may detect a lot of points and it will result in very low availability. Method of ObjectWithoutPath has the characteristics of an intermediate of Object and Points . Method Description Object use an object with the predicted path for collision detection. ObjectWithoutPath use an object but not use the predicted path for collision detection. replace the path assuming that an object jumps out to the lane at specified velocity. Points use filtered points for collision detection. the path is created assuming that points jump out to the lane. points are regarded as an small circular shaped obstacle. Exclude obstacles outside of partition # This module can exclude the obstacles outside of partition such as guardrail, fence, and wall. We need lanelet map that has the information of partition to use this feature. By this feature, we can reduce unnecessary deceleration by obstacles that are unlikely to jump out to the lane. You can choose whether to use this feature by parameter of use_partition_lanelet . Collision detection # Detect collision with dynamic obstacles # Along the ego vehicle path, determine the points where collision detection is to be performed for each detection_span . The travel times to the each points are calculated from the expected target velocity . For the each points, collision detection is performed using the footprint polygon of the ego vehicle and the polygon of the predicted location of the obstacles. The predicted location of the obstacles is described as rectangle or polygon that has the range calculated by min velocity, max velocity and the ego vehicle's travel time to the point. If the input type of the dynamic obstacle is Points , the obstacle shape is defined as a small cylinder. Multiple points are detected as collision points because collision detection is calculated between two polygons. So we select the point that is on the same side as the obstacle and close to ego vehicle as the collision point. Insert velocity # Insert velocity to decelerate for obstacles # If the collision is detected, stop point is inserted on distance of base link to front + stop margin from the selected collision point. The base link to front means the distance between base_link (center of rear-wheel axis) and front of the car. Stop margin is determined by the parameter of stop_margin . Limit velocity with specified jerk and acc limit # The maximum slowdown velocity is calculated in order not to slowdown too much. See the Occlusion Spot document for more details. You can choose whether to use this feature by parameter of slow_down_limit.enable . Module Parameters # Parameter Type Description detection_method string [-] candidate: Object, ObjectWithoutPath, Points use_partition_lanelet bool [-] whether to use partition lanelet map data specify_decel_jerk bool [-] whether to specify jerk when ego decelerates stop_margin double [m] the vehicle decelerates to be able to stop with this margin passing_margin double [m] the vehicle begins to accelerate if the vehicle's front in predicted position is ahead of the obstacle + this margin deceleration_jerk double [m/s^3] ego decelerates with this jerk when stopping for obstacles obstacle_velocity_kph double [km/h] assumption for obstacle velocity detection_distance double [m] ahead distance from ego to detect the obstacles detection_span double [m] calculate collision with this span to reduce calculation time min_vel_ego_kmph double [km/h] min velocity to calculate time to collision Parameter /detection_area_size Type Description dist_ahead double [m] ahead distance from ego position dist_behind double [m] behind distance from ego position dist_right double [m] right distance from ego position dist_left double [m] left distance from ego position Parameter /dynamic_obstacle Type Description min_vel_kmph double [km/h] minimum velocity for dynamic obstacles max_vel_kmph double [km/h] maximum velocity for dynamic obstacles diameter double [m] diameter of obstacles. used for creating dynamic obstacles from points height double [m] height of obstacles. used for creating dynamic obstacles from points max_prediction_time double [sec] create predicted path until this time time_step double [sec] time step for each path step. used for creating dynamic obstacles from points or objects without path Parameter /approaching Type Description enable bool [-] whether to enable approaching after stopping margin double [m] distance on how close ego approaches the obstacle limit_vel_kmph double [km/h] limit velocity for approaching after stopping stop_thresh double [m/s] threshold to decide if ego is stopping stop_time_thresh double [sec] threshold for stopping time to transit to approaching state dist_thresh double [m] end the approaching state if distance to the obstacle is longer than stop_margin + dist_thresh Parameter /slow_down_limit Type Description enable bool [-] whether to enable to limit velocity with max jerk and acc max_jerk double [m/s^3] minimum jerk deceleration for safe brake. max_acc double [m/s^2] minimum accel deceleration for safe brake. Future extensions / Unimplemented parts # Calculate obstacle's min velocity and max velocity from covariance Detect collisions with polygon object Handle the case when the predicted path of obstacles are not straight line Currently collision check is calculated based on the assumption that the predicted path of the obstacle is a straight line","title":"Run out design"},{"location":"planning/behavior_velocity_planner/run-out-design/#run-out","text":"","title":"Run Out"},{"location":"planning/behavior_velocity_planner/run-out-design/#role","text":"run_out is the module that decelerates and stops for dynamic obstacles such as pedestrians and bicycles.","title":"Role"},{"location":"planning/behavior_velocity_planner/run-out-design/#activation-timing","text":"This module is activated if launch_run_out becomes true","title":"Activation Timing"},{"location":"planning/behavior_velocity_planner/run-out-design/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"planning/behavior_velocity_planner/run-out-design/#flow-chart","text":"","title":"Flow chart"},{"location":"planning/behavior_velocity_planner/run-out-design/#preprocess-path","text":"","title":"Preprocess path"},{"location":"planning/behavior_velocity_planner/run-out-design/#calculate-the-expected-target-velocity-for-ego-vehicle","text":"Calculate the expected target velocity for the ego vehicle path to calculate time to collision with obstacles more precisely. The expected target velocity is calculated with motion velocity smoother module by using current velocity, current acceleration and velocity limits directed by the map and external API.","title":"Calculate the expected target velocity for ego vehicle"},{"location":"planning/behavior_velocity_planner/run-out-design/#extend-the-path","text":"The path is extended by the length of base link to front to consider obstacles after the goal.","title":"Extend the path"},{"location":"planning/behavior_velocity_planner/run-out-design/#trim-path-from-ego-position","text":"The path is trimmed from ego position to a certain distance to reduce calculation time. Trimmed distance is specified by parameter of detection_distance .","title":"Trim path from ego position"},{"location":"planning/behavior_velocity_planner/run-out-design/#preprocess-obstacles","text":"","title":"Preprocess obstacles"},{"location":"planning/behavior_velocity_planner/run-out-design/#create-data-of-abstracted-dynamic-obstacle","text":"This module can handle multiple types of obstacles by creating abstracted dynamic obstacle data layer. Currently we have 3 types of detection method (Object, ObjectWithoutPath, Points) to create abstracted obstacle data.","title":"Create data of abstracted dynamic obstacle"},{"location":"planning/behavior_velocity_planner/run-out-design/#abstracted-dynamic-obstacle","text":"Abstracted obstacle data has following information. Name Type Description pose geometry_msgs::msg::Pose pose of the obstacle classifications std::vector<autoware_auto_perception_msgs::msg::ObjectClassification> classifications with probability shape autoware_auto_perception_msgs::msg::Shape shape of the obstacle predicted_paths std::vector<DynamicObstacle::PredictedPath> predicted paths with confidence. this data doesn't have time step because we use minimum and maximum velocity instead. min_velocity_mps float minimum velocity of the obstacle. specified by parameter of dynamic_obstacle.min_vel_kmph max_velocity_mps float maximum velocity of the obstacle. specified by parameter of dynamic_obstacle.max_vel_kmph Enter the maximum/minimum velocity of the object as a parameter, adding enough margin to the expected velocity. This parameter is used to create polygons for collision detection . Future work: Determine the maximum/minimum velocity from the estimated velocity with covariance of the object","title":"Abstracted dynamic obstacle"},{"location":"planning/behavior_velocity_planner/run-out-design/#3-types-of-detection-method","text":"We have 3 types of detection method to meet different safety and availability requirements. The characteristics of them are shown in the table below. Method of Object has high availability (less false positive) because it detects only objects whose predicted path is on the lane. However, sometimes it is not safe because perception may fail to detect obstacles or generate incorrect predicted paths. On the other hand, method of Points has high safety (less false negative) because it uses pointcloud as input. Since points don't have a predicted path, the path that moves in the direction normal to the path of ego vehicle is considered to be the predicted path of abstracted dynamic obstacle data. However, without proper adjustment of filter of points, it may detect a lot of points and it will result in very low availability. Method of ObjectWithoutPath has the characteristics of an intermediate of Object and Points . Method Description Object use an object with the predicted path for collision detection. ObjectWithoutPath use an object but not use the predicted path for collision detection. replace the path assuming that an object jumps out to the lane at specified velocity. Points use filtered points for collision detection. the path is created assuming that points jump out to the lane. points are regarded as an small circular shaped obstacle.","title":"3 types of detection method"},{"location":"planning/behavior_velocity_planner/run-out-design/#exclude-obstacles-outside-of-partition","text":"This module can exclude the obstacles outside of partition such as guardrail, fence, and wall. We need lanelet map that has the information of partition to use this feature. By this feature, we can reduce unnecessary deceleration by obstacles that are unlikely to jump out to the lane. You can choose whether to use this feature by parameter of use_partition_lanelet .","title":"Exclude obstacles outside of partition"},{"location":"planning/behavior_velocity_planner/run-out-design/#collision-detection","text":"","title":"Collision detection"},{"location":"planning/behavior_velocity_planner/run-out-design/#detect-collision-with-dynamic-obstacles","text":"Along the ego vehicle path, determine the points where collision detection is to be performed for each detection_span . The travel times to the each points are calculated from the expected target velocity . For the each points, collision detection is performed using the footprint polygon of the ego vehicle and the polygon of the predicted location of the obstacles. The predicted location of the obstacles is described as rectangle or polygon that has the range calculated by min velocity, max velocity and the ego vehicle's travel time to the point. If the input type of the dynamic obstacle is Points , the obstacle shape is defined as a small cylinder. Multiple points are detected as collision points because collision detection is calculated between two polygons. So we select the point that is on the same side as the obstacle and close to ego vehicle as the collision point.","title":"Detect collision with dynamic obstacles"},{"location":"planning/behavior_velocity_planner/run-out-design/#insert-velocity","text":"","title":"Insert velocity"},{"location":"planning/behavior_velocity_planner/run-out-design/#insert-velocity-to-decelerate-for-obstacles","text":"If the collision is detected, stop point is inserted on distance of base link to front + stop margin from the selected collision point. The base link to front means the distance between base_link (center of rear-wheel axis) and front of the car. Stop margin is determined by the parameter of stop_margin .","title":"Insert velocity to decelerate for obstacles"},{"location":"planning/behavior_velocity_planner/run-out-design/#limit-velocity-with-specified-jerk-and-acc-limit","text":"The maximum slowdown velocity is calculated in order not to slowdown too much. See the Occlusion Spot document for more details. You can choose whether to use this feature by parameter of slow_down_limit.enable .","title":"Limit velocity with specified jerk and acc limit"},{"location":"planning/behavior_velocity_planner/run-out-design/#module-parameters","text":"Parameter Type Description detection_method string [-] candidate: Object, ObjectWithoutPath, Points use_partition_lanelet bool [-] whether to use partition lanelet map data specify_decel_jerk bool [-] whether to specify jerk when ego decelerates stop_margin double [m] the vehicle decelerates to be able to stop with this margin passing_margin double [m] the vehicle begins to accelerate if the vehicle's front in predicted position is ahead of the obstacle + this margin deceleration_jerk double [m/s^3] ego decelerates with this jerk when stopping for obstacles obstacle_velocity_kph double [km/h] assumption for obstacle velocity detection_distance double [m] ahead distance from ego to detect the obstacles detection_span double [m] calculate collision with this span to reduce calculation time min_vel_ego_kmph double [km/h] min velocity to calculate time to collision Parameter /detection_area_size Type Description dist_ahead double [m] ahead distance from ego position dist_behind double [m] behind distance from ego position dist_right double [m] right distance from ego position dist_left double [m] left distance from ego position Parameter /dynamic_obstacle Type Description min_vel_kmph double [km/h] minimum velocity for dynamic obstacles max_vel_kmph double [km/h] maximum velocity for dynamic obstacles diameter double [m] diameter of obstacles. used for creating dynamic obstacles from points height double [m] height of obstacles. used for creating dynamic obstacles from points max_prediction_time double [sec] create predicted path until this time time_step double [sec] time step for each path step. used for creating dynamic obstacles from points or objects without path Parameter /approaching Type Description enable bool [-] whether to enable approaching after stopping margin double [m] distance on how close ego approaches the obstacle limit_vel_kmph double [km/h] limit velocity for approaching after stopping stop_thresh double [m/s] threshold to decide if ego is stopping stop_time_thresh double [sec] threshold for stopping time to transit to approaching state dist_thresh double [m] end the approaching state if distance to the obstacle is longer than stop_margin + dist_thresh Parameter /slow_down_limit Type Description enable bool [-] whether to enable to limit velocity with max jerk and acc max_jerk double [m/s^3] minimum jerk deceleration for safe brake. max_acc double [m/s^2] minimum accel deceleration for safe brake.","title":"Module Parameters"},{"location":"planning/behavior_velocity_planner/run-out-design/#future-extensions-unimplemented-parts","text":"Calculate obstacle's min velocity and max velocity from covariance Detect collisions with polygon object Handle the case when the predicted path of obstacles are not straight line Currently collision check is calculated based on the assumption that the predicted path of the obstacle is a straight line","title":"Future extensions / Unimplemented parts"},{"location":"planning/behavior_velocity_planner/stop-line-design/","text":"Stop Line # Role # This module plans velocity so that the vehicle can stop right before stop lines and restart driving after stopped. Activation Timing # This module is activated when there is a stop line in a target lane. Inner-workings / Algorithms # Gets a stop line from map information. insert a stop point on the path from the stop line defined in the map and the ego vehicle length. Sets velocities of the path after the stop point to 0[m/s]. Release the inserted stop velocity when the vehicle stops within a radius of 2[m] from the stop point. Module Parameters # Parameter Type Description stop_margin double a margin that the vehicle tries to stop before stop_line stop_check_dist double when the vehicle is within stop_check_dist from stop_line and stopped, move to STOPPED state Flowchart # This algorithm is based on segment . segment consists of two node points. It's useful for removing boundary conditions because if segment(i) exists we can assume node(i) and node(i+1) exist. First, this algorithm finds a collision between reference path and stop line. Then, we can get collision segment and collision point . Next, based on collision point , it finds offset segment by iterating backward points up to a specific offset length. The offset length is stop_margin (parameter) + base_link to front (to adjust head pose to stop line). Then, we can get offset segment and offset from segment start . After that, we can calculate a offset point from offset segment and offset . This will be stop_pose .","title":"Stop line design"},{"location":"planning/behavior_velocity_planner/stop-line-design/#stop-line","text":"","title":"Stop Line"},{"location":"planning/behavior_velocity_planner/stop-line-design/#role","text":"This module plans velocity so that the vehicle can stop right before stop lines and restart driving after stopped.","title":"Role"},{"location":"planning/behavior_velocity_planner/stop-line-design/#activation-timing","text":"This module is activated when there is a stop line in a target lane.","title":"Activation Timing"},{"location":"planning/behavior_velocity_planner/stop-line-design/#inner-workings-algorithms","text":"Gets a stop line from map information. insert a stop point on the path from the stop line defined in the map and the ego vehicle length. Sets velocities of the path after the stop point to 0[m/s]. Release the inserted stop velocity when the vehicle stops within a radius of 2[m] from the stop point.","title":"Inner-workings / Algorithms"},{"location":"planning/behavior_velocity_planner/stop-line-design/#module-parameters","text":"Parameter Type Description stop_margin double a margin that the vehicle tries to stop before stop_line stop_check_dist double when the vehicle is within stop_check_dist from stop_line and stopped, move to STOPPED state","title":"Module Parameters"},{"location":"planning/behavior_velocity_planner/stop-line-design/#flowchart","text":"This algorithm is based on segment . segment consists of two node points. It's useful for removing boundary conditions because if segment(i) exists we can assume node(i) and node(i+1) exist. First, this algorithm finds a collision between reference path and stop line. Then, we can get collision segment and collision point . Next, based on collision point , it finds offset segment by iterating backward points up to a specific offset length. The offset length is stop_margin (parameter) + base_link to front (to adjust head pose to stop line). Then, we can get offset segment and offset from segment start . After that, we can calculate a offset point from offset segment and offset . This will be stop_pose .","title":"Flowchart"},{"location":"planning/behavior_velocity_planner/traffic-light-design/","text":"Traffic Light # Role # Judgement whether a vehicle can go into an intersection or not by internal and external traffic light status, and planning a velocity of the stop if necessary. This module is designed for rule-based velocity decision that is easy for developers to design its behavior. It generates proper velocity for traffic light scene. In addition, the STOP/GO interface of behavior_velocity_planner allows external users / modules (e.g. remote operation) to intervene the decision of the internal perception. This function is expected to be used, for example, for remote intervention in detection failure or gathering information on operator decisions during development. Limitations # This module allows developers to design STOP/GO in traffic light module using specific rules. Due to the property of rule-based planning, the algorithm is greatly depends on object detection and perception accuracy considering traffic light. Also, this module only handles STOP/Go at traffic light scene, so rushing or quick decision according to traffic condition is future work. Activation Timing # This module is activated when there is traffic light in ego lane. Algorithm # Obtains a traffic light mapped to the route and a stop line correspond to the traffic light from a map information. Uses the highest reliability one of the traffic light recognition result and if the color of that was red, generates a stop point. When vehicle current velocity is higher than 2.0m/s \u21d2 pass judge(using next slide formula) lower than 2.0m/s \u21d2 stop When it to be judged that vehicle can\u2019t stop before stop line, autoware chooses one of the following behaviors \"can pass through\" stop line during yellow lamp => pass \"can\u2019t pass through\" stop line during yellow lamp => emergency stop Dilemma Zone # yellow lamp line It\u2019s called \u201cyellow lamp line\u201d which shows the distance traveled by the vehicle during yellow lamp. dilemma zone It\u2019s called \u201cdilemma zone\u201d which satisfies following conditions: vehicle can\u2019t pass through stop line during yellow lamp.(right side of the yellow lamp line) vehicle can\u2019t stop under deceleration and jerk limit.(left side of the pass judge curve) \u21d2emergency stop(relax deceleration and jerk limitation in order to observe the traffic regulation) optional zone It\u2019s called \u201coptional zone\u201d which satisfies following conditions: vehicle can pass through stop line during yellow lamp.(left side of the yellow lamp line) vehicle can stop under deceleration and jerk limit.(right side of the pass judge curve) \u21d2 stop(autoware selects the safety choice) Module Parameters # Parameter Type Description stop_margin double [m] margin before stop point tl_state_timeout double [s] time out for detected traffic light result. external_tl_state_timeout double [s] time out for external traffic input yellow_lamp_period double [s] time for yellow lamp enable_pass_judge bool [-] weather to use pass judge Flowchart # Known Limits # tbd.","title":"Traffic light design"},{"location":"planning/behavior_velocity_planner/traffic-light-design/#traffic-light","text":"","title":"Traffic Light"},{"location":"planning/behavior_velocity_planner/traffic-light-design/#role","text":"Judgement whether a vehicle can go into an intersection or not by internal and external traffic light status, and planning a velocity of the stop if necessary. This module is designed for rule-based velocity decision that is easy for developers to design its behavior. It generates proper velocity for traffic light scene. In addition, the STOP/GO interface of behavior_velocity_planner allows external users / modules (e.g. remote operation) to intervene the decision of the internal perception. This function is expected to be used, for example, for remote intervention in detection failure or gathering information on operator decisions during development.","title":"Role"},{"location":"planning/behavior_velocity_planner/traffic-light-design/#limitations","text":"This module allows developers to design STOP/GO in traffic light module using specific rules. Due to the property of rule-based planning, the algorithm is greatly depends on object detection and perception accuracy considering traffic light. Also, this module only handles STOP/Go at traffic light scene, so rushing or quick decision according to traffic condition is future work.","title":"Limitations"},{"location":"planning/behavior_velocity_planner/traffic-light-design/#activation-timing","text":"This module is activated when there is traffic light in ego lane.","title":"Activation Timing"},{"location":"planning/behavior_velocity_planner/traffic-light-design/#algorithm","text":"Obtains a traffic light mapped to the route and a stop line correspond to the traffic light from a map information. Uses the highest reliability one of the traffic light recognition result and if the color of that was red, generates a stop point. When vehicle current velocity is higher than 2.0m/s \u21d2 pass judge(using next slide formula) lower than 2.0m/s \u21d2 stop When it to be judged that vehicle can\u2019t stop before stop line, autoware chooses one of the following behaviors \"can pass through\" stop line during yellow lamp => pass \"can\u2019t pass through\" stop line during yellow lamp => emergency stop","title":"Algorithm"},{"location":"planning/behavior_velocity_planner/traffic-light-design/#dilemma-zone","text":"yellow lamp line It\u2019s called \u201cyellow lamp line\u201d which shows the distance traveled by the vehicle during yellow lamp. dilemma zone It\u2019s called \u201cdilemma zone\u201d which satisfies following conditions: vehicle can\u2019t pass through stop line during yellow lamp.(right side of the yellow lamp line) vehicle can\u2019t stop under deceleration and jerk limit.(left side of the pass judge curve) \u21d2emergency stop(relax deceleration and jerk limitation in order to observe the traffic regulation) optional zone It\u2019s called \u201coptional zone\u201d which satisfies following conditions: vehicle can pass through stop line during yellow lamp.(left side of the yellow lamp line) vehicle can stop under deceleration and jerk limit.(right side of the pass judge curve) \u21d2 stop(autoware selects the safety choice)","title":"Dilemma Zone"},{"location":"planning/behavior_velocity_planner/traffic-light-design/#module-parameters","text":"Parameter Type Description stop_margin double [m] margin before stop point tl_state_timeout double [s] time out for detected traffic light result. external_tl_state_timeout double [s] time out for external traffic input yellow_lamp_period double [s] time for yellow lamp enable_pass_judge bool [-] weather to use pass judge","title":"Module Parameters"},{"location":"planning/behavior_velocity_planner/traffic-light-design/#flowchart","text":"","title":"Flowchart"},{"location":"planning/behavior_velocity_planner/traffic-light-design/#known-limits","text":"tbd.","title":"Known Limits"},{"location":"planning/behavior_velocity_planner/virtual-traffic-light-design/","text":"Virtual Traffic Light # Role # Autonomous vehicles have to cooperate with the infrastructures such as: Warehouse shutters Traffic lights with V2X support Communication devices at intersections Fleet Management Systems (FMS) The following items are example cases: Traffic control by traffic lights with V2X support Intersection coordination of multiple vehicles by FMS. It's possible to make each function individually, however, the use cases can be generalized with these three elements. start : Start a cooperation procedure after the vehicle enters a certain zone. stop : Stop at a defined stop line according to the status received from infrastructures. end : Finalize the cooperation procedure after the vehicle reaches the exit zone. This should be done within the range of stable communication. This module sends/receives status from infrastructures and plans the velocity of the cooperation result. System Configuration Diagram # Planner and each infrastructure communicate with each other using common abstracted messages. Special handling for each infrastructure is not scalable. The interface is defined as an Autoware API. The requirements for each infrastructure are slightly different, but will be handled flexibly. FMS: Intersection coordination when multiple vehicles are in operation and the relevant lane is occupied Automatic shutter: Open the shutter when approaching/close it when leaving Manual shutter: Have the driver open and close the shutter. Remote control signal: Have the driver change the signal status to match the direction of travel. Warning light: Activate the warning light Support different communication methods for different infrastructures HTTP Bluetooth ZigBee Have different meta-information for each geographic location Associated lane ID Hardware ID Communication method FMS: Fleet Management System Module Parameters # Parameter Type Description max_delay_sec double [s] maximum allowed delay for command near_line_distance double [m] threshold distance to stop line to check ego stop. dead_line_margin double [m] threshold distance that this module continue to insert stop line. check_timeout_after_stop_line bool [-] check timeout to stop when linkage is disconnected Flowchart # Known Limits # tbd.","title":"Virtual traffic light design"},{"location":"planning/behavior_velocity_planner/virtual-traffic-light-design/#virtual-traffic-light","text":"","title":"Virtual Traffic Light"},{"location":"planning/behavior_velocity_planner/virtual-traffic-light-design/#role","text":"Autonomous vehicles have to cooperate with the infrastructures such as: Warehouse shutters Traffic lights with V2X support Communication devices at intersections Fleet Management Systems (FMS) The following items are example cases: Traffic control by traffic lights with V2X support Intersection coordination of multiple vehicles by FMS. It's possible to make each function individually, however, the use cases can be generalized with these three elements. start : Start a cooperation procedure after the vehicle enters a certain zone. stop : Stop at a defined stop line according to the status received from infrastructures. end : Finalize the cooperation procedure after the vehicle reaches the exit zone. This should be done within the range of stable communication. This module sends/receives status from infrastructures and plans the velocity of the cooperation result.","title":"Role"},{"location":"planning/behavior_velocity_planner/virtual-traffic-light-design/#system-configuration-diagram","text":"Planner and each infrastructure communicate with each other using common abstracted messages. Special handling for each infrastructure is not scalable. The interface is defined as an Autoware API. The requirements for each infrastructure are slightly different, but will be handled flexibly. FMS: Intersection coordination when multiple vehicles are in operation and the relevant lane is occupied Automatic shutter: Open the shutter when approaching/close it when leaving Manual shutter: Have the driver open and close the shutter. Remote control signal: Have the driver change the signal status to match the direction of travel. Warning light: Activate the warning light Support different communication methods for different infrastructures HTTP Bluetooth ZigBee Have different meta-information for each geographic location Associated lane ID Hardware ID Communication method FMS: Fleet Management System","title":"System Configuration Diagram"},{"location":"planning/behavior_velocity_planner/virtual-traffic-light-design/#module-parameters","text":"Parameter Type Description max_delay_sec double [s] maximum allowed delay for command near_line_distance double [m] threshold distance to stop line to check ego stop. dead_line_margin double [m] threshold distance that this module continue to insert stop line. check_timeout_after_stop_line bool [-] check timeout to stop when linkage is disconnected","title":"Module Parameters"},{"location":"planning/behavior_velocity_planner/virtual-traffic-light-design/#flowchart","text":"","title":"Flowchart"},{"location":"planning/behavior_velocity_planner/virtual-traffic-light-design/#known-limits","text":"tbd.","title":"Known Limits"},{"location":"planning/costmap_generator/","text":"costmap_generator # costmap_generator_node # This node reads PointCloud and/or DynamicObjectArray and creates an OccupancyGrid and GridMap . VectorMap(Lanelet2) is optional. Input topics # Name Type Description ~input/objects autoware_auto_perception_msgs::PredictedObjects predicted objects, for obstacles areas ~input/points_no_ground sensor_msgs::PointCloud2 ground-removed points, for obstacle areas which can't be detected as objects ~input/vector_map autoware_auto_mapping_msgs::HADMapBin vector map, for drivable areas ~input/scenario tier4_planning_msgs::Scenario scenarios to be activated, for node activation Output topics # Name Type Description ~output/grid_map grid_map_msgs::GridMap costmap as GridMap, values are from 0.0 to 1.0 ~output/occupancy_grid nav_msgs::OccupancyGrid costmap as OccupancyGrid, values are from 0 to 100 Output TFs # None How to launch # Write your remapping info in costmap_generator.launch or add args when executing roslaunch Run roslaunch costmap_generator costmap_generator.launch Parameters # Name Type Description update_rate double timer's update rate activate_by_scenario bool if true, activate by scenario = parking. Otherwise, activate if vehicle is inside parking lot. use_objects bool whether using ~input/objects or not use_points bool whether using ~input/points_no_ground or not use_wayarea bool whether using wayarea from ~input/vector_map or not costmap_frame string created costmap's coordinate vehicle_frame string vehicle's coordinate map_frame string map's coordinate grid_min_value double minimum cost for gridmap grid_max_value double maximum cost for gridmap grid_resolution double resolution for gridmap grid_length_x int size of gridmap for x direction grid_length_y int size of gridmap for y direction grid_position_x int offset from coordinate in x direction grid_position_y int offset from coordinate in y direction maximum_lidar_height_thres double maximum height threshold for pointcloud data minimum_lidar_height_thres double minimum height threshold for pointcloud data expand_rectangle_size double expand object's rectangle with this value size_of_expansion_kernel int kernel size for blurring effect on object's costmap Flowchart #","title":"costmap_generator"},{"location":"planning/costmap_generator/#costmap_generator","text":"","title":"costmap_generator"},{"location":"planning/costmap_generator/#costmap_generator_node","text":"This node reads PointCloud and/or DynamicObjectArray and creates an OccupancyGrid and GridMap . VectorMap(Lanelet2) is optional.","title":"costmap_generator_node"},{"location":"planning/costmap_generator/#input-topics","text":"Name Type Description ~input/objects autoware_auto_perception_msgs::PredictedObjects predicted objects, for obstacles areas ~input/points_no_ground sensor_msgs::PointCloud2 ground-removed points, for obstacle areas which can't be detected as objects ~input/vector_map autoware_auto_mapping_msgs::HADMapBin vector map, for drivable areas ~input/scenario tier4_planning_msgs::Scenario scenarios to be activated, for node activation","title":"Input topics"},{"location":"planning/costmap_generator/#output-topics","text":"Name Type Description ~output/grid_map grid_map_msgs::GridMap costmap as GridMap, values are from 0.0 to 1.0 ~output/occupancy_grid nav_msgs::OccupancyGrid costmap as OccupancyGrid, values are from 0 to 100","title":"Output topics"},{"location":"planning/costmap_generator/#output-tfs","text":"None","title":"Output TFs"},{"location":"planning/costmap_generator/#how-to-launch","text":"Write your remapping info in costmap_generator.launch or add args when executing roslaunch Run roslaunch costmap_generator costmap_generator.launch","title":"How to launch"},{"location":"planning/costmap_generator/#parameters","text":"Name Type Description update_rate double timer's update rate activate_by_scenario bool if true, activate by scenario = parking. Otherwise, activate if vehicle is inside parking lot. use_objects bool whether using ~input/objects or not use_points bool whether using ~input/points_no_ground or not use_wayarea bool whether using wayarea from ~input/vector_map or not costmap_frame string created costmap's coordinate vehicle_frame string vehicle's coordinate map_frame string map's coordinate grid_min_value double minimum cost for gridmap grid_max_value double maximum cost for gridmap grid_resolution double resolution for gridmap grid_length_x int size of gridmap for x direction grid_length_y int size of gridmap for y direction grid_position_x int offset from coordinate in x direction grid_position_y int offset from coordinate in y direction maximum_lidar_height_thres double maximum height threshold for pointcloud data minimum_lidar_height_thres double minimum height threshold for pointcloud data expand_rectangle_size double expand object's rectangle with this value size_of_expansion_kernel int kernel size for blurring effect on object's costmap","title":"Parameters"},{"location":"planning/costmap_generator/#flowchart","text":"","title":"Flowchart"},{"location":"planning/external_velocity_limit_selector/","text":"External Velocity Limit Selector # Purpose # The external_velocity_limit_selector_node is a node that keeps consistency of external velocity limits. This module subscribes velocity limit command sent by API , velocity limit command sent by Autoware internal modules . VelocityLimit.msg contains not only max velocity but also information about the acceleration/jerk constraints on deceleration. The external_velocity_limit_selector_node integrates the lowest velocity limit and the highest jerk constraint to calculate the hardest velocity limit that protects all the deceleration points and max velocities sent by API and Autoware internal modules. Inner-workings / Algorithms # WIP Inputs # Name Type Description ~input/velocity_limit_from_api tier4_planning_msgs::VelocityLimit velocity limit from api ~input/velocity_limit_from_internal tier4_planning_msgs::VelocityLimit velocity limit from autoware internal modules ~input/velocity_limit_clear_command_from_internal tier4_planning_msgs::VelocityLimitClearCommand velocity limit clear command Outputs # Name Type Description ~output/max_velocity tier4_planning_msgs::VelocityLimit current information of the hardest velocity limit Parameters # Parameter Type Description max_velocity double default max velocity [m/s] normal.min_acc double minimum acceleration [m/ss] normal.max_acc double maximum acceleration [m/ss] normal.min_jerk double minimum jerk [m/sss] normal.max_jerk double maximum jerk [m/sss] limit.min_acc double minimum acceleration to be observed [m/ss] limit.max_acc double maximum acceleration to be observed [m/ss] limit.min_jerk double minimum jerk to be observed [m/sss] limit.max_jerk double maximum jerk to be observed [m/sss] Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"External Velocity Limit Selector"},{"location":"planning/external_velocity_limit_selector/#external-velocity-limit-selector","text":"","title":"External Velocity Limit Selector"},{"location":"planning/external_velocity_limit_selector/#purpose","text":"The external_velocity_limit_selector_node is a node that keeps consistency of external velocity limits. This module subscribes velocity limit command sent by API , velocity limit command sent by Autoware internal modules . VelocityLimit.msg contains not only max velocity but also information about the acceleration/jerk constraints on deceleration. The external_velocity_limit_selector_node integrates the lowest velocity limit and the highest jerk constraint to calculate the hardest velocity limit that protects all the deceleration points and max velocities sent by API and Autoware internal modules.","title":"Purpose"},{"location":"planning/external_velocity_limit_selector/#inner-workings-algorithms","text":"WIP","title":"Inner-workings / Algorithms"},{"location":"planning/external_velocity_limit_selector/#inputs","text":"Name Type Description ~input/velocity_limit_from_api tier4_planning_msgs::VelocityLimit velocity limit from api ~input/velocity_limit_from_internal tier4_planning_msgs::VelocityLimit velocity limit from autoware internal modules ~input/velocity_limit_clear_command_from_internal tier4_planning_msgs::VelocityLimitClearCommand velocity limit clear command","title":"Inputs"},{"location":"planning/external_velocity_limit_selector/#outputs","text":"Name Type Description ~output/max_velocity tier4_planning_msgs::VelocityLimit current information of the hardest velocity limit","title":"Outputs"},{"location":"planning/external_velocity_limit_selector/#parameters","text":"Parameter Type Description max_velocity double default max velocity [m/s] normal.min_acc double minimum acceleration [m/ss] normal.max_acc double maximum acceleration [m/ss] normal.min_jerk double minimum jerk [m/sss] normal.max_jerk double maximum jerk [m/sss] limit.min_acc double minimum acceleration to be observed [m/ss] limit.max_acc double maximum acceleration to be observed [m/ss] limit.min_jerk double minimum jerk to be observed [m/sss] limit.max_jerk double maximum jerk to be observed [m/sss]","title":"Parameters"},{"location":"planning/external_velocity_limit_selector/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"planning/external_velocity_limit_selector/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"planning/external_velocity_limit_selector/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"planning/external_velocity_limit_selector/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"planning/external_velocity_limit_selector/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"planning/freespace_planner/","text":"The freespace_planner # freespace_planner_node # freespace_planner_node is a global path planner node that plans trajectory in the space having static/dynamic obstacles. This node is currently based on Hybrid A* search algorithm in freespace_planning_algorithms package. Other algorithms such as rrt* will be also added and selectable in the future. Note Due to the constraint of trajectory following, the output trajectory will be split to include only the single direction path. In other words, the output trajectory doesn't include both forward and backward trajectories at once. Input topics # Name Type Description ~input/route autoware_auto_planning_msgs::Route route and goal pose ~input/occupancy_grid nav_msgs::OccupancyGrid costmap, for drivable areas ~input/odometry nav_msgs::Odometry vehicle velocity, for checking whether vehicle is stopped ~input/scenario tier4_planning_msgs::Scenario scenarios to be activated, for node activation Output topics # Name Type Description ~output/trajectory autoware_auto_planning_msgs::Trajectory trajectory to be followed is_completed bool (implemented as rosparam) whether all split trajectory are published Output TFs # None How to launch # Write your remapping info in freespace_planner.launch or add args when executing roslaunch roslaunch freespace_planner freespace_planner.launch Parameters # Node parameters # Parameter Type Description update_rate double timer's update rate waypoints_velocity double velocity in output trajectory (currently, only constant velocity is supported) th_arrived_distance_m double threshold distance to check if vehicle has arrived at the trajectory's endpoint th_stopped_time_sec double threshold time to check if vehicle is stopped th_stopped_velocity_mps double threshold velocity to check if vehicle is stopped th_course_out_distance_m double threshold distance to check if vehicle is out of course replan_when_obstacle_found bool whether replanning when obstacle has found on the trajectory replan_when_course_out bool whether replanning when vehicle is out of course Planner common parameters # Parameter Type Description planning_algorithms string algorithms used in the node time_limit double time limit of planning robot_length double robot length robot_width double robot width robot_base2back double distance from robot's back to robot's base_link minimum_turning_radius double minimum turning radius of robot theta_size double the number of angle's discretization lateral_goal_range double goal range of lateral position longitudinal_goal_range double goal range of longitudinal position angle_goal_range double goal range of angle curve_weight double additional cost factor for curve actions reverse_weight double additional cost factor for reverse actions obstacle_threshold double threshold for regarding a certain grid as obstacle A* search parameters # Parameter Type Description only_behind_solutions bool whether restricting the solutions to be behind the goal use_back bool whether using backward trajectory distance_heuristic_weight double heuristic weight for estimating node's cost Flowchart #","title":"The `freespace_planner`"},{"location":"planning/freespace_planner/#the-freespace_planner","text":"","title":"The freespace_planner"},{"location":"planning/freespace_planner/#freespace_planner_node","text":"freespace_planner_node is a global path planner node that plans trajectory in the space having static/dynamic obstacles. This node is currently based on Hybrid A* search algorithm in freespace_planning_algorithms package. Other algorithms such as rrt* will be also added and selectable in the future. Note Due to the constraint of trajectory following, the output trajectory will be split to include only the single direction path. In other words, the output trajectory doesn't include both forward and backward trajectories at once.","title":"freespace_planner_node"},{"location":"planning/freespace_planner/#input-topics","text":"Name Type Description ~input/route autoware_auto_planning_msgs::Route route and goal pose ~input/occupancy_grid nav_msgs::OccupancyGrid costmap, for drivable areas ~input/odometry nav_msgs::Odometry vehicle velocity, for checking whether vehicle is stopped ~input/scenario tier4_planning_msgs::Scenario scenarios to be activated, for node activation","title":"Input topics"},{"location":"planning/freespace_planner/#output-topics","text":"Name Type Description ~output/trajectory autoware_auto_planning_msgs::Trajectory trajectory to be followed is_completed bool (implemented as rosparam) whether all split trajectory are published","title":"Output topics"},{"location":"planning/freespace_planner/#output-tfs","text":"None","title":"Output TFs"},{"location":"planning/freespace_planner/#how-to-launch","text":"Write your remapping info in freespace_planner.launch or add args when executing roslaunch roslaunch freespace_planner freespace_planner.launch","title":"How to launch"},{"location":"planning/freespace_planner/#parameters","text":"","title":"Parameters"},{"location":"planning/freespace_planner/#node-parameters","text":"Parameter Type Description update_rate double timer's update rate waypoints_velocity double velocity in output trajectory (currently, only constant velocity is supported) th_arrived_distance_m double threshold distance to check if vehicle has arrived at the trajectory's endpoint th_stopped_time_sec double threshold time to check if vehicle is stopped th_stopped_velocity_mps double threshold velocity to check if vehicle is stopped th_course_out_distance_m double threshold distance to check if vehicle is out of course replan_when_obstacle_found bool whether replanning when obstacle has found on the trajectory replan_when_course_out bool whether replanning when vehicle is out of course","title":"Node parameters"},{"location":"planning/freespace_planner/#planner-common-parameters","text":"Parameter Type Description planning_algorithms string algorithms used in the node time_limit double time limit of planning robot_length double robot length robot_width double robot width robot_base2back double distance from robot's back to robot's base_link minimum_turning_radius double minimum turning radius of robot theta_size double the number of angle's discretization lateral_goal_range double goal range of lateral position longitudinal_goal_range double goal range of longitudinal position angle_goal_range double goal range of angle curve_weight double additional cost factor for curve actions reverse_weight double additional cost factor for reverse actions obstacle_threshold double threshold for regarding a certain grid as obstacle","title":"Planner common parameters"},{"location":"planning/freespace_planner/#a-search-parameters","text":"Parameter Type Description only_behind_solutions bool whether restricting the solutions to be behind the goal use_back bool whether using backward trajectory distance_heuristic_weight double heuristic weight for estimating node's cost","title":"A* search parameters"},{"location":"planning/freespace_planner/#flowchart","text":"","title":"Flowchart"},{"location":"planning/freespace_planning_algorithms/","text":"freespace planning algorithms # Role # This package is for development of path planning algorithms in free space. Implemented algorithms # Hybrid A* Guide to implement a new algorithm # All planning algorithm class in this package must inherit AbstractPlanningAlgorithm class. If necessary, please overwrite the virtual functions. All algorithms must use nav_msgs::OccupancyGrid -typed costmap. Thus, AbstractPlanningAlgorithm class mainly implements the collision checking using the costmap, grid-based indexing, and coordinate transformation related to costmap. All algorithms must take both PlannerCommonParam -typed and algorithm-specific- type structs as inputs of the constructor. For example, AstarSearch class's constructor takes both PlannerCommonParam and AstarParam . Running the standalone tests and visualization # Building the package with ros-test and run tests: colcon build --packages-select freespace_planning_algorithms colcon test --packages-select freespace_planning_algorithms Inside the test, simulation results are stored in /tmp/result_*.txt . Note that the postfix corresponds to the testing scenario (multiple curvatures and single curvature cases). Loading these resulting files, by using test/debug_plot.py , one can create plots visualizing the path and obstacles as shown in the figures below. The created figures are then again saved in /tmp with the name like /tmp/result_multi0.png . The black cells, green box, and red box, respectively, indicate obstacles, start configuration, and goal configuration. The sequence of the blue boxes indicate the solution path. License notice # Files src/reeds_shepp.cpp and include/astar_search/reeds_shepp.h are fetched from pyReedsShepp . Note that the implementation in pyReedsShepp is also heavily based on the code in ompl . Both pyReedsShepp and ompl are distributed under 3-clause BSD license.","title":"`freespace planning algorithms`"},{"location":"planning/freespace_planning_algorithms/#freespace-planning-algorithms","text":"","title":"freespace planning algorithms"},{"location":"planning/freespace_planning_algorithms/#role","text":"This package is for development of path planning algorithms in free space.","title":"Role"},{"location":"planning/freespace_planning_algorithms/#implemented-algorithms","text":"Hybrid A*","title":"Implemented algorithms"},{"location":"planning/freespace_planning_algorithms/#guide-to-implement-a-new-algorithm","text":"All planning algorithm class in this package must inherit AbstractPlanningAlgorithm class. If necessary, please overwrite the virtual functions. All algorithms must use nav_msgs::OccupancyGrid -typed costmap. Thus, AbstractPlanningAlgorithm class mainly implements the collision checking using the costmap, grid-based indexing, and coordinate transformation related to costmap. All algorithms must take both PlannerCommonParam -typed and algorithm-specific- type structs as inputs of the constructor. For example, AstarSearch class's constructor takes both PlannerCommonParam and AstarParam .","title":"Guide to implement a new algorithm"},{"location":"planning/freespace_planning_algorithms/#running-the-standalone-tests-and-visualization","text":"Building the package with ros-test and run tests: colcon build --packages-select freespace_planning_algorithms colcon test --packages-select freespace_planning_algorithms Inside the test, simulation results are stored in /tmp/result_*.txt . Note that the postfix corresponds to the testing scenario (multiple curvatures and single curvature cases). Loading these resulting files, by using test/debug_plot.py , one can create plots visualizing the path and obstacles as shown in the figures below. The created figures are then again saved in /tmp with the name like /tmp/result_multi0.png . The black cells, green box, and red box, respectively, indicate obstacles, start configuration, and goal configuration. The sequence of the blue boxes indicate the solution path.","title":"Running the standalone tests and visualization"},{"location":"planning/freespace_planning_algorithms/#license-notice","text":"Files src/reeds_shepp.cpp and include/astar_search/reeds_shepp.h are fetched from pyReedsShepp . Note that the implementation in pyReedsShepp is also heavily based on the code in ompl . Both pyReedsShepp and ompl are distributed under 3-clause BSD license.","title":"License notice"},{"location":"planning/mission_planner/","text":"Mission Planner # Purpose # Mission Planner calculates a route that navigates from the current ego pose to the goal pose following the given check points. The route is made of a sequence of lanes on a static map. Dynamic objects (e.g. pedestrians and other vehicles) and dynamic map information (e.g. road construction which blocks some lanes) are not considered during route planning. Therefore, the output topic is only published when the goal pose or check points are given and will be latched until the new goal pose or check points are given. The core implementation does not depend on a map format. In current Autoware.universe, only Lanelet2 map format is supported. Inputs / Outputs # input # Name Type Description ~input/vector_map autoware_auto_mapping_msgs/HADMapBin vector map of Lanelet2 ~input/goal_pose geometry_msgs/PoseStamped goal pose ~input/checkpoints geometry_msgs/PoseStamped checkpoint to follow while heading to goal output # Name Type Description ~output/route autoware_auto_planning_msgs/HADMapRoute route from ego pose to goal tier4_planning_msgs/Route consists of route sections and goal pose. Route section, whose type is autoware_auto_mapping_msgs/HADMapSegment , is a \"slice\" of a road that bundles lane changeable lanes. Note that the most atomic unit of route is autoware_auto_mapping_msgs/MapPrimitive , which has the unique id of a lane in a vector map and its type. Therefore, route message does not contain geometric information about the lane since we did not want to have planning module\u2019s message to have dependency on map data structure. The ROS message of route section contains following three elements for each route section. preferred_primitive_id : Preferred lane to follow towards the goal. primitives : All neighbor lanes in the same direction including the preferred lane. Implementation # Mission Planner # Two callbacks (goal and check points) are a trigger for route planning. Routing graph, which plans route in Lanelet2, must be created before those callbacks, and this routing graph is created in vector map callback. plan route is explained in detail in the following section. Note that during the goal callback, previously memorized check points are removed, and only current ego pose and goal pose are memorized as check points. Note that at least two check points must be already memorized, which are start and goal pose, before the check point callback. Route Planner # plan route is executed with check points including current ego pose and goal pose. plan path between each check points firstly calculates closest lanes to start and goal pose. Then routing graph of Lanelet2 plans the shortest path from start and goal pose. initialize route lanelets initializes route handler, and calculates route_lanelets . route_lanelets , all of which will be registered in route sections, are lanelets next to the lanelets in the planned path, and used when planning lane change. To calculate route_lanelets , All the neighbor (right and left) lanes for the planned path which is lane-changeable is memorized as route_lanelets . All the neighbor (right and left) lanes for the planned path which is not lane-changeable is memorized as candidate_lanelets . If the following and previous lanelets of each candidate_lanelets are route_lanelets , the candidate_lanelet is registered as route_lanelets This is because even though candidate_lanelet (an adjacent lane) is not lane-changeable, we can pass the candidate_lanelet without lane change if the following and previous lanelets of the candidate_lanelet are route_lanelets get preferred lanelets extracts preferred_primitive_id from route_lanelets with the route handler. create route sections extracts primitives from route_lanelets for each route section with the route handler, and creates route sections. Limitations # Dynamic objects (e.g. pedestrians and other vehicles) and dynamic map information (e.g. road construction which blocks some lanes) are not considered during route planning. Looped route is not supported.","title":"Mission Planner"},{"location":"planning/mission_planner/#mission-planner","text":"","title":"Mission Planner"},{"location":"planning/mission_planner/#purpose","text":"Mission Planner calculates a route that navigates from the current ego pose to the goal pose following the given check points. The route is made of a sequence of lanes on a static map. Dynamic objects (e.g. pedestrians and other vehicles) and dynamic map information (e.g. road construction which blocks some lanes) are not considered during route planning. Therefore, the output topic is only published when the goal pose or check points are given and will be latched until the new goal pose or check points are given. The core implementation does not depend on a map format. In current Autoware.universe, only Lanelet2 map format is supported.","title":"Purpose"},{"location":"planning/mission_planner/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"planning/mission_planner/#input","text":"Name Type Description ~input/vector_map autoware_auto_mapping_msgs/HADMapBin vector map of Lanelet2 ~input/goal_pose geometry_msgs/PoseStamped goal pose ~input/checkpoints geometry_msgs/PoseStamped checkpoint to follow while heading to goal","title":"input"},{"location":"planning/mission_planner/#output","text":"Name Type Description ~output/route autoware_auto_planning_msgs/HADMapRoute route from ego pose to goal tier4_planning_msgs/Route consists of route sections and goal pose. Route section, whose type is autoware_auto_mapping_msgs/HADMapSegment , is a \"slice\" of a road that bundles lane changeable lanes. Note that the most atomic unit of route is autoware_auto_mapping_msgs/MapPrimitive , which has the unique id of a lane in a vector map and its type. Therefore, route message does not contain geometric information about the lane since we did not want to have planning module\u2019s message to have dependency on map data structure. The ROS message of route section contains following three elements for each route section. preferred_primitive_id : Preferred lane to follow towards the goal. primitives : All neighbor lanes in the same direction including the preferred lane.","title":"output"},{"location":"planning/mission_planner/#implementation","text":"","title":"Implementation"},{"location":"planning/mission_planner/#mission-planner_1","text":"Two callbacks (goal and check points) are a trigger for route planning. Routing graph, which plans route in Lanelet2, must be created before those callbacks, and this routing graph is created in vector map callback. plan route is explained in detail in the following section. Note that during the goal callback, previously memorized check points are removed, and only current ego pose and goal pose are memorized as check points. Note that at least two check points must be already memorized, which are start and goal pose, before the check point callback.","title":"Mission Planner"},{"location":"planning/mission_planner/#route-planner","text":"plan route is executed with check points including current ego pose and goal pose. plan path between each check points firstly calculates closest lanes to start and goal pose. Then routing graph of Lanelet2 plans the shortest path from start and goal pose. initialize route lanelets initializes route handler, and calculates route_lanelets . route_lanelets , all of which will be registered in route sections, are lanelets next to the lanelets in the planned path, and used when planning lane change. To calculate route_lanelets , All the neighbor (right and left) lanes for the planned path which is lane-changeable is memorized as route_lanelets . All the neighbor (right and left) lanes for the planned path which is not lane-changeable is memorized as candidate_lanelets . If the following and previous lanelets of each candidate_lanelets are route_lanelets , the candidate_lanelet is registered as route_lanelets This is because even though candidate_lanelet (an adjacent lane) is not lane-changeable, we can pass the candidate_lanelet without lane change if the following and previous lanelets of the candidate_lanelet are route_lanelets get preferred lanelets extracts preferred_primitive_id from route_lanelets with the route handler. create route sections extracts primitives from route_lanelets for each route section with the route handler, and creates route sections.","title":"Route Planner"},{"location":"planning/mission_planner/#limitations","text":"Dynamic objects (e.g. pedestrians and other vehicles) and dynamic map information (e.g. road construction which blocks some lanes) are not considered during route planning. Looped route is not supported.","title":"Limitations"},{"location":"planning/motion_velocity_smoother/","text":"Motion Velocity Smoother # Purpose # motion_velocity_smoother outputs a desired velocity profile on a reference trajectory. This module plans a velocity profile within the limitations of the velocity, the acceleration and the jerk to realize both the maximization of velocity and the ride quality. We call this module motion_velocity_smoother because the limitations of the acceleration and the jerk means the smoothness of the velocity profile. Inner-workings / Algorithms # Flow chart # Extract trajectory # For the point on the reference trajectory closest to the center of the rear wheel axle of the vehicle, it extracts the reference path between extract_behind_dist behind and extract_ahead_dist ahead. Apply external velocity limit # It applies the velocity limit input from the external of motion_velocity_smoother . Remark that the external velocity limit is different from the velocity limit already set on the map and the reference trajectory. The external velocity is applied at the position that it is able to reach the velocity limit with the deceleration and the jerk constraints set as the parameter. Apply stop approaching velocity # It applies the velocity limit near the stopping point. This function is used to approach near the obstacle or improve the accuracy of stopping. Apply lateral acceleration limit # It applies the velocity limit to decelerate at the curve. It calculate the velocity limit from the curvature of the reference trajectory and the maximum lateral acceleration max_lateral_accel . The velocity limit is set as not to fall under min_curve_velocity . Resample trajectory # It resamples the points on the reference trajectory with designated time interval. Note that the range of the length of the trajectory is set between min_trajectory_length and max_trajectory_length , and the distance between two points is longer than min_trajectory_interval_distance . It samples densely up to the distance traveled between resample_time with the current velocity, then samples sparsely after that. By sampling according to the velocity, both calculation load and accuracy are achieved since it samples finely at low velocity and coarsely at high velocity. Calculate initial state # Calculate initial values for velocity planning. The initial values are calculated according to the situation as shown in the following table. Situation Initial velocity Initial acceleration First calculation Current velocity 0.0 Engaging engage_velocity engage_acceleration Deviate between the planned velocity and the current velocity Current velocity Previous planned value Normal Previous planned value Previous planned value Smooth velocity # It plans the velocity. The algorithm of velocity planning is chosen from JerkFiltered , L2 and Linf , and it is set in the launch file. In these algorithms, they use OSQP[1] as the solver of the optimization. JerkFiltered # It minimizes the sum of the minus of the square of the velocity and the square of the violation of the velocity limit, the acceleration limit and the jerk limit. L2 # It minimizes the sum of the minus of the square of the velocity, the square of the the pseudo-jerk[2] and the square of the violation of the velocity limit and the acceleration limit. Linf # It minimizes the sum of the minus of the square of the velocity, the maximum absolute value of the the pseudo-jerk[2] and the square of the violation of the velocity limit and the acceleration limit. Post process # It performs the post-process of the planned velocity. Set zero velocity ahead of the stopping point Set maximum velocity given in the config named max_velocity Set velocity behind the current pose Resample trajectory ( post resampling ) Output debug data After the optimization, a resampling called post resampling is performed before passing the optimized trajectory to the next node. Since the required path interval from optimization may be different from the one for the next module, post resampling helps to fill this gap. Therefore, in post resampling , it is necessary to check the path specification of the following module to determine the parameters. Note that if the computational load of the optimization algorithm is high and the path interval is sparser than the path specification of the following module in the first resampling, post resampling would resample the trajectory densely. On the other hand, if the computational load of the optimization algorithm is small and the path interval is denser than the path specification of the following module in the first resampling, the path is sparsely resampled according to the specification of the following module. Inputs / Outputs # Input # Name Type Description ~/input/trajectory autoware_auto_planning_msgs/Trajectory Reference trajectory /planning/scenario_planning/max_velocity std_msgs/Float32 External velocity limit [m/s] /localization/kinematic_state nav_msgs/Odometry Current odometry /tf tf2_msgs/TFMessage TF /tf_static tf2_msgs/TFMessage TF static Output # Name Type Description ~/output/trajectory autoware_auto_planning_msgs/Trajectory Modified trajectory /planning/scenario_planning/current_max_velocity std_msgs/Float32 Current external velocity limit [m/s] ~/closest_velocity std_msgs/Float32 Planned velocity closest to ego base_link (for debug) ~/closest_acceleration std_msgs/Float32 Planned acceleration closest to ego base_link (for debug) ~/closest_jerk std_msgs/Float32 Planned jerk closest to ego base_link (for debug) ~/debug/trajectory_raw autoware_auto_planning_msgs/Trajectory Extracted trajectory (for debug) ~/debug/trajectory_external_velocity_limited autoware_auto_planning_msgs/Trajectory External velocity limited trajectory (for debug) ~/debug/trajectory_lateral_acc_filtered autoware_auto_planning_msgs/Trajectory Lateral acceleration limit filtered trajectory (for debug) ~/debug/trajectory_time_resampled autoware_auto_planning_msgs/Trajectory Time resampled trajectory (for debug) ~/distance_to_stopline std_msgs/Float32 Distance to stop line from current ego pose (max 50 m) (for debug) ~/stop_speed_exceeded std_msgs/Bool It publishes true if planned velocity on the point which the maximum velocity is zero is over threshold Parameters # Constraint parameters # Name Type Description Default value max_velocity double Max velocity limit [m/s] 20.0 max_accel double Max acceleration limit [m/ss] 1.0 min_decel double Min deceleration limit [m/ss] -0.5 stop_decel double Stop deceleration value at a stop point [m/ss] 0.0 max_jerk double Max jerk limit [m/sss] 1.0 min_jerk double Min jerk limit [m/sss] -0.5 External velocity limit parameter # Name Type Description Default value margin_to_insert_external_velocity_limit double margin distance to insert external velocity limit [m] 0.3 Curve parameters # Name Type Description Default value max_lateral_accel double Max lateral acceleration limit [m/ss] 0.5 min_curve_velocity double Min velocity at lateral acceleration limit [m/ss] 2.74 decel_distance_before_curve double Distance to slowdown before a curve for lateral acceleration limit [m] 3.5 decel_distance_after_curve double Distance to slowdown after a curve for lateral acceleration limit [m] 2.0 Engage & replan parameters # Name Type Description Default value replan_vel_deviation double Velocity deviation to replan initial velocity [m/s] 5.53 engage_velocity double Engage velocity threshold [m/s] (if the trajectory velocity is higher than this value, use this velocity for engage vehicle speed) 0.25 engage_acceleration double Engage acceleration [m/ss] (use this acceleration when engagement) 0.1 engage_exit_ratio double Exit engage sequence to normal velocity planning when the velocity exceeds engage_exit_ratio x engage_velocity. 0.5 stop_dist_to_prohibit_engage double If the stop point is in this distance, the speed is set to 0 not to move the vehicle [m] 0.5 Stopping velocity parameters # Name Type Description Default value stopping_velocity double change target velocity to this value before v=0 point [m/s] 2.778 stopping_distance double distance for the stopping_velocity [m]. 0 means the stopping velocity is not applied. 0.0 Extraction parameters # Name Type Description Default value extract_ahead_dist double Forward trajectory distance used for planning [m] 200.0 extract_behind_dist double backward trajectory distance used for planning [m] 5.0 delta_yaw_threshold double Allowed delta yaw between ego pose and trajectory pose [radian] 1.0472 Resampling parameters # Name Type Description Default value max_trajectory_length double Max trajectory length for resampling [m] 200.0 min_trajectory_length double Min trajectory length for resampling [m] 30.0 resample_time double Resample total time [s] 10.0 dense_dt double resample time interval for dense sampling [s] 0.1 dense_min_interval_distance double minimum points-interval length for dense sampling [m] 0.1 sparse_dt double resample time interval for sparse sampling [s] 0.5 sparse_min_interval_distance double minimum points-interval length for sparse sampling [m] 4.0 Resampling parameters for post process # Name Type Description Default value post_max_trajectory_length double max trajectory length for resampling [m] 300.0 post_min_trajectory_length double min trajectory length for resampling [m] 30.0 post_resample_time double resample total time for dense sampling [s] 10.0 post_dense_dt double resample time interval for dense sampling [s] 0.1 post_dense_min_interval_distance double minimum points-interval length for dense sampling [m] 0.1 post_sparse_dt double resample time interval for sparse sampling [s] 0.1 post_sparse_min_interval_distance double minimum points-interval length for sparse sampling [m] 1.0 Weights for optimization # JerkFiltered # Name Type Description Default value jerk_weight double Weight for \"smoothness\" cost for jerk 10.0 over_v_weight double Weight for \"over speed limit\" cost 100000.0 over_a_weight double Weight for \"over accel limit\" cost 5000.0 over_j_weight double Weight for \"over jerk limit\" cost 1000.0 L2 # Name Type Description Default value pseudo_jerk_weight double Weight for \"smoothness\" cost 100.0 over_v_weight double Weight for \"over speed limit\" cost 100000.0 over_a_weight double Weight for \"over accel limit\" cost 1000.0 Linf # Name Type Description Default value pseudo_jerk_weight double Weight for \"smoothness\" cost 100.0 over_v_weight double Weight for \"over speed limit\" cost 100000.0 over_a_weight double Weight for \"over accel limit\" cost 1000.0 Others # Name Type Description Default value over_stop_velocity_warn_thr double Threshold to judge that the optimized velocity exceeds the input velocity on the stop point [m/s] 1.389 Assumptions / Known limits # Assume that the velocity limit or the stopping point is properly set at the point on the reference trajectory If the velocity limit set in the reference path cannot be achieved by the designated constraints of the deceleration and the jerk, decelerate while suppressing the velocity, the acceleration and the jerk deviation as much as possible The importance of the deviations is set in the config file (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # [1] B. Stellato, et al., \"OSQP: an operator splitting solver for quadratic programs\", Mathematical Programming Computation, 2020, 10.1007/s12532-020-00179-2 . [2] Y. Zhang, et al., \"Toward a More Complete, Flexible, and Safer Speed Planning for Autonomous Driving via Convex Optimization\", Sensors, vol. 18, no. 7, p. 2185, 2018, 10.3390/s18072185 (Optional) Future extensions / Unimplemented parts #","title":"Motion Velocity Smoother"},{"location":"planning/motion_velocity_smoother/#motion-velocity-smoother","text":"","title":"Motion Velocity Smoother"},{"location":"planning/motion_velocity_smoother/#purpose","text":"motion_velocity_smoother outputs a desired velocity profile on a reference trajectory. This module plans a velocity profile within the limitations of the velocity, the acceleration and the jerk to realize both the maximization of velocity and the ride quality. We call this module motion_velocity_smoother because the limitations of the acceleration and the jerk means the smoothness of the velocity profile.","title":"Purpose"},{"location":"planning/motion_velocity_smoother/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"planning/motion_velocity_smoother/#flow-chart","text":"","title":"Flow chart"},{"location":"planning/motion_velocity_smoother/#extract-trajectory","text":"For the point on the reference trajectory closest to the center of the rear wheel axle of the vehicle, it extracts the reference path between extract_behind_dist behind and extract_ahead_dist ahead.","title":"Extract trajectory"},{"location":"planning/motion_velocity_smoother/#apply-external-velocity-limit","text":"It applies the velocity limit input from the external of motion_velocity_smoother . Remark that the external velocity limit is different from the velocity limit already set on the map and the reference trajectory. The external velocity is applied at the position that it is able to reach the velocity limit with the deceleration and the jerk constraints set as the parameter.","title":"Apply external velocity limit"},{"location":"planning/motion_velocity_smoother/#apply-stop-approaching-velocity","text":"It applies the velocity limit near the stopping point. This function is used to approach near the obstacle or improve the accuracy of stopping.","title":"Apply stop approaching velocity"},{"location":"planning/motion_velocity_smoother/#apply-lateral-acceleration-limit","text":"It applies the velocity limit to decelerate at the curve. It calculate the velocity limit from the curvature of the reference trajectory and the maximum lateral acceleration max_lateral_accel . The velocity limit is set as not to fall under min_curve_velocity .","title":"Apply lateral acceleration limit"},{"location":"planning/motion_velocity_smoother/#resample-trajectory","text":"It resamples the points on the reference trajectory with designated time interval. Note that the range of the length of the trajectory is set between min_trajectory_length and max_trajectory_length , and the distance between two points is longer than min_trajectory_interval_distance . It samples densely up to the distance traveled between resample_time with the current velocity, then samples sparsely after that. By sampling according to the velocity, both calculation load and accuracy are achieved since it samples finely at low velocity and coarsely at high velocity.","title":"Resample trajectory"},{"location":"planning/motion_velocity_smoother/#calculate-initial-state","text":"Calculate initial values for velocity planning. The initial values are calculated according to the situation as shown in the following table. Situation Initial velocity Initial acceleration First calculation Current velocity 0.0 Engaging engage_velocity engage_acceleration Deviate between the planned velocity and the current velocity Current velocity Previous planned value Normal Previous planned value Previous planned value","title":"Calculate initial state"},{"location":"planning/motion_velocity_smoother/#smooth-velocity","text":"It plans the velocity. The algorithm of velocity planning is chosen from JerkFiltered , L2 and Linf , and it is set in the launch file. In these algorithms, they use OSQP[1] as the solver of the optimization.","title":"Smooth velocity"},{"location":"planning/motion_velocity_smoother/#jerkfiltered","text":"It minimizes the sum of the minus of the square of the velocity and the square of the violation of the velocity limit, the acceleration limit and the jerk limit.","title":"JerkFiltered"},{"location":"planning/motion_velocity_smoother/#l2","text":"It minimizes the sum of the minus of the square of the velocity, the square of the the pseudo-jerk[2] and the square of the violation of the velocity limit and the acceleration limit.","title":"L2"},{"location":"planning/motion_velocity_smoother/#linf","text":"It minimizes the sum of the minus of the square of the velocity, the maximum absolute value of the the pseudo-jerk[2] and the square of the violation of the velocity limit and the acceleration limit.","title":"Linf"},{"location":"planning/motion_velocity_smoother/#post-process","text":"It performs the post-process of the planned velocity. Set zero velocity ahead of the stopping point Set maximum velocity given in the config named max_velocity Set velocity behind the current pose Resample trajectory ( post resampling ) Output debug data After the optimization, a resampling called post resampling is performed before passing the optimized trajectory to the next node. Since the required path interval from optimization may be different from the one for the next module, post resampling helps to fill this gap. Therefore, in post resampling , it is necessary to check the path specification of the following module to determine the parameters. Note that if the computational load of the optimization algorithm is high and the path interval is sparser than the path specification of the following module in the first resampling, post resampling would resample the trajectory densely. On the other hand, if the computational load of the optimization algorithm is small and the path interval is denser than the path specification of the following module in the first resampling, the path is sparsely resampled according to the specification of the following module.","title":"Post process"},{"location":"planning/motion_velocity_smoother/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"planning/motion_velocity_smoother/#input","text":"Name Type Description ~/input/trajectory autoware_auto_planning_msgs/Trajectory Reference trajectory /planning/scenario_planning/max_velocity std_msgs/Float32 External velocity limit [m/s] /localization/kinematic_state nav_msgs/Odometry Current odometry /tf tf2_msgs/TFMessage TF /tf_static tf2_msgs/TFMessage TF static","title":"Input"},{"location":"planning/motion_velocity_smoother/#output","text":"Name Type Description ~/output/trajectory autoware_auto_planning_msgs/Trajectory Modified trajectory /planning/scenario_planning/current_max_velocity std_msgs/Float32 Current external velocity limit [m/s] ~/closest_velocity std_msgs/Float32 Planned velocity closest to ego base_link (for debug) ~/closest_acceleration std_msgs/Float32 Planned acceleration closest to ego base_link (for debug) ~/closest_jerk std_msgs/Float32 Planned jerk closest to ego base_link (for debug) ~/debug/trajectory_raw autoware_auto_planning_msgs/Trajectory Extracted trajectory (for debug) ~/debug/trajectory_external_velocity_limited autoware_auto_planning_msgs/Trajectory External velocity limited trajectory (for debug) ~/debug/trajectory_lateral_acc_filtered autoware_auto_planning_msgs/Trajectory Lateral acceleration limit filtered trajectory (for debug) ~/debug/trajectory_time_resampled autoware_auto_planning_msgs/Trajectory Time resampled trajectory (for debug) ~/distance_to_stopline std_msgs/Float32 Distance to stop line from current ego pose (max 50 m) (for debug) ~/stop_speed_exceeded std_msgs/Bool It publishes true if planned velocity on the point which the maximum velocity is zero is over threshold","title":"Output"},{"location":"planning/motion_velocity_smoother/#parameters","text":"","title":"Parameters"},{"location":"planning/motion_velocity_smoother/#constraint-parameters","text":"Name Type Description Default value max_velocity double Max velocity limit [m/s] 20.0 max_accel double Max acceleration limit [m/ss] 1.0 min_decel double Min deceleration limit [m/ss] -0.5 stop_decel double Stop deceleration value at a stop point [m/ss] 0.0 max_jerk double Max jerk limit [m/sss] 1.0 min_jerk double Min jerk limit [m/sss] -0.5","title":"Constraint parameters"},{"location":"planning/motion_velocity_smoother/#external-velocity-limit-parameter","text":"Name Type Description Default value margin_to_insert_external_velocity_limit double margin distance to insert external velocity limit [m] 0.3","title":"External velocity limit parameter"},{"location":"planning/motion_velocity_smoother/#curve-parameters","text":"Name Type Description Default value max_lateral_accel double Max lateral acceleration limit [m/ss] 0.5 min_curve_velocity double Min velocity at lateral acceleration limit [m/ss] 2.74 decel_distance_before_curve double Distance to slowdown before a curve for lateral acceleration limit [m] 3.5 decel_distance_after_curve double Distance to slowdown after a curve for lateral acceleration limit [m] 2.0","title":"Curve parameters"},{"location":"planning/motion_velocity_smoother/#engage-replan-parameters","text":"Name Type Description Default value replan_vel_deviation double Velocity deviation to replan initial velocity [m/s] 5.53 engage_velocity double Engage velocity threshold [m/s] (if the trajectory velocity is higher than this value, use this velocity for engage vehicle speed) 0.25 engage_acceleration double Engage acceleration [m/ss] (use this acceleration when engagement) 0.1 engage_exit_ratio double Exit engage sequence to normal velocity planning when the velocity exceeds engage_exit_ratio x engage_velocity. 0.5 stop_dist_to_prohibit_engage double If the stop point is in this distance, the speed is set to 0 not to move the vehicle [m] 0.5","title":"Engage &amp; replan parameters"},{"location":"planning/motion_velocity_smoother/#stopping-velocity-parameters","text":"Name Type Description Default value stopping_velocity double change target velocity to this value before v=0 point [m/s] 2.778 stopping_distance double distance for the stopping_velocity [m]. 0 means the stopping velocity is not applied. 0.0","title":"Stopping velocity parameters"},{"location":"planning/motion_velocity_smoother/#extraction-parameters","text":"Name Type Description Default value extract_ahead_dist double Forward trajectory distance used for planning [m] 200.0 extract_behind_dist double backward trajectory distance used for planning [m] 5.0 delta_yaw_threshold double Allowed delta yaw between ego pose and trajectory pose [radian] 1.0472","title":"Extraction parameters"},{"location":"planning/motion_velocity_smoother/#resampling-parameters","text":"Name Type Description Default value max_trajectory_length double Max trajectory length for resampling [m] 200.0 min_trajectory_length double Min trajectory length for resampling [m] 30.0 resample_time double Resample total time [s] 10.0 dense_dt double resample time interval for dense sampling [s] 0.1 dense_min_interval_distance double minimum points-interval length for dense sampling [m] 0.1 sparse_dt double resample time interval for sparse sampling [s] 0.5 sparse_min_interval_distance double minimum points-interval length for sparse sampling [m] 4.0","title":"Resampling parameters"},{"location":"planning/motion_velocity_smoother/#resampling-parameters-for-post-process","text":"Name Type Description Default value post_max_trajectory_length double max trajectory length for resampling [m] 300.0 post_min_trajectory_length double min trajectory length for resampling [m] 30.0 post_resample_time double resample total time for dense sampling [s] 10.0 post_dense_dt double resample time interval for dense sampling [s] 0.1 post_dense_min_interval_distance double minimum points-interval length for dense sampling [m] 0.1 post_sparse_dt double resample time interval for sparse sampling [s] 0.1 post_sparse_min_interval_distance double minimum points-interval length for sparse sampling [m] 1.0","title":"Resampling parameters for post process"},{"location":"planning/motion_velocity_smoother/#weights-for-optimization","text":"","title":"Weights for optimization"},{"location":"planning/motion_velocity_smoother/#jerkfiltered_1","text":"Name Type Description Default value jerk_weight double Weight for \"smoothness\" cost for jerk 10.0 over_v_weight double Weight for \"over speed limit\" cost 100000.0 over_a_weight double Weight for \"over accel limit\" cost 5000.0 over_j_weight double Weight for \"over jerk limit\" cost 1000.0","title":"JerkFiltered"},{"location":"planning/motion_velocity_smoother/#l2_1","text":"Name Type Description Default value pseudo_jerk_weight double Weight for \"smoothness\" cost 100.0 over_v_weight double Weight for \"over speed limit\" cost 100000.0 over_a_weight double Weight for \"over accel limit\" cost 1000.0","title":"L2"},{"location":"planning/motion_velocity_smoother/#linf_1","text":"Name Type Description Default value pseudo_jerk_weight double Weight for \"smoothness\" cost 100.0 over_v_weight double Weight for \"over speed limit\" cost 100000.0 over_a_weight double Weight for \"over accel limit\" cost 1000.0","title":"Linf"},{"location":"planning/motion_velocity_smoother/#others","text":"Name Type Description Default value over_stop_velocity_warn_thr double Threshold to judge that the optimized velocity exceeds the input velocity on the stop point [m/s] 1.389","title":"Others"},{"location":"planning/motion_velocity_smoother/#assumptions-known-limits","text":"Assume that the velocity limit or the stopping point is properly set at the point on the reference trajectory If the velocity limit set in the reference path cannot be achieved by the designated constraints of the deceleration and the jerk, decelerate while suppressing the velocity, the acceleration and the jerk deviation as much as possible The importance of the deviations is set in the config file","title":"Assumptions / Known limits"},{"location":"planning/motion_velocity_smoother/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"planning/motion_velocity_smoother/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"planning/motion_velocity_smoother/#optional-referencesexternal-links","text":"[1] B. Stellato, et al., \"OSQP: an operator splitting solver for quadratic programs\", Mathematical Programming Computation, 2020, 10.1007/s12532-020-00179-2 . [2] Y. Zhang, et al., \"Toward a More Complete, Flexible, and Safer Speed Planning for Autonomous Driving via Convex Optimization\", Sensors, vol. 18, no. 7, p. 2185, 2018, 10.3390/s18072185","title":"(Optional) References/External links"},{"location":"planning/motion_velocity_smoother/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"planning/motion_velocity_smoother/README.ja/","text":"Motion Velocity Smoother # Purpose # motion_velocity_smoother \u306f\u76ee\u6a19\u8ecc\u9053\u4e0a\u306e\u5404\u70b9\u306b\u304a\u3051\u308b\u671b\u307e\u3057\u3044\u8eca\u901f\u3092\u8a08\u753b\u3057\u3066\u51fa\u529b\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3042\u308b\u3002 \u3053\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u306f\u3001\u901f\u5ea6\u306e\u6700\u5927\u5316\u3068\u4e57\u308a\u5fc3\u5730\u306e\u826f\u3055\u3092\u4e21\u7acb\u3059\u308b\u305f\u3081\u306b\u3001\u4e8b\u524d\u306b\u6307\u5b9a\u3055\u308c\u305f\u5236\u9650\u901f\u5ea6\u3001\u5236\u9650\u52a0\u901f\u5ea6\u304a\u3088\u3073\u5236\u9650\u8e8d\u5ea6\u306e\u7bc4\u56f2\u3067\u8eca\u901f\u3092\u8a08\u753b\u3059\u308b\u3002 \u52a0\u901f\u5ea6\u3084\u8e8d\u5ea6\u306e\u5236\u9650\u3092\u4e0e\u3048\u308b\u3053\u3068\u306f\u8eca\u901f\u306e\u5909\u5316\u3092\u6ed1\u3089\u304b\u306b\u3059\u308b\u3053\u3068\u306b\u5bfe\u5fdc\u3059\u308b\u305f\u3081\u3001\u3053\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u3092 motion_velocity_smoother \u3068\u547c\u3093\u3067\u3044\u308b\u3002 Inner-workings / Algorithms # Flow chart # Extract trajectory # \u81ea\u8eca\u5f8c\u8f2a\u8ef8\u4e2d\u5fc3\u4f4d\u7f6e\u306b\u6700\u3082\u8fd1\u3044\u53c2\u7167\u7d4c\u8def\u4e0a\u306e\u70b9\u306b\u5bfe\u3057\u3001 extract_behind_dist \u3060\u3051\u623b\u3063\u305f\u70b9\u304b\u3089 extract_ahead_dist \u3060\u3051\u9032\u3093\u3060\u70b9\u307e\u3067\u306e\u53c2\u7167\u7d4c\u8def\u3092\u629c\u304d\u51fa\u3059\u3002 Apply external velocity limit # \u30e2\u30b8\u30e5\u30fc\u30eb\u5916\u90e8\u304b\u3089\u6307\u5b9a\u3055\u308c\u305f\u901f\u5ea6\u5236\u9650\u3092\u9069\u7528\u3059\u308b\u3002 \u3053\u3053\u3067\u6271\u3046\u5916\u90e8\u306e\u901f\u5ea6\u5236\u9650\u306f /planning/scenario_planning/max_velocity \u306e topic \u3067\u6e21\u3055\u308c\u308b\u3082\u306e\u3067\u3001\u5730\u56f3\u4e0a\u3067\u8a2d\u5b9a\u3055\u308c\u305f\u901f\u5ea6\u5236\u9650\u306a\u3069\u3001\u53c2\u7167\u7d4c\u8def\u306b\u3059\u3067\u306b\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u5236\u9650\u901f\u5ea6\u3068\u306f\u5225\u3067\u3042\u308b\u3002 \u5916\u90e8\u304b\u3089\u6307\u5b9a\u3055\u308c\u308b\u901f\u5ea6\u5236\u9650\u306f\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u6307\u5b9a\u3055\u308c\u3066\u3044\u308b\u6e1b\u901f\u5ea6\u304a\u3088\u3073\u8e8d\u5ea6\u306e\u5236\u9650\u306e\u7bc4\u56f2\u3067\u6e1b\u901f\u53ef\u80fd\u306a\u4f4d\u7f6e\u304b\u3089\u901f\u5ea6\u5236\u9650\u3092\u9069\u7528\u3059\u308b\u3002 Apply stop approaching velocity # \u505c\u6b62\u70b9\u306b\u8fd1\u3065\u3044\u305f\u3068\u304d\u306e\u901f\u5ea6\u3092\u8a2d\u5b9a\u3059\u308b\u3002\u969c\u5bb3\u7269\u8fd1\u508d\u307e\u3067\u8fd1\u3065\u304f\u5834\u5408\u3084\u3001\u6b63\u7740\u7cbe\u5ea6\u5411\u4e0a\u306a\u3069\u306e\u76ee\u7684\u306b\u7528\u3044\u308b\u3002 Apply lateral acceleration limit # \u7d4c\u8def\u306e\u66f2\u7387\u306b\u5fdc\u3058\u3066\u3001\u6307\u5b9a\u3055\u308c\u305f\u6700\u5927\u6a2a\u52a0\u901f\u5ea6 max_lateral_accel \u3092\u8d85\u3048\u306a\u3044\u901f\u5ea6\u3092\u5236\u9650\u901f\u5ea6\u3068\u3057\u3066\u8a2d\u5b9a\u3059\u308b\u3002\u305f\u3060\u3057\u3001\u5236\u9650\u901f\u5ea6\u306f min_curve_velocity \u3092\u4e0b\u56de\u3089\u306a\u3044\u3088\u3046\u306b\u8a2d\u5b9a\u3059\u308b\u3002 Resample trajectory # \u6307\u5b9a\u3055\u308c\u305f\u6642\u9593\u9593\u9694\u3067\u7d4c\u8def\u306e\u70b9\u3092\u518d\u30b5\u30f3\u30d7\u30eb\u3059\u308b\u3002\u305f\u3060\u3057\u3001\u7d4c\u8def\u5168\u4f53\u306e\u9577\u3055\u306f min_trajectory_length \u304b\u3089 max_trajectory_length \u306e\u9593\u3068\u306a\u308b\u3088\u3046\u306b\u518d\u30b5\u30f3\u30d7\u30eb\u3092\u884c\u3044\u3001\u70b9\u306e\u9593\u9694\u306f min_trajectory_interval_distance \u3088\u308a\u5c0f\u3055\u304f\u306a\u3089\u306a\u3044\u3088\u3046\u306b\u3059\u308b\u3002 \u73fe\u5728\u8eca\u901f\u3067 resample_time \u306e\u9593\u9032\u3080\u8ddd\u96e2\u307e\u3067\u306f\u5bc6\u306b\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3001\u305d\u308c\u4ee5\u964d\u306f\u758e\u306b\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3002 \u3053\u306e\u65b9\u6cd5\u3067\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3053\u3068\u3067\u3001\u4f4e\u901f\u6642\u306f\u5bc6\u306b\u3001\u9ad8\u901f\u6642\u306f\u758e\u306b\u30b5\u30f3\u30d7\u30eb\u3055\u308c\u308b\u305f\u3081\u3001\u505c\u6b62\u7cbe\u5ea6\u3068\u8a08\u7b97\u8ca0\u8377\u8efd\u6e1b\u306e\u4e21\u7acb\u3092\u56f3\u3063\u3066\u3044\u308b\u3002 Calculate initial state # \u901f\u5ea6\u8a08\u753b\u306e\u305f\u3081\u306e\u521d\u671f\u5024\u3092\u8a08\u7b97\u3059\u308b\u3002\u521d\u671f\u5024\u306f\u72b6\u6cc1\u306b\u5fdc\u3058\u3066\u4e0b\u8868\u306e\u3088\u3046\u306b\u8a08\u7b97\u3059\u308b\u3002 \u72b6\u6cc1 \u521d\u671f\u901f\u5ea6 \u521d\u671f\u52a0\u901f\u5ea6 \u6700\u521d\u306e\u8a08\u7b97\u6642 \u73fe\u5728\u8eca\u901f 0.0 \u767a\u9032\u6642 engage_velocity engage_acceleration \u73fe\u5728\u8eca\u901f\u3068\u8a08\u753b\u8eca\u901f\u304c\u4e56\u96e2 \u73fe\u5728\u8eca\u901f \u524d\u56de\u8a08\u753b\u5024 \u901a\u5e38\u6642 \u524d\u56de\u8a08\u753b\u5024 \u524d\u56de\u8a08\u753b\u5024 Smooth velocity # \u901f\u5ea6\u306e\u8a08\u753b\u3092\u884c\u3046\u3002\u901f\u5ea6\u8a08\u753b\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f JerkFiltered , L2 , Linf \u306e 3 \u7a2e\u985e\u306e\u3046\u3061\u304b\u3089\u30b3\u30f3\u30d5\u30a3\u30b0\u3067\u6307\u5b9a\u3059\u308b\u3002 \u6700\u9069\u5316\u306e\u30bd\u30eb\u30d0\u306f OSQP[1]\u3092\u5229\u7528\u3059\u308b\u3002 JerkFiltered # \u901f\u5ea6\u306e 2 \u4e57\uff08\u6700\u5c0f\u5316\u3067\u8868\u3059\u305f\u3081\u8ca0\u5024\u3067\u8868\u73fe\uff09\u3001\u5236\u9650\u901f\u5ea6\u9038\u8131\u91cf\u306e 2 \u4e57\u3001\u5236\u9650\u52a0\u5ea6\u9038\u8131\u91cf\u306e 2 \u4e57\u3001\u5236\u9650\u30b8\u30e3\u30fc\u30af\u9038\u8131\u91cf\u306e 2 \u4e57\u3001\u30b8\u30e3\u30fc\u30af\u306e 2 \u4e57\u306e\u7dcf\u548c\u3092\u6700\u5c0f\u5316\u3059\u308b\u3002 L2 # \u901f\u5ea6\u306e 2 \u4e57\uff08\u6700\u5c0f\u5316\u3067\u8868\u3059\u305f\u3081\u8ca0\u5024\u3067\u8868\u73fe\uff09\u3001\u5236\u9650\u901f\u5ea6\u9038\u8131\u91cf\u306e 2 \u4e57\u3001\u5236\u9650\u52a0\u5ea6\u9038\u8131\u91cf\u306e 2 \u4e57\u3001\u7591\u4f3c\u30b8\u30e3\u30fc\u30af[2]\u306e 2 \u4e57\u306e\u7dcf\u548c\u3092\u6700\u5c0f\u5316\u3059\u308b\u3002 Linf # \u901f\u5ea6\u306e 2 \u4e57\uff08\u6700\u5c0f\u5316\u3067\u8868\u3059\u305f\u3081\u8ca0\u5024\u3067\u8868\u73fe\uff09\u3001\u5236\u9650\u901f\u5ea6\u9038\u8131\u91cf\u306e 2 \u4e57\u3001\u5236\u9650\u52a0\u5ea6\u9038\u8131\u91cf\u306e 2 \u4e57\u306e\u7dcf\u548c\u3068\u7591\u4f3c\u30b8\u30e3\u30fc\u30af[2]\u306e\u7d76\u5bfe\u6700\u5927\u5024\u306e\u548c\u306e\u6700\u5c0f\u5316\u3059\u308b\u3002 Post process # \u8a08\u753b\u3055\u308c\u305f\u8ecc\u9053\u306e\u5f8c\u51e6\u7406\u3092\u884c\u3046\u3002 \u505c\u6b62\u70b9\u3088\u308a\u5148\u306e\u901f\u5ea6\u3092 0 \u306b\u8a2d\u5b9a \u901f\u5ea6\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u4e0e\u3048\u3089\u308c\u308b max_velocity \u4ee5\u4e0b\u3068\u306a\u308b\u3088\u3046\u306b\u8a2d\u5b9a \u81ea\u8eca\u4f4d\u7f6e\u3088\u308a\u624b\u524d\u306e\u70b9\u306b\u304a\u3051\u308b\u901f\u5ea6\u3092\u8a2d\u5b9a Trajectory \u306e\u4ed5\u69d8\u306b\u5408\u308f\u305b\u3066\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0( post resampling ) \u30c7\u30d0\u30c3\u30b0\u30c7\u30fc\u30bf\u306e\u51fa\u529b \u6700\u9069\u5316\u306e\u8a08\u7b97\u304c\u7d42\u308f\u3063\u305f\u3042\u3068\u3001\u6b21\u306e\u30ce\u30fc\u30c9\u306b\u7d4c\u8def\u3092\u6e21\u3059\u524d\u306b post resampling \u3068\u547c\u3070\u308c\u308b\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3092\u884c\u3046\u3002\u3053\u3053\u3067\u518d\u5ea6\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3092\u884c\u3063\u3066\u3044\u308b\u7406\u7531\u3068\u3057\u3066\u306f\u3001\u6700\u9069\u5316\u524d\u3067\u5fc5\u8981\u306a\u7d4c\u8def\u9593\u9694\u3068\u5f8c\u6bb5\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u306b\u6e21\u3059\u7d4c\u8def\u9593\u9694\u304c\u5fc5\u305a\u3057\u3082\u4e00\u81f4\u3057\u3066\u3044\u306a\u3044\u304b\u3089\u3067\u3042\u308a\u3001\u305d\u306e\u5dee\u3092\u57cb\u3081\u308b\u305f\u3081\u306b\u518d\u5ea6\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3092\u884c\u3063\u3066\u3044\u308b\u3002\u305d\u306e\u305f\u3081\u3001 post resampling \u3067\u306f\u5f8c\u6bb5\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u7d4c\u8def\u4ed5\u69d8\u3092\u78ba\u8a8d\u3057\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6c7a\u3081\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002\u306a\u304a\u3001\u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u8a08\u7b97\u8ca0\u8377\u304c\u9ad8\u304f\u3001\u6700\u521d\u306e\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3067\u7d4c\u8def\u9593\u9694\u304c\u5f8c\u6bb5\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u7d4c\u8def\u4ed5\u69d8\u3088\u308a\u758e\u306b\u306a\u3063\u3066\u3044\u308b\u5834\u5408\u3001 post resampling \u3067\u7d4c\u8def\u3092\u871c\u306b\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3002\u9006\u306b\u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u8a08\u7b97\u8ca0\u8377\u304c\u5c0f\u3055\u304f\u3001\u6700\u521d\u306e\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3067\u7d4c\u8def\u9593\u9694\u304c\u5f8c\u6bb5\u306e\u7d4c\u8def\u4ed5\u69d8\u3088\u308a\u871c\u306b\u306a\u3063\u3066\u3044\u308b\u5834\u5408\u306f\u3001 post resampling \u3067\u7d4c\u8def\u3092\u305d\u306e\u4ed5\u69d8\u306b\u5408\u308f\u305b\u3066\u758e\u306b\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3002 Inputs / Outputs # Input # Name Type Description ~/input/trajectory autoware_auto_planning_msgs/Trajectory Reference trajectory /planning/scenario_planning/max_velocity std_msgs/Float32 External velocity limit [m/s] /localization/kinematic_state nav_msgs/Odometry Current odometry /tf tf2_msgs/TFMessage TF /tf_static tf2_msgs/TFMessage TF static Output # Name Type Description ~/output/trajectory autoware_auto_planning_msgs/Trajectory Modified trajectory /planning/scenario_planning/current_max_velocity std_msgs/Float32 Current external velocity limit [m/s] ~/closest_velocity std_msgs/Float32 Planned velocity closest to ego base_link (for debug) ~/closest_acceleration std_msgs/Float32 Planned acceleration closest to ego base_link (for debug) ~/closest_jerk std_msgs/Float32 Planned jerk closest to ego base_link (for debug) ~/debug/trajectory_raw autoware_auto_planning_msgs/Trajectory Extracted trajectory (for debug) ~/debug/trajectory_external_velocity_limited autoware_auto_planning_msgs/Trajectory External velocity limited trajectory (for debug) ~/debug/trajectory_lateral_acc_filtered autoware_auto_planning_msgs/Trajectory Lateral acceleration limit filtered trajectory (for debug) ~/debug/trajectory_time_resampled autoware_auto_planning_msgs/Trajectory Time resampled trajectory (for debug) ~/distance_to_stopline std_msgs/Float32 Distance to stop line from current ego pose (max 50 m) (for debug) ~/stop_speed_exceeded std_msgs/Bool It publishes true if planned velocity on the point which the maximum velocity is zero is over threshold Parameters # Constraint parameters # Name Type Description Default value max_velocity double Max velocity limit [m/s] 20.0 max_accel double Max acceleration limit [m/ss] 1.0 min_decel double Min deceleration limit [m/ss] -0.5 stop_decel double Stop deceleration value at a stop point [m/ss] 0.0 max_jerk double Max jerk limit [m/sss] 1.0 min_jerk double Min jerk limit [m/sss] -0.5 External velocity limit parameter # Name Type Description Default value margin_to_insert_external_velocity_limit double margin distance to insert external velocity limit [m] 0.3 Curve parameters # Name Type Description Default value max_lateral_accel double Max lateral acceleration limit [m/ss] 0.5 min_curve_velocity double Min velocity at lateral acceleration limit [m/ss] 2.74 decel_distance_before_curve double Distance to slowdown before a curve for lateral acceleration limit [m] 3.5 decel_distance_after_curve double Distance to slowdown after a curve for lateral acceleration limit [m] 2.0 Engage & replan parameters # Name Type Description Default value replan_vel_deviation double Velocity deviation to replan initial velocity [m/s] 5.53 engage_velocity double Engage velocity threshold [m/s] (if the trajectory velocity is higher than this value, use this velocity for engage vehicle speed) 0.25 engage_acceleration double Engage acceleration [m/ss] (use this acceleration when engagement) 0.1 engage_exit_ratio double Exit engage sequence to normal velocity planning when the velocity exceeds engage_exit_ratio x engage_velocity. 0.5 stop_dist_to_prohibit_engage double If the stop point is in this distance, the speed is set to 0 not to move the vehicle [m] 0.5 Stopping velocity parameters # Name Type Description Default value stopping_velocity double change target velocity to this value before v=0 point [m/s] 2.778 stopping_distance double distance for the stopping_velocity [m]. 0 means the stopping velocity is not applied. 0.0 Extraction parameters # Name Type Description Default value extract_ahead_dist double Forward trajectory distance used for planning [m] 200.0 extract_behind_dist double backward trajectory distance used for planning [m] 5.0 delta_yaw_threshold double Allowed delta yaw between ego pose and trajectory pose [radian] 1.0472 Resampling parameters # Name Type Description Default value max_trajectory_length double Max trajectory length for resampling [m] 200.0 min_trajectory_length double Min trajectory length for resampling [m] 30.0 resample_time double Resample total time [s] 10.0 dense_dt double resample time interval for dense sampling [s] 0.1 dense_min_interval_distance double minimum points-interval length for dense sampling [m] 0.1 sparse_dt double resample time interval for sparse sampling [s] 0.5 sparse_min_interval_distance double minimum points-interval length for sparse sampling [m] 4.0 Resampling parameters for post process # Name Type Description Default value post_max_trajectory_length double max trajectory length for resampling [m] 300.0 post_min_trajectory_length double min trajectory length for resampling [m] 30.0 post_resample_time double resample total time for dense sampling [s] 10.0 post_dense_dt double resample time interval for dense sampling [s] 0.1 post_dense_min_interval_distance double minimum points-interval length for dense sampling [m] 0.1 post_sparse_dt double resample time interval for sparse sampling [s] 0.1 post_sparse_min_interval_distance double minimum points-interval length for sparse sampling [m] 1.0 Weights for optimization # JerkFiltered # Name Type Description Default value jerk_weight double Weight for \"smoothness\" cost for jerk 10.0 over_v_weight double Weight for \"over speed limit\" cost 100000.0 over_a_weight double Weight for \"over accel limit\" cost 5000.0 over_j_weight double Weight for \"over jerk limit\" cost 1000.0 L2 # Name Type Description Default value pseudo_jerk_weight double Weight for \"smoothness\" cost 100.0 over_v_weight double Weight for \"over speed limit\" cost 100000.0 over_a_weight double Weight for \"over accel limit\" cost 1000.0 Linf # Name Type Description Default value pseudo_jerk_weight double Weight for \"smoothness\" cost 100.0 over_v_weight double Weight for \"over speed limit\" cost 100000.0 over_a_weight double Weight for \"over accel limit\" cost 1000.0 Others # Name Type Description Default value over_stop_velocity_warn_thr double Threshold to judge that the optimized velocity exceeds the input velocity on the stop point [m/s] 1.389 Assumptions / Known limits # \u53c2\u7167\u7d4c\u8def\u4e0a\u306e\u70b9\u306b\u306f\u5236\u9650\u901f\u5ea6\uff08\u505c\u6b62\u70b9\uff09\u304c\u6b63\u3057\u304f\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u4eee\u5b9a \u53c2\u7167\u7d4c\u8def\u306b\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u5236\u9650\u901f\u5ea6\u3092\u6307\u5b9a\u3057\u305f\u6e1b\u901f\u5ea6\u3084\u30b8\u30e3\u30fc\u30af\u3067\u9054\u6210\u4e0d\u53ef\u80fd\u306a\u5834\u5408\u3001\u53ef\u80fd\u306a\u7bc4\u56f2\u3067\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\u3001\u30b8\u30e3\u30fc\u30af\u306e\u9038\u8131\u91cf\u3092\u6291\u3048\u306a\u304c\u3089\u6e1b\u901f \u5404\u9038\u8131\u91cf\u306e\u91cd\u8996\u306e\u5ea6\u5408\u3044\u306f\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u3088\u308a\u6307\u5b9a (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # [1] B. Stellato, et al., \"OSQP: an operator splitting solver for quadratic programs\", Mathematical Programming Computation, 2020, 10.1007/s12532-020-00179-2 . [2] Y. Zhang, et al., \"Toward a More Complete, Flexible, and Safer Speed Planning for Autonomous Driving via Convex Optimization\", Sensors, vol. 18, no. 7, p. 2185, 2018, 10.3390/s18072185 (Optional) Future extensions / Unimplemented parts #","title":"Motion Velocity Smoother"},{"location":"planning/motion_velocity_smoother/README.ja/#motion-velocity-smoother","text":"","title":"Motion Velocity Smoother"},{"location":"planning/motion_velocity_smoother/README.ja/#purpose","text":"motion_velocity_smoother \u306f\u76ee\u6a19\u8ecc\u9053\u4e0a\u306e\u5404\u70b9\u306b\u304a\u3051\u308b\u671b\u307e\u3057\u3044\u8eca\u901f\u3092\u8a08\u753b\u3057\u3066\u51fa\u529b\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3042\u308b\u3002 \u3053\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u306f\u3001\u901f\u5ea6\u306e\u6700\u5927\u5316\u3068\u4e57\u308a\u5fc3\u5730\u306e\u826f\u3055\u3092\u4e21\u7acb\u3059\u308b\u305f\u3081\u306b\u3001\u4e8b\u524d\u306b\u6307\u5b9a\u3055\u308c\u305f\u5236\u9650\u901f\u5ea6\u3001\u5236\u9650\u52a0\u901f\u5ea6\u304a\u3088\u3073\u5236\u9650\u8e8d\u5ea6\u306e\u7bc4\u56f2\u3067\u8eca\u901f\u3092\u8a08\u753b\u3059\u308b\u3002 \u52a0\u901f\u5ea6\u3084\u8e8d\u5ea6\u306e\u5236\u9650\u3092\u4e0e\u3048\u308b\u3053\u3068\u306f\u8eca\u901f\u306e\u5909\u5316\u3092\u6ed1\u3089\u304b\u306b\u3059\u308b\u3053\u3068\u306b\u5bfe\u5fdc\u3059\u308b\u305f\u3081\u3001\u3053\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u3092 motion_velocity_smoother \u3068\u547c\u3093\u3067\u3044\u308b\u3002","title":"Purpose"},{"location":"planning/motion_velocity_smoother/README.ja/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"planning/motion_velocity_smoother/README.ja/#flow-chart","text":"","title":"Flow chart"},{"location":"planning/motion_velocity_smoother/README.ja/#extract-trajectory","text":"\u81ea\u8eca\u5f8c\u8f2a\u8ef8\u4e2d\u5fc3\u4f4d\u7f6e\u306b\u6700\u3082\u8fd1\u3044\u53c2\u7167\u7d4c\u8def\u4e0a\u306e\u70b9\u306b\u5bfe\u3057\u3001 extract_behind_dist \u3060\u3051\u623b\u3063\u305f\u70b9\u304b\u3089 extract_ahead_dist \u3060\u3051\u9032\u3093\u3060\u70b9\u307e\u3067\u306e\u53c2\u7167\u7d4c\u8def\u3092\u629c\u304d\u51fa\u3059\u3002","title":"Extract trajectory"},{"location":"planning/motion_velocity_smoother/README.ja/#apply-external-velocity-limit","text":"\u30e2\u30b8\u30e5\u30fc\u30eb\u5916\u90e8\u304b\u3089\u6307\u5b9a\u3055\u308c\u305f\u901f\u5ea6\u5236\u9650\u3092\u9069\u7528\u3059\u308b\u3002 \u3053\u3053\u3067\u6271\u3046\u5916\u90e8\u306e\u901f\u5ea6\u5236\u9650\u306f /planning/scenario_planning/max_velocity \u306e topic \u3067\u6e21\u3055\u308c\u308b\u3082\u306e\u3067\u3001\u5730\u56f3\u4e0a\u3067\u8a2d\u5b9a\u3055\u308c\u305f\u901f\u5ea6\u5236\u9650\u306a\u3069\u3001\u53c2\u7167\u7d4c\u8def\u306b\u3059\u3067\u306b\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u5236\u9650\u901f\u5ea6\u3068\u306f\u5225\u3067\u3042\u308b\u3002 \u5916\u90e8\u304b\u3089\u6307\u5b9a\u3055\u308c\u308b\u901f\u5ea6\u5236\u9650\u306f\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u6307\u5b9a\u3055\u308c\u3066\u3044\u308b\u6e1b\u901f\u5ea6\u304a\u3088\u3073\u8e8d\u5ea6\u306e\u5236\u9650\u306e\u7bc4\u56f2\u3067\u6e1b\u901f\u53ef\u80fd\u306a\u4f4d\u7f6e\u304b\u3089\u901f\u5ea6\u5236\u9650\u3092\u9069\u7528\u3059\u308b\u3002","title":"Apply external velocity limit"},{"location":"planning/motion_velocity_smoother/README.ja/#apply-stop-approaching-velocity","text":"\u505c\u6b62\u70b9\u306b\u8fd1\u3065\u3044\u305f\u3068\u304d\u306e\u901f\u5ea6\u3092\u8a2d\u5b9a\u3059\u308b\u3002\u969c\u5bb3\u7269\u8fd1\u508d\u307e\u3067\u8fd1\u3065\u304f\u5834\u5408\u3084\u3001\u6b63\u7740\u7cbe\u5ea6\u5411\u4e0a\u306a\u3069\u306e\u76ee\u7684\u306b\u7528\u3044\u308b\u3002","title":"Apply stop approaching velocity"},{"location":"planning/motion_velocity_smoother/README.ja/#apply-lateral-acceleration-limit","text":"\u7d4c\u8def\u306e\u66f2\u7387\u306b\u5fdc\u3058\u3066\u3001\u6307\u5b9a\u3055\u308c\u305f\u6700\u5927\u6a2a\u52a0\u901f\u5ea6 max_lateral_accel \u3092\u8d85\u3048\u306a\u3044\u901f\u5ea6\u3092\u5236\u9650\u901f\u5ea6\u3068\u3057\u3066\u8a2d\u5b9a\u3059\u308b\u3002\u305f\u3060\u3057\u3001\u5236\u9650\u901f\u5ea6\u306f min_curve_velocity \u3092\u4e0b\u56de\u3089\u306a\u3044\u3088\u3046\u306b\u8a2d\u5b9a\u3059\u308b\u3002","title":"Apply lateral acceleration limit"},{"location":"planning/motion_velocity_smoother/README.ja/#resample-trajectory","text":"\u6307\u5b9a\u3055\u308c\u305f\u6642\u9593\u9593\u9694\u3067\u7d4c\u8def\u306e\u70b9\u3092\u518d\u30b5\u30f3\u30d7\u30eb\u3059\u308b\u3002\u305f\u3060\u3057\u3001\u7d4c\u8def\u5168\u4f53\u306e\u9577\u3055\u306f min_trajectory_length \u304b\u3089 max_trajectory_length \u306e\u9593\u3068\u306a\u308b\u3088\u3046\u306b\u518d\u30b5\u30f3\u30d7\u30eb\u3092\u884c\u3044\u3001\u70b9\u306e\u9593\u9694\u306f min_trajectory_interval_distance \u3088\u308a\u5c0f\u3055\u304f\u306a\u3089\u306a\u3044\u3088\u3046\u306b\u3059\u308b\u3002 \u73fe\u5728\u8eca\u901f\u3067 resample_time \u306e\u9593\u9032\u3080\u8ddd\u96e2\u307e\u3067\u306f\u5bc6\u306b\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3001\u305d\u308c\u4ee5\u964d\u306f\u758e\u306b\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3002 \u3053\u306e\u65b9\u6cd5\u3067\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3053\u3068\u3067\u3001\u4f4e\u901f\u6642\u306f\u5bc6\u306b\u3001\u9ad8\u901f\u6642\u306f\u758e\u306b\u30b5\u30f3\u30d7\u30eb\u3055\u308c\u308b\u305f\u3081\u3001\u505c\u6b62\u7cbe\u5ea6\u3068\u8a08\u7b97\u8ca0\u8377\u8efd\u6e1b\u306e\u4e21\u7acb\u3092\u56f3\u3063\u3066\u3044\u308b\u3002","title":"Resample trajectory"},{"location":"planning/motion_velocity_smoother/README.ja/#calculate-initial-state","text":"\u901f\u5ea6\u8a08\u753b\u306e\u305f\u3081\u306e\u521d\u671f\u5024\u3092\u8a08\u7b97\u3059\u308b\u3002\u521d\u671f\u5024\u306f\u72b6\u6cc1\u306b\u5fdc\u3058\u3066\u4e0b\u8868\u306e\u3088\u3046\u306b\u8a08\u7b97\u3059\u308b\u3002 \u72b6\u6cc1 \u521d\u671f\u901f\u5ea6 \u521d\u671f\u52a0\u901f\u5ea6 \u6700\u521d\u306e\u8a08\u7b97\u6642 \u73fe\u5728\u8eca\u901f 0.0 \u767a\u9032\u6642 engage_velocity engage_acceleration \u73fe\u5728\u8eca\u901f\u3068\u8a08\u753b\u8eca\u901f\u304c\u4e56\u96e2 \u73fe\u5728\u8eca\u901f \u524d\u56de\u8a08\u753b\u5024 \u901a\u5e38\u6642 \u524d\u56de\u8a08\u753b\u5024 \u524d\u56de\u8a08\u753b\u5024","title":"Calculate initial state"},{"location":"planning/motion_velocity_smoother/README.ja/#smooth-velocity","text":"\u901f\u5ea6\u306e\u8a08\u753b\u3092\u884c\u3046\u3002\u901f\u5ea6\u8a08\u753b\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f JerkFiltered , L2 , Linf \u306e 3 \u7a2e\u985e\u306e\u3046\u3061\u304b\u3089\u30b3\u30f3\u30d5\u30a3\u30b0\u3067\u6307\u5b9a\u3059\u308b\u3002 \u6700\u9069\u5316\u306e\u30bd\u30eb\u30d0\u306f OSQP[1]\u3092\u5229\u7528\u3059\u308b\u3002","title":"Smooth velocity"},{"location":"planning/motion_velocity_smoother/README.ja/#jerkfiltered","text":"\u901f\u5ea6\u306e 2 \u4e57\uff08\u6700\u5c0f\u5316\u3067\u8868\u3059\u305f\u3081\u8ca0\u5024\u3067\u8868\u73fe\uff09\u3001\u5236\u9650\u901f\u5ea6\u9038\u8131\u91cf\u306e 2 \u4e57\u3001\u5236\u9650\u52a0\u5ea6\u9038\u8131\u91cf\u306e 2 \u4e57\u3001\u5236\u9650\u30b8\u30e3\u30fc\u30af\u9038\u8131\u91cf\u306e 2 \u4e57\u3001\u30b8\u30e3\u30fc\u30af\u306e 2 \u4e57\u306e\u7dcf\u548c\u3092\u6700\u5c0f\u5316\u3059\u308b\u3002","title":"JerkFiltered"},{"location":"planning/motion_velocity_smoother/README.ja/#l2","text":"\u901f\u5ea6\u306e 2 \u4e57\uff08\u6700\u5c0f\u5316\u3067\u8868\u3059\u305f\u3081\u8ca0\u5024\u3067\u8868\u73fe\uff09\u3001\u5236\u9650\u901f\u5ea6\u9038\u8131\u91cf\u306e 2 \u4e57\u3001\u5236\u9650\u52a0\u5ea6\u9038\u8131\u91cf\u306e 2 \u4e57\u3001\u7591\u4f3c\u30b8\u30e3\u30fc\u30af[2]\u306e 2 \u4e57\u306e\u7dcf\u548c\u3092\u6700\u5c0f\u5316\u3059\u308b\u3002","title":"L2"},{"location":"planning/motion_velocity_smoother/README.ja/#linf","text":"\u901f\u5ea6\u306e 2 \u4e57\uff08\u6700\u5c0f\u5316\u3067\u8868\u3059\u305f\u3081\u8ca0\u5024\u3067\u8868\u73fe\uff09\u3001\u5236\u9650\u901f\u5ea6\u9038\u8131\u91cf\u306e 2 \u4e57\u3001\u5236\u9650\u52a0\u5ea6\u9038\u8131\u91cf\u306e 2 \u4e57\u306e\u7dcf\u548c\u3068\u7591\u4f3c\u30b8\u30e3\u30fc\u30af[2]\u306e\u7d76\u5bfe\u6700\u5927\u5024\u306e\u548c\u306e\u6700\u5c0f\u5316\u3059\u308b\u3002","title":"Linf"},{"location":"planning/motion_velocity_smoother/README.ja/#post-process","text":"\u8a08\u753b\u3055\u308c\u305f\u8ecc\u9053\u306e\u5f8c\u51e6\u7406\u3092\u884c\u3046\u3002 \u505c\u6b62\u70b9\u3088\u308a\u5148\u306e\u901f\u5ea6\u3092 0 \u306b\u8a2d\u5b9a \u901f\u5ea6\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u4e0e\u3048\u3089\u308c\u308b max_velocity \u4ee5\u4e0b\u3068\u306a\u308b\u3088\u3046\u306b\u8a2d\u5b9a \u81ea\u8eca\u4f4d\u7f6e\u3088\u308a\u624b\u524d\u306e\u70b9\u306b\u304a\u3051\u308b\u901f\u5ea6\u3092\u8a2d\u5b9a Trajectory \u306e\u4ed5\u69d8\u306b\u5408\u308f\u305b\u3066\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0( post resampling ) \u30c7\u30d0\u30c3\u30b0\u30c7\u30fc\u30bf\u306e\u51fa\u529b \u6700\u9069\u5316\u306e\u8a08\u7b97\u304c\u7d42\u308f\u3063\u305f\u3042\u3068\u3001\u6b21\u306e\u30ce\u30fc\u30c9\u306b\u7d4c\u8def\u3092\u6e21\u3059\u524d\u306b post resampling \u3068\u547c\u3070\u308c\u308b\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3092\u884c\u3046\u3002\u3053\u3053\u3067\u518d\u5ea6\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3092\u884c\u3063\u3066\u3044\u308b\u7406\u7531\u3068\u3057\u3066\u306f\u3001\u6700\u9069\u5316\u524d\u3067\u5fc5\u8981\u306a\u7d4c\u8def\u9593\u9694\u3068\u5f8c\u6bb5\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u306b\u6e21\u3059\u7d4c\u8def\u9593\u9694\u304c\u5fc5\u305a\u3057\u3082\u4e00\u81f4\u3057\u3066\u3044\u306a\u3044\u304b\u3089\u3067\u3042\u308a\u3001\u305d\u306e\u5dee\u3092\u57cb\u3081\u308b\u305f\u3081\u306b\u518d\u5ea6\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3092\u884c\u3063\u3066\u3044\u308b\u3002\u305d\u306e\u305f\u3081\u3001 post resampling \u3067\u306f\u5f8c\u6bb5\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u7d4c\u8def\u4ed5\u69d8\u3092\u78ba\u8a8d\u3057\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6c7a\u3081\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002\u306a\u304a\u3001\u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u8a08\u7b97\u8ca0\u8377\u304c\u9ad8\u304f\u3001\u6700\u521d\u306e\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3067\u7d4c\u8def\u9593\u9694\u304c\u5f8c\u6bb5\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u7d4c\u8def\u4ed5\u69d8\u3088\u308a\u758e\u306b\u306a\u3063\u3066\u3044\u308b\u5834\u5408\u3001 post resampling \u3067\u7d4c\u8def\u3092\u871c\u306b\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3002\u9006\u306b\u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u8a08\u7b97\u8ca0\u8377\u304c\u5c0f\u3055\u304f\u3001\u6700\u521d\u306e\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3067\u7d4c\u8def\u9593\u9694\u304c\u5f8c\u6bb5\u306e\u7d4c\u8def\u4ed5\u69d8\u3088\u308a\u871c\u306b\u306a\u3063\u3066\u3044\u308b\u5834\u5408\u306f\u3001 post resampling \u3067\u7d4c\u8def\u3092\u305d\u306e\u4ed5\u69d8\u306b\u5408\u308f\u305b\u3066\u758e\u306b\u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3002","title":"Post process"},{"location":"planning/motion_velocity_smoother/README.ja/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"planning/motion_velocity_smoother/README.ja/#input","text":"Name Type Description ~/input/trajectory autoware_auto_planning_msgs/Trajectory Reference trajectory /planning/scenario_planning/max_velocity std_msgs/Float32 External velocity limit [m/s] /localization/kinematic_state nav_msgs/Odometry Current odometry /tf tf2_msgs/TFMessage TF /tf_static tf2_msgs/TFMessage TF static","title":"Input"},{"location":"planning/motion_velocity_smoother/README.ja/#output","text":"Name Type Description ~/output/trajectory autoware_auto_planning_msgs/Trajectory Modified trajectory /planning/scenario_planning/current_max_velocity std_msgs/Float32 Current external velocity limit [m/s] ~/closest_velocity std_msgs/Float32 Planned velocity closest to ego base_link (for debug) ~/closest_acceleration std_msgs/Float32 Planned acceleration closest to ego base_link (for debug) ~/closest_jerk std_msgs/Float32 Planned jerk closest to ego base_link (for debug) ~/debug/trajectory_raw autoware_auto_planning_msgs/Trajectory Extracted trajectory (for debug) ~/debug/trajectory_external_velocity_limited autoware_auto_planning_msgs/Trajectory External velocity limited trajectory (for debug) ~/debug/trajectory_lateral_acc_filtered autoware_auto_planning_msgs/Trajectory Lateral acceleration limit filtered trajectory (for debug) ~/debug/trajectory_time_resampled autoware_auto_planning_msgs/Trajectory Time resampled trajectory (for debug) ~/distance_to_stopline std_msgs/Float32 Distance to stop line from current ego pose (max 50 m) (for debug) ~/stop_speed_exceeded std_msgs/Bool It publishes true if planned velocity on the point which the maximum velocity is zero is over threshold","title":"Output"},{"location":"planning/motion_velocity_smoother/README.ja/#parameters","text":"","title":"Parameters"},{"location":"planning/motion_velocity_smoother/README.ja/#constraint-parameters","text":"Name Type Description Default value max_velocity double Max velocity limit [m/s] 20.0 max_accel double Max acceleration limit [m/ss] 1.0 min_decel double Min deceleration limit [m/ss] -0.5 stop_decel double Stop deceleration value at a stop point [m/ss] 0.0 max_jerk double Max jerk limit [m/sss] 1.0 min_jerk double Min jerk limit [m/sss] -0.5","title":"Constraint parameters"},{"location":"planning/motion_velocity_smoother/README.ja/#external-velocity-limit-parameter","text":"Name Type Description Default value margin_to_insert_external_velocity_limit double margin distance to insert external velocity limit [m] 0.3","title":"External velocity limit parameter"},{"location":"planning/motion_velocity_smoother/README.ja/#curve-parameters","text":"Name Type Description Default value max_lateral_accel double Max lateral acceleration limit [m/ss] 0.5 min_curve_velocity double Min velocity at lateral acceleration limit [m/ss] 2.74 decel_distance_before_curve double Distance to slowdown before a curve for lateral acceleration limit [m] 3.5 decel_distance_after_curve double Distance to slowdown after a curve for lateral acceleration limit [m] 2.0","title":"Curve parameters"},{"location":"planning/motion_velocity_smoother/README.ja/#engage-replan-parameters","text":"Name Type Description Default value replan_vel_deviation double Velocity deviation to replan initial velocity [m/s] 5.53 engage_velocity double Engage velocity threshold [m/s] (if the trajectory velocity is higher than this value, use this velocity for engage vehicle speed) 0.25 engage_acceleration double Engage acceleration [m/ss] (use this acceleration when engagement) 0.1 engage_exit_ratio double Exit engage sequence to normal velocity planning when the velocity exceeds engage_exit_ratio x engage_velocity. 0.5 stop_dist_to_prohibit_engage double If the stop point is in this distance, the speed is set to 0 not to move the vehicle [m] 0.5","title":"Engage &amp; replan parameters"},{"location":"planning/motion_velocity_smoother/README.ja/#stopping-velocity-parameters","text":"Name Type Description Default value stopping_velocity double change target velocity to this value before v=0 point [m/s] 2.778 stopping_distance double distance for the stopping_velocity [m]. 0 means the stopping velocity is not applied. 0.0","title":"Stopping velocity parameters"},{"location":"planning/motion_velocity_smoother/README.ja/#extraction-parameters","text":"Name Type Description Default value extract_ahead_dist double Forward trajectory distance used for planning [m] 200.0 extract_behind_dist double backward trajectory distance used for planning [m] 5.0 delta_yaw_threshold double Allowed delta yaw between ego pose and trajectory pose [radian] 1.0472","title":"Extraction parameters"},{"location":"planning/motion_velocity_smoother/README.ja/#resampling-parameters","text":"Name Type Description Default value max_trajectory_length double Max trajectory length for resampling [m] 200.0 min_trajectory_length double Min trajectory length for resampling [m] 30.0 resample_time double Resample total time [s] 10.0 dense_dt double resample time interval for dense sampling [s] 0.1 dense_min_interval_distance double minimum points-interval length for dense sampling [m] 0.1 sparse_dt double resample time interval for sparse sampling [s] 0.5 sparse_min_interval_distance double minimum points-interval length for sparse sampling [m] 4.0","title":"Resampling parameters"},{"location":"planning/motion_velocity_smoother/README.ja/#resampling-parameters-for-post-process","text":"Name Type Description Default value post_max_trajectory_length double max trajectory length for resampling [m] 300.0 post_min_trajectory_length double min trajectory length for resampling [m] 30.0 post_resample_time double resample total time for dense sampling [s] 10.0 post_dense_dt double resample time interval for dense sampling [s] 0.1 post_dense_min_interval_distance double minimum points-interval length for dense sampling [m] 0.1 post_sparse_dt double resample time interval for sparse sampling [s] 0.1 post_sparse_min_interval_distance double minimum points-interval length for sparse sampling [m] 1.0","title":"Resampling parameters for post process"},{"location":"planning/motion_velocity_smoother/README.ja/#weights-for-optimization","text":"","title":"Weights for optimization"},{"location":"planning/motion_velocity_smoother/README.ja/#jerkfiltered_1","text":"Name Type Description Default value jerk_weight double Weight for \"smoothness\" cost for jerk 10.0 over_v_weight double Weight for \"over speed limit\" cost 100000.0 over_a_weight double Weight for \"over accel limit\" cost 5000.0 over_j_weight double Weight for \"over jerk limit\" cost 1000.0","title":"JerkFiltered"},{"location":"planning/motion_velocity_smoother/README.ja/#l2_1","text":"Name Type Description Default value pseudo_jerk_weight double Weight for \"smoothness\" cost 100.0 over_v_weight double Weight for \"over speed limit\" cost 100000.0 over_a_weight double Weight for \"over accel limit\" cost 1000.0","title":"L2"},{"location":"planning/motion_velocity_smoother/README.ja/#linf_1","text":"Name Type Description Default value pseudo_jerk_weight double Weight for \"smoothness\" cost 100.0 over_v_weight double Weight for \"over speed limit\" cost 100000.0 over_a_weight double Weight for \"over accel limit\" cost 1000.0","title":"Linf"},{"location":"planning/motion_velocity_smoother/README.ja/#others","text":"Name Type Description Default value over_stop_velocity_warn_thr double Threshold to judge that the optimized velocity exceeds the input velocity on the stop point [m/s] 1.389","title":"Others"},{"location":"planning/motion_velocity_smoother/README.ja/#assumptions-known-limits","text":"\u53c2\u7167\u7d4c\u8def\u4e0a\u306e\u70b9\u306b\u306f\u5236\u9650\u901f\u5ea6\uff08\u505c\u6b62\u70b9\uff09\u304c\u6b63\u3057\u304f\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u4eee\u5b9a \u53c2\u7167\u7d4c\u8def\u306b\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u5236\u9650\u901f\u5ea6\u3092\u6307\u5b9a\u3057\u305f\u6e1b\u901f\u5ea6\u3084\u30b8\u30e3\u30fc\u30af\u3067\u9054\u6210\u4e0d\u53ef\u80fd\u306a\u5834\u5408\u3001\u53ef\u80fd\u306a\u7bc4\u56f2\u3067\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\u3001\u30b8\u30e3\u30fc\u30af\u306e\u9038\u8131\u91cf\u3092\u6291\u3048\u306a\u304c\u3089\u6e1b\u901f \u5404\u9038\u8131\u91cf\u306e\u91cd\u8996\u306e\u5ea6\u5408\u3044\u306f\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u3088\u308a\u6307\u5b9a","title":"Assumptions / Known limits"},{"location":"planning/motion_velocity_smoother/README.ja/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"planning/motion_velocity_smoother/README.ja/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"planning/motion_velocity_smoother/README.ja/#optional-referencesexternal-links","text":"[1] B. Stellato, et al., \"OSQP: an operator splitting solver for quadratic programs\", Mathematical Programming Computation, 2020, 10.1007/s12532-020-00179-2 . [2] Y. Zhang, et al., \"Toward a More Complete, Flexible, and Safer Speed Planning for Autonomous Driving via Convex Optimization\", Sensors, vol. 18, no. 7, p. 2185, 2018, 10.3390/s18072185","title":"(Optional) References/External links"},{"location":"planning/motion_velocity_smoother/README.ja/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"planning/obstacle_avoidance_planner/","text":"Purpose # This package generates a trajectory that is feasible to drive and collision free based on a reference path, drivable area, and static/dynamic obstacles. Only position and orientation of trajectory are calculated in this module (velocity is just aligned from the one in the path), and velocity or acceleration will be updated in the latter modules. Feature # This package is able to follow the behavior path smoothly make the trajectory inside the drivable area as much as possible insert stop point if its trajectory point is outside the drivable area Inputs / Outputs # input # Name Type Description ~/input/path autoware_auto_planning_msgs/Path Reference path and the corresponding drivable area ~/input/objects autoware_auto_perception_msgs/PredictedObjects Recognized objects around the vehicle /localization/kinematic_kinematics nav_msgs/Odometry Current Velocity of ego vehicle /planning/scenario_planning/lane_driving/obstacle_avoidance_approval tier4_planning_msgs/EnableAvoidance Approval to execute obstacle avoidance output # Name Type Description ~/output/trajectory autoware_auto_planning_msgs/Trajectory Optimized trajectory that is feasible to drive and collision-free Flowchart # Flowchart of functions is explained here. checkReplan # When one of the following conditions are realized, callback function to generate a trajectory is called and publish the trajectory. Otherwise, previously generated (optimized) trajectory is published just with aligning the velocity from the latest behavior path. Ego moves a certain distance compared to the previous ego pose (default: 3.0 [m]) Time passes (default: 1.0 [s]) Ego is far from the previously generated trajectory getRoadClearanceMap # Cost map is generated according to the distance to the road boundaries. These cost maps are used in the optimization to generate a collision-free trajectory. drawObstacleOnImage # Only obstacles that are static and locate in a shoulder lane is decided to avoid. In detail, this equals to the following three conditions at the same time, and the red obstacles in the figure (id: 3, 4, 5) is to be avoided. Velocity is under a certain value (default: 0.1 [m/s]) CoG of the obstacles is not on the center line so that the ego will not avoid the car in front of the ego in the same lane. At least one point of the obstacle polygon is outside the drivable area. getObstacleClearanceMap # Cost map is generated according to the distance to the target obstacles to be avoided. getEBTrajectory # The latter optimization (MPT) assumes that the reference path is smooth enough. Therefore the path from behavior is made smooth here, and send to the optimization as a format of trajectory. Obstacles are ignored in this function. More details can be seen in the Elastic Band section. getModelPredictiveTrajectory # This module makes the trajectory kinematically-feasible and collision-free. We define vehicle pose in the frenet coordinate, and minimize tracking errors by optimization. This optimization considers vehicle kinematics and collision checking with road boundary and obstacles. To decrease the computation cost, the optimization is applied to the shorter trajectory (default: 50 [m]) than the whole trajectory, and concatenate the remained trajectory with the optimized one at last. The trajectory just in front of the ego must not be changed a lot so that the steering wheel will be stable. Therefore, we use the previously generated trajectory in front of the ego. Optimization center on the vehicle, that tries to locate just on the trajectory, can be tuned along side the vehicle vertical axis. This parameter mpt.kinematics.optimization center offset is defined as the signed length from the back-wheel center to the optimization center. Some examples are shown in the following figure, and it is shown that the trajectory of vehicle shape differs according to the optimization center even if the reference trajectory (green one) is the same. More details can be seen in the Model Predictive Trajectory section. insertZeroVelocityOutsideDrivableArea # Optimized trajectory is too short for velocity planning, therefore extend the trajectory by concatenating the optimized trajectory and the behavior path considering drivability. Generated trajectory is checked if it is inside the drivable area or not, and if outside drivable area, output a trajectory inside drivable area with the behavior path or the previously generated trajectory. As described above, the behavior path is separated into two paths: one is for optimization and the other is the remain. The first path becomes optimized trajectory, and the second path just is transformed to a trajectory. Then a trajectory inside the drivable area is calculated as follows. If optimized trajectory is inside the drivable area , and the remained trajectory is inside/outside the drivable area, the output trajectory will be just concatenation of those two trajectories. In this case, we do not care if the remained trajectory is inside or outside the drivable area since generally it is outside the drivable area (especially in a narrow road), but we want to pass a trajectory as long as possible to the latter module. If optimized trajectory is outside the drivable area , and the remained trajectory is inside/outside the drivable area, and if the previously generated trajectory is memorized , the output trajectory will be the previously generated trajectory, where zero velocity is inserted to the point firstly going outside the drivable area. and if the previously generated trajectory is not memorized , the output trajectory will be a part of trajectory just transformed from the behavior path, where zero velocity is inserted to the point firstly going outside the drivable area. Optimization failure is dealt with the same as if the optimized trajectory is outside the drivable area. The output trajectory is memorized as a previously generated trajectory for the next cycle. Rationale In the current design, since there are some modelling errors, the constraints are considered to be soft constraints. Therefore, we have to make sure that the optimized trajectory is inside the drivable area or not after optimization. alignVelocity # Velocity is assigned in the result trajectory from the velocity in the behavior path. The shapes of the trajectory and the path are different, therefore the each nearest trajectory point to the path is searched and interpolated linearly. Algorithms # In this section, Elastic band (to make the path smooth) and Model Predictive Trajectory (to make the trajectory kinematically feasible and collision-free) will be explained in detail. Elastic band # Abstract # Elastic band smooths the path generated in the behavior. Since the latter process of optimization uses the curvature and normal vector of the reference path, smoothing should be applied here so that the optimization will be stable. This smoothing process does not consider collision. Therefore the output path may have a collision with road boundaries or obstacles. Formulation # We formulate a QP problem minimizing the distance between the previous point and the next point for each point. Conditions that each point can move to a certain extent are used so that the path will not changed a lot but will be smoother. For k k 'th point ( \\boldsymbol{p}_k = (x_k, y_k) \\boldsymbol{p}_k = (x_k, y_k) ), the objective function is as follows. The beginning and end point are fixed during the optimization. \\begin{align} \\ J & = \\min \\sum_{k=1}^{n-1} ||(\\boldsymbol{p}_{k+1} - \\boldsymbol{p}_{k}) - (\\boldsymbol{p}_{k} - \\boldsymbol{p}_{k-1})||^2 \\\\ \\ & = \\min \\sum_{k=1}^{n-1} ||\\boldsymbol{p}_{k+1} - 2 \\boldsymbol{p}_{k} + \\boldsymbol{p}_{k-1}||^2 \\\\ \\ & = \\min \\sum_{k=1}^{n-1} \\{(x_{k+1} - x_k + x_{k-1})^2 + (y_{k+1} - y_k + y_{k-1})^2\\} \\\\ \\ & = \\min \\begin{pmatrix} \\ x_0 \\\\ \\ x_1 \\\\ \\ x_2 \\\\ \\vdots \\\\ \\ x_{n-2}\\\\ \\ x_{n-1} \\\\ \\ x_{n} \\\\ \\ y_0 \\\\ \\ y_1 \\\\ \\ y_2 \\\\ \\vdots \\\\ \\ y_{n-2}\\\\ \\ y_{n-1} \\\\ \\ y_{n} \\\\ \\end{pmatrix}^T \\begin{pmatrix} 1 & -2 & 1 & 0 & \\dots& \\\\ -2 & 5 & -4 & 1 & 0 &\\dots \\\\ 1 & -4 & 6 & -4 & 1 & \\\\ 0 & 1 & -4 & 6 & -4 & \\\\ \\vdots & 0 & \\ddots&\\ddots& \\ddots \\\\ & \\vdots & & & \\\\ & & & 1 & -4 & 6 & -4 & 1 \\\\ & & & & 1 & -4 & 5 & -2 \\\\ & & & & & 1 & -2 & 1& \\\\ & & & & & & & &1 & -2 & 1 & 0 & \\dots& \\\\ & & & & & & & &-2 & 5 & -4 & 1 & 0 &\\dots \\\\ & & & & & & & &1 & -4 & 6 & -4 & 1 & \\\\ & & & & & & & &0 & 1 & -4 & 6 & -4 & \\\\ & & & & & & & &\\vdots & 0 & \\ddots&\\ddots& \\ddots \\\\ & & & & & & & & & \\vdots & & & \\\\ & & & & & & & & & & & 1 & -4 & 6 & -4 & 1 \\\\ & & & & & & & & & & & & 1 & -4 & 5 & -2 \\\\ & & & & & & & & & & & & & 1 & -2 & 1& \\\\ \\end{pmatrix} \\begin{pmatrix} \\ x_0 \\\\ \\ x_1 \\\\ \\ x_2 \\\\ \\vdots \\\\ \\ x_{n-2}\\\\ \\ x_{n-1} \\\\ \\ x_{n} \\\\ \\ y_0 \\\\ \\ y_1 \\\\ \\ y_2 \\\\ \\vdots \\\\ \\ y_{n-2}\\\\ \\ y_{n-1} \\\\ \\ y_{n} \\\\ \\end{pmatrix} \\end{align} \\begin{align} \\ J & = \\min \\sum_{k=1}^{n-1} ||(\\boldsymbol{p}_{k+1} - \\boldsymbol{p}_{k}) - (\\boldsymbol{p}_{k} - \\boldsymbol{p}_{k-1})||^2 \\\\ \\ & = \\min \\sum_{k=1}^{n-1} ||\\boldsymbol{p}_{k+1} - 2 \\boldsymbol{p}_{k} + \\boldsymbol{p}_{k-1}||^2 \\\\ \\ & = \\min \\sum_{k=1}^{n-1} \\{(x_{k+1} - x_k + x_{k-1})^2 + (y_{k+1} - y_k + y_{k-1})^2\\} \\\\ \\ & = \\min \\begin{pmatrix} \\ x_0 \\\\ \\ x_1 \\\\ \\ x_2 \\\\ \\vdots \\\\ \\ x_{n-2}\\\\ \\ x_{n-1} \\\\ \\ x_{n} \\\\ \\ y_0 \\\\ \\ y_1 \\\\ \\ y_2 \\\\ \\vdots \\\\ \\ y_{n-2}\\\\ \\ y_{n-1} \\\\ \\ y_{n} \\\\ \\end{pmatrix}^T \\begin{pmatrix} 1 & -2 & 1 & 0 & \\dots& \\\\ -2 & 5 & -4 & 1 & 0 &\\dots \\\\ 1 & -4 & 6 & -4 & 1 & \\\\ 0 & 1 & -4 & 6 & -4 & \\\\ \\vdots & 0 & \\ddots&\\ddots& \\ddots \\\\ & \\vdots & & & \\\\ & & & 1 & -4 & 6 & -4 & 1 \\\\ & & & & 1 & -4 & 5 & -2 \\\\ & & & & & 1 & -2 & 1& \\\\ & & & & & & & &1 & -2 & 1 & 0 & \\dots& \\\\ & & & & & & & &-2 & 5 & -4 & 1 & 0 &\\dots \\\\ & & & & & & & &1 & -4 & 6 & -4 & 1 & \\\\ & & & & & & & &0 & 1 & -4 & 6 & -4 & \\\\ & & & & & & & &\\vdots & 0 & \\ddots&\\ddots& \\ddots \\\\ & & & & & & & & & \\vdots & & & \\\\ & & & & & & & & & & & 1 & -4 & 6 & -4 & 1 \\\\ & & & & & & & & & & & & 1 & -4 & 5 & -2 \\\\ & & & & & & & & & & & & & 1 & -2 & 1& \\\\ \\end{pmatrix} \\begin{pmatrix} \\ x_0 \\\\ \\ x_1 \\\\ \\ x_2 \\\\ \\vdots \\\\ \\ x_{n-2}\\\\ \\ x_{n-1} \\\\ \\ x_{n} \\\\ \\ y_0 \\\\ \\ y_1 \\\\ \\ y_2 \\\\ \\vdots \\\\ \\ y_{n-2}\\\\ \\ y_{n-1} \\\\ \\ y_{n} \\\\ \\end{pmatrix} \\end{align} Model predictive trajectory # Abstract # Model Predictive Trajectory (MPT) calculates the trajectory that realizes the following conditions. Kinematically feasible for linear vehicle kinematics model Collision free with obstacles and road boundaries Conditions for collision free is considered to be not hard constraints but soft constraints. When the optimization failed or the optimized trajectory is not collision free, the output trajectory will be previously generated trajectory. Trajectory near the ego must be stable, therefore the condition where trajectory points near the ego are the same as previously generated trajectory is considered, and this is the only hard constraints in MPT. Vehicle kinematics # As the following figure, we consider the bicycle kinematics model in the frenet frame to track the reference path. At time step k k , we define lateral distance to the reference path, heading angle against the reference path, and steer angle as y_k y_k , \\theta_k \\theta_k , and \\delta_k \\delta_k respectively. Assuming that the commanded steer angle is \\delta_{des, k} \\delta_{des, k} , the kinematics model in the frenet frame is formulated as follows. We also assume that the steer angle \\delta_k \\delta_k is first-order lag to the commanded one. \\begin{align} y_{k+1} & = y_{k} + v \\sin \\theta_k dt \\\\ \\theta_{k+1} & = \\theta_k + \\frac{v \\tan \\delta_k}{L}dt - \\kappa_k v \\cos \\theta_k dt \\\\ \\delta_{k+1} & = \\delta_k - \\frac{\\delta_k - \\delta_{des,k}}{\\tau}dt \\end{align} \\begin{align} y_{k+1} & = y_{k} + v \\sin \\theta_k dt \\\\ \\theta_{k+1} & = \\theta_k + \\frac{v \\tan \\delta_k}{L}dt - \\kappa_k v \\cos \\theta_k dt \\\\ \\delta_{k+1} & = \\delta_k - \\frac{\\delta_k - \\delta_{des,k}}{\\tau}dt \\end{align} Linearization # Then we linearize these equations. y_k y_k and \\theta_k \\theta_k are tracking errors, so we assume that those are small enough. Therefore \\sin \\theta_k \\approx \\theta_k \\sin \\theta_k \\approx \\theta_k . Since \\delta_k \\delta_k is a steer angle, it is not always small. By using a reference steer angle \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} calculated by the reference path curvature \\kappa_k \\kappa_k , we express \\delta_k \\delta_k with a small value \\Delta \\delta_k \\Delta \\delta_k . Note that the steer angle \\delta_k \\delta_k is within the steer angle limitation \\delta_{\\max} \\delta_{\\max} . When the reference steer angle \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} is larger than the steer angle limitation \\delta_{\\max} \\delta_{\\max} , and \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} is used to linearize the steer angle, \\Delta \\delta_k \\Delta \\delta_k is \\Delta \\delta_k = \\delta - \\delta_{\\mathrm{ref}, k} = \\delta_{\\max} - \\delta_{\\mathrm{ref}, k} \\Delta \\delta_k = \\delta - \\delta_{\\mathrm{ref}, k} = \\delta_{\\max} - \\delta_{\\mathrm{ref}, k} , and the absolute \\Delta \\delta_k \\Delta \\delta_k gets larger. Therefore, we have to apply the steer angle limitation to \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} as well. \\begin{align} \\delta_{\\mathrm{ref}, k} & = \\mathrm{clamp}(\\arctan(L \\kappa_k), -\\delta_{\\max}, \\delta_{\\max}) \\\\ \\delta_k & = \\delta_{\\mathrm{ref}, k} + \\Delta \\delta_k, \\ \\Delta \\delta_k \\ll 1 \\\\ \\end{align} \\begin{align} \\delta_{\\mathrm{ref}, k} & = \\mathrm{clamp}(\\arctan(L \\kappa_k), -\\delta_{\\max}, \\delta_{\\max}) \\\\ \\delta_k & = \\delta_{\\mathrm{ref}, k} + \\Delta \\delta_k, \\ \\Delta \\delta_k \\ll 1 \\\\ \\end{align} \\mathrm{clamp}(v, v_{\\min}, v_{\\max}) \\mathrm{clamp}(v, v_{\\min}, v_{\\max}) is a function to convert v v to be larger than v_{\\min} v_{\\min} and smaller than v_{\\max} v_{\\max} . Using this \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} , \\tan \\delta_k \\tan \\delta_k is linearized as follows. \\begin{align} \\tan \\delta_k & \\approx \\tan \\delta_{\\mathrm{ref}, k} + \\left.\\frac{d \\tan \\delta}{d \\delta}\\right|_{\\delta = \\delta_{\\mathrm{ref}, k}} \\Delta \\delta_k \\\\ & = \\tan \\delta_{\\mathrm{ref}, k} + \\left.\\frac{d \\tan \\delta}{d \\delta}\\right|_{\\delta = \\delta_{\\mathrm{ref}, k}} (\\delta_{\\mathrm{ref}, k} - \\delta_k) \\\\ & = \\tan \\delta_{\\mathrm{ref}, k} - \\frac{\\delta_{\\mathrm{ref}, k}}{\\cos^2 \\delta_{\\mathrm{ref}, k}} + \\frac{1}{\\cos^2 \\delta_{\\mathrm{ref}, k}} \\delta_k \\end{align} \\begin{align} \\tan \\delta_k & \\approx \\tan \\delta_{\\mathrm{ref}, k} + \\left.\\frac{d \\tan \\delta}{d \\delta}\\right|_{\\delta = \\delta_{\\mathrm{ref}, k}} \\Delta \\delta_k \\\\ & = \\tan \\delta_{\\mathrm{ref}, k} + \\left.\\frac{d \\tan \\delta}{d \\delta}\\right|_{\\delta = \\delta_{\\mathrm{ref}, k}} (\\delta_{\\mathrm{ref}, k} - \\delta_k) \\\\ & = \\tan \\delta_{\\mathrm{ref}, k} - \\frac{\\delta_{\\mathrm{ref}, k}}{\\cos^2 \\delta_{\\mathrm{ref}, k}} + \\frac{1}{\\cos^2 \\delta_{\\mathrm{ref}, k}} \\delta_k \\end{align} One-step state equation # Based on the linearization, the error kinematics is formulated with the following linear equations, \\begin{align} \\begin{pmatrix} y_{k+1} \\\\ \\theta_{k+1} \\end{pmatrix} = \\begin{pmatrix} 1 & v dt \\\\ 0 & 1 \\\\ \\end{pmatrix} \\begin{pmatrix} y_k \\\\ \\theta_k \\\\ \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ \\frac{v dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} \\\\ \\end{pmatrix} \\delta_{k} + \\begin{pmatrix} 0 \\\\ \\frac{v \\tan(\\delta_{\\mathrm{ref}, k}) dt}{L} - \\frac{v \\delta_{\\mathrm{ref}, k} dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} - \\kappa_k v dt\\\\ \\end{pmatrix} \\end{align} \\begin{align} \\begin{pmatrix} y_{k+1} \\\\ \\theta_{k+1} \\end{pmatrix} = \\begin{pmatrix} 1 & v dt \\\\ 0 & 1 \\\\ \\end{pmatrix} \\begin{pmatrix} y_k \\\\ \\theta_k \\\\ \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ \\frac{v dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} \\\\ \\end{pmatrix} \\delta_{k} + \\begin{pmatrix} 0 \\\\ \\frac{v \\tan(\\delta_{\\mathrm{ref}, k}) dt}{L} - \\frac{v \\delta_{\\mathrm{ref}, k} dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} - \\kappa_k v dt\\\\ \\end{pmatrix} \\end{align} which can be formulated as follows with the state \\boldsymbol{x} \\boldsymbol{x} , control input u u and some matrices, where \\boldsymbol{x} = (y_k, \\theta_k) \\boldsymbol{x} = (y_k, \\theta_k) \\begin{align} \\boldsymbol{x}_{k+1} = A_k \\boldsymbol{x}_k + \\boldsymbol{b}_k u_k + \\boldsymbol{w}_k \\end{align} \\begin{align} \\boldsymbol{x}_{k+1} = A_k \\boldsymbol{x}_k + \\boldsymbol{b}_k u_k + \\boldsymbol{w}_k \\end{align} Time-series state equation # Then, we formulate time-series state equation by concatenating states, control inputs and matrices respectively as \\begin{align} \\boldsymbol{x} = A \\boldsymbol{x}_0 + B \\boldsymbol{u} + \\boldsymbol{w} \\end{align} \\begin{align} \\boldsymbol{x} = A \\boldsymbol{x}_0 + B \\boldsymbol{u} + \\boldsymbol{w} \\end{align} where \\begin{align} \\boldsymbol{x} = (\\boldsymbol{x}^T_1, \\boldsymbol{x}^T_2, \\boldsymbol{x}^T_3, \\dots, \\boldsymbol{x}^T_{n-1})^T \\\\ \\boldsymbol{u} = (u_0, u_1, u_2, \\dots, u_{n-2})^T \\\\ \\boldsymbol{w} = (\\boldsymbol{w}^T_0, \\boldsymbol{w}^T_1, \\boldsymbol{w}^T_2, \\dots, \\boldsymbol{w}^T_{n-1})^T. \\\\ \\end{align} \\begin{align} \\boldsymbol{x} = (\\boldsymbol{x}^T_1, \\boldsymbol{x}^T_2, \\boldsymbol{x}^T_3, \\dots, \\boldsymbol{x}^T_{n-1})^T \\\\ \\boldsymbol{u} = (u_0, u_1, u_2, \\dots, u_{n-2})^T \\\\ \\boldsymbol{w} = (\\boldsymbol{w}^T_0, \\boldsymbol{w}^T_1, \\boldsymbol{w}^T_2, \\dots, \\boldsymbol{w}^T_{n-1})^T. \\\\ \\end{align} In detail, each matrices are constructed as follows. \\begin{align} \\begin{pmatrix} \\boldsymbol{x}_1 \\\\ \\boldsymbol{x}_2 \\\\ \\boldsymbol{x}_3 \\\\ \\vdots \\\\ \\boldsymbol{x}_{n-1} \\end{pmatrix} = \\begin{pmatrix} A_0 \\\\ A_1 A_0 \\\\ A_2 A_1 A_0\\\\ \\vdots \\\\ \\prod\\limits_{k=0}^{n-1} A_{k} \\end{pmatrix} \\boldsymbol{x}_0 + \\begin{pmatrix} B_0 & 0 & & \\dots & 0 \\\\ A_0 B_0 & B_1 & 0 & \\dots & 0 \\\\ A_1 A_0 B_0 & A_0 B_1 & B_2 & \\dots & 0 \\\\ \\vdots & \\vdots & & \\ddots & 0 \\\\ \\prod\\limits_{k=0}^{n-3} A_k B_0 & \\prod\\limits_{k=0}^{n-4} A_k B_1 & \\dots & A_0 B_{n-3} & B_{n-2} \\end{pmatrix} \\begin{pmatrix} u_0 \\\\ u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_{n-2} \\end{pmatrix} + \\begin{pmatrix} I & 0 & & \\dots & 0 \\\\ A_0 & I & 0 & \\dots & 0 \\\\ A_1 A_0 & A_0 & I & \\dots & 0 \\\\ \\vdots & \\vdots & & \\ddots & 0 \\\\ \\prod\\limits_{k=0}^{n-3} A_k & \\prod\\limits_{k=0}^{n-4} A_k & \\dots & A_0 & I \\end{pmatrix} \\begin{pmatrix} \\boldsymbol{w}_0 \\\\ \\boldsymbol{w}_1 \\\\ \\boldsymbol{w}_2 \\\\ \\vdots \\\\ \\boldsymbol{w}_{n-2} \\end{pmatrix} \\end{align} \\begin{align} \\begin{pmatrix} \\boldsymbol{x}_1 \\\\ \\boldsymbol{x}_2 \\\\ \\boldsymbol{x}_3 \\\\ \\vdots \\\\ \\boldsymbol{x}_{n-1} \\end{pmatrix} = \\begin{pmatrix} A_0 \\\\ A_1 A_0 \\\\ A_2 A_1 A_0\\\\ \\vdots \\\\ \\prod\\limits_{k=0}^{n-1} A_{k} \\end{pmatrix} \\boldsymbol{x}_0 + \\begin{pmatrix} B_0 & 0 & & \\dots & 0 \\\\ A_0 B_0 & B_1 & 0 & \\dots & 0 \\\\ A_1 A_0 B_0 & A_0 B_1 & B_2 & \\dots & 0 \\\\ \\vdots & \\vdots & & \\ddots & 0 \\\\ \\prod\\limits_{k=0}^{n-3} A_k B_0 & \\prod\\limits_{k=0}^{n-4} A_k B_1 & \\dots & A_0 B_{n-3} & B_{n-2} \\end{pmatrix} \\begin{pmatrix} u_0 \\\\ u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_{n-2} \\end{pmatrix} + \\begin{pmatrix} I & 0 & & \\dots & 0 \\\\ A_0 & I & 0 & \\dots & 0 \\\\ A_1 A_0 & A_0 & I & \\dots & 0 \\\\ \\vdots & \\vdots & & \\ddots & 0 \\\\ \\prod\\limits_{k=0}^{n-3} A_k & \\prod\\limits_{k=0}^{n-4} A_k & \\dots & A_0 & I \\end{pmatrix} \\begin{pmatrix} \\boldsymbol{w}_0 \\\\ \\boldsymbol{w}_1 \\\\ \\boldsymbol{w}_2 \\\\ \\vdots \\\\ \\boldsymbol{w}_{n-2} \\end{pmatrix} \\end{align} Free-boundary-conditioned time-series state equation # For path planning which does not start from the current ego pose, \\boldsymbol{x}_0 \\boldsymbol{x}_0 should be the design variable of optimization. Therefore, we make \\boldsymbol{u}' \\boldsymbol{u}' by concatenating \\boldsymbol{x}_0 \\boldsymbol{x}_0 and \\boldsymbol{u} \\boldsymbol{u} , and redefine \\boldsymbol{x} \\boldsymbol{x} as follows. \\begin{align} \\boldsymbol{u}' & = (\\boldsymbol{x}^T_0, \\boldsymbol{u}^T)^T \\\\ \\boldsymbol{x} & = (\\boldsymbol{x}^T_0, \\boldsymbol{x}^T_1, \\boldsymbol{x}^T_2, \\dots, \\boldsymbol{x}^T_{n-1})^T \\end{align} \\begin{align} \\boldsymbol{u}' & = (\\boldsymbol{x}^T_0, \\boldsymbol{u}^T)^T \\\\ \\boldsymbol{x} & = (\\boldsymbol{x}^T_0, \\boldsymbol{x}^T_1, \\boldsymbol{x}^T_2, \\dots, \\boldsymbol{x}^T_{n-1})^T \\end{align} Then we get the following state equation \\begin{align} \\boldsymbol{x}' = B \\boldsymbol{u}' + \\boldsymbol{w}, \\end{align} \\begin{align} \\boldsymbol{x}' = B \\boldsymbol{u}' + \\boldsymbol{w}, \\end{align} which is in detail \\begin{align} \\begin{pmatrix} \\boldsymbol{x}_0 \\\\ \\boldsymbol{x}_1 \\\\ \\boldsymbol{x}_2 \\\\ \\boldsymbol{x}_3 \\\\ \\vdots \\\\ \\boldsymbol{x}_{n-1} \\end{pmatrix} = \\begin{pmatrix} I & 0 & \\dots & & & 0 \\\\ A_0 & B_0 & 0 & & \\dots & 0 \\\\ A_1 A_0 & A_0 B_0 & B_1 & 0 & \\dots & 0 \\\\ A_2 A_1 A_0 & A_1 A_0 B_0 & A_0 B_1 & B_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\vdots & & \\ddots & 0 \\\\ \\prod\\limits_{k=0}^{n-1} A_k & \\prod\\limits_{k=0}^{n-3} A_k B_0 & \\prod\\limits_{k=0}^{n-4} A_k B_1 & \\dots & A_0 B_{n-3} & B_{n-2} \\end{pmatrix} \\begin{pmatrix} \\boldsymbol{x}_0 \\\\ u_0 \\\\ u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_{n-2} \\end{pmatrix} + \\begin{pmatrix} 0 & \\dots & & & 0 \\\\ I & 0 & & \\dots & 0 \\\\ A_0 & I & 0 & \\dots & 0 \\\\ A_1 A_0 & A_0 & I & \\dots & 0 \\\\ \\vdots & \\vdots & & \\ddots & 0 \\\\ \\prod\\limits_{k=0}^{n-3} A_k & \\prod\\limits_{k=0}^{n-4} A_k & \\dots & A_0 & I \\end{pmatrix} \\begin{pmatrix} \\boldsymbol{w}_0 \\\\ \\boldsymbol{w}_1 \\\\ \\boldsymbol{w}_2 \\\\ \\vdots \\\\ \\boldsymbol{w}_{n-2} \\end{pmatrix}. \\end{align} \\begin{align} \\begin{pmatrix} \\boldsymbol{x}_0 \\\\ \\boldsymbol{x}_1 \\\\ \\boldsymbol{x}_2 \\\\ \\boldsymbol{x}_3 \\\\ \\vdots \\\\ \\boldsymbol{x}_{n-1} \\end{pmatrix} = \\begin{pmatrix} I & 0 & \\dots & & & 0 \\\\ A_0 & B_0 & 0 & & \\dots & 0 \\\\ A_1 A_0 & A_0 B_0 & B_1 & 0 & \\dots & 0 \\\\ A_2 A_1 A_0 & A_1 A_0 B_0 & A_0 B_1 & B_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\vdots & & \\ddots & 0 \\\\ \\prod\\limits_{k=0}^{n-1} A_k & \\prod\\limits_{k=0}^{n-3} A_k B_0 & \\prod\\limits_{k=0}^{n-4} A_k B_1 & \\dots & A_0 B_{n-3} & B_{n-2} \\end{pmatrix} \\begin{pmatrix} \\boldsymbol{x}_0 \\\\ u_0 \\\\ u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_{n-2} \\end{pmatrix} + \\begin{pmatrix} 0 & \\dots & & & 0 \\\\ I & 0 & & \\dots & 0 \\\\ A_0 & I & 0 & \\dots & 0 \\\\ A_1 A_0 & A_0 & I & \\dots & 0 \\\\ \\vdots & \\vdots & & \\ddots & 0 \\\\ \\prod\\limits_{k=0}^{n-3} A_k & \\prod\\limits_{k=0}^{n-4} A_k & \\dots & A_0 & I \\end{pmatrix} \\begin{pmatrix} \\boldsymbol{w}_0 \\\\ \\boldsymbol{w}_1 \\\\ \\boldsymbol{w}_2 \\\\ \\vdots \\\\ \\boldsymbol{w}_{n-2} \\end{pmatrix}. \\end{align} Objective function # The objective function for smoothing and tracking is shown as follows, which can be formulated with value function matrices Q, R Q, R . \\begin{align} J_1 (\\boldsymbol{x}', \\boldsymbol{u}') & = w_y \\sum_{k} y_k^2 + w_{\\theta} \\sum_{k} \\theta_k^2 + w_{\\delta} \\sum_k \\delta_k^2 + w_{\\dot{\\delta}} \\sum_k \\dot{\\delta}_k^2 + w_{\\ddot{\\delta}} \\sum_k \\ddot{\\delta}_k^2 \\\\ & = \\boldsymbol{x}'^T Q \\boldsymbol{x}' + \\boldsymbol{u}'^T R \\boldsymbol{u}' \\\\ & = \\boldsymbol{u}'^T H \\boldsymbol{u}' + \\boldsymbol{u}'^T \\boldsymbol{f} \\end{align} \\begin{align} J_1 (\\boldsymbol{x}', \\boldsymbol{u}') & = w_y \\sum_{k} y_k^2 + w_{\\theta} \\sum_{k} \\theta_k^2 + w_{\\delta} \\sum_k \\delta_k^2 + w_{\\dot{\\delta}} \\sum_k \\dot{\\delta}_k^2 + w_{\\ddot{\\delta}} \\sum_k \\ddot{\\delta}_k^2 \\\\ & = \\boldsymbol{x}'^T Q \\boldsymbol{x}' + \\boldsymbol{u}'^T R \\boldsymbol{u}' \\\\ & = \\boldsymbol{u}'^T H \\boldsymbol{u}' + \\boldsymbol{u}'^T \\boldsymbol{f} \\end{align} As mentioned before, the constraints to be collision free with obstacles and road boundaries are formulated to be soft constraints. Assuming that the lateral distance to the road boundaries or obstacles from the back wheel center, front wheel center, and the point between them are y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k} y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k} respectively, and slack variables for each point are \\lambda_{\\mathrm{base}}, \\lambda_{\\mathrm{top}}, \\lambda_{\\mathrm{mid}} \\lambda_{\\mathrm{base}}, \\lambda_{\\mathrm{top}}, \\lambda_{\\mathrm{mid}} , the soft constraints can be formulated as follows. y_{\\mathrm{base}, k, \\min} - \\lambda_{\\mathrm{base}, k} \\leq y_{\\mathrm{base}, k} (y_k) \\leq y_{\\mathrm{base}, k, \\max} + \\lambda_{\\mathrm{base}, k}\\\\ y_{\\mathrm{top}, k, \\min} - \\lambda_{\\mathrm{top}, k} \\leq y_{\\mathrm{top}, k} (y_k) \\leq y_{\\mathrm{top}, k, \\max} + \\lambda_{\\mathrm{top}, k}\\\\ y_{\\mathrm{mid}, k, \\min} - \\lambda_{\\mathrm{mid}, k} \\leq y_{\\mathrm{mid}, k} (y_k) \\leq y_{\\mathrm{mid}, k, \\max} + \\lambda_{\\mathrm{mid}, k} \\\\ 0 \\leq \\lambda_{\\mathrm{base}, k} \\\\ 0 \\leq \\lambda_{\\mathrm{top}, k} \\\\ 0 \\leq \\lambda_{\\mathrm{mid}, k} y_{\\mathrm{base}, k, \\min} - \\lambda_{\\mathrm{base}, k} \\leq y_{\\mathrm{base}, k} (y_k) \\leq y_{\\mathrm{base}, k, \\max} + \\lambda_{\\mathrm{base}, k}\\\\ y_{\\mathrm{top}, k, \\min} - \\lambda_{\\mathrm{top}, k} \\leq y_{\\mathrm{top}, k} (y_k) \\leq y_{\\mathrm{top}, k, \\max} + \\lambda_{\\mathrm{top}, k}\\\\ y_{\\mathrm{mid}, k, \\min} - \\lambda_{\\mathrm{mid}, k} \\leq y_{\\mathrm{mid}, k} (y_k) \\leq y_{\\mathrm{mid}, k, \\max} + \\lambda_{\\mathrm{mid}, k} \\\\ 0 \\leq \\lambda_{\\mathrm{base}, k} \\\\ 0 \\leq \\lambda_{\\mathrm{top}, k} \\\\ 0 \\leq \\lambda_{\\mathrm{mid}, k} Since y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k} y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k} is formulated as a linear function of y_k y_k , the objective function for soft constraints is formulated as follows. \\begin{align} J_2 & (\\boldsymbol{\\lambda}_\\mathrm{base}, \\boldsymbol{\\lambda}_\\mathrm{top}, \\boldsymbol {\\lambda}_\\mathrm{mid})\\\\ & = w_{\\mathrm{base}} \\sum_{k} \\lambda_{\\mathrm{base}, k} + w_{\\mathrm{mid}} \\sum_k \\lambda_{\\mathrm{mid}, k} + w_{\\mathrm{top}} \\sum_k \\lambda_{\\mathrm{top}, k} \\end{align} \\begin{align} J_2 & (\\boldsymbol{\\lambda}_\\mathrm{base}, \\boldsymbol{\\lambda}_\\mathrm{top}, \\boldsymbol {\\lambda}_\\mathrm{mid})\\\\ & = w_{\\mathrm{base}} \\sum_{k} \\lambda_{\\mathrm{base}, k} + w_{\\mathrm{mid}} \\sum_k \\lambda_{\\mathrm{mid}, k} + w_{\\mathrm{top}} \\sum_k \\lambda_{\\mathrm{top}, k} \\end{align} Slack variables are also design variables for optimization. We define a vector \\boldsymbol{v} \\boldsymbol{v} , that concatenates all the design variables. \\begin{align} \\boldsymbol{v} = \\begin{pmatrix} \\boldsymbol{u}'^T & \\boldsymbol{\\lambda}_\\mathrm{base}^T & \\boldsymbol{\\lambda}_\\mathrm{top}^T & \\boldsymbol{\\lambda}_\\mathrm{mid}^T \\end{pmatrix}^T \\end{align} \\begin{align} \\boldsymbol{v} = \\begin{pmatrix} \\boldsymbol{u}'^T & \\boldsymbol{\\lambda}_\\mathrm{base}^T & \\boldsymbol{\\lambda}_\\mathrm{top}^T & \\boldsymbol{\\lambda}_\\mathrm{mid}^T \\end{pmatrix}^T \\end{align} The summation of these two objective functions is the objective function for the optimization problem. \\begin{align} \\min_{\\boldsymbol{v}} J (\\boldsymbol{v}) = \\min_{\\boldsymbol{v}} J_1 (\\boldsymbol{u}') + J_2 (\\boldsymbol{\\lambda}_\\mathrm{base}, \\boldsymbol{\\lambda}_\\mathrm{top}, \\boldsymbol{\\lambda}_\\mathrm{mid}) \\end{align} \\begin{align} \\min_{\\boldsymbol{v}} J (\\boldsymbol{v}) = \\min_{\\boldsymbol{v}} J_1 (\\boldsymbol{u}') + J_2 (\\boldsymbol{\\lambda}_\\mathrm{base}, \\boldsymbol{\\lambda}_\\mathrm{top}, \\boldsymbol{\\lambda}_\\mathrm{mid}) \\end{align} As mentioned before, we use hard constraints where some trajectory points in front of the ego are the same as the previously generated trajectory points. This hard constraints is formulated as follows. \\begin{align} \\delta_k = \\delta_{k}^{\\mathrm{prev}} (0 \\leq i \\leq N_{\\mathrm{fix}}) \\end{align} \\begin{align} \\delta_k = \\delta_{k}^{\\mathrm{prev}} (0 \\leq i \\leq N_{\\mathrm{fix}}) \\end{align} Finally we transform those objective functions to the following QP problem, and solve it. \\begin{align} \\min_{\\boldsymbol{v}} \\ & \\frac{1}{2} \\boldsymbol{v}^T \\boldsymbol{H} \\boldsymbol{v} + \\boldsymbol{f} \\boldsymbol{v} \\\\ \\mathrm{s.t.} \\ & \\boldsymbol{b}_{lower} \\leq \\boldsymbol{A} \\boldsymbol{v} \\leq \\boldsymbol{b}_{upper} \\end{align} \\begin{align} \\min_{\\boldsymbol{v}} \\ & \\frac{1}{2} \\boldsymbol{v}^T \\boldsymbol{H} \\boldsymbol{v} + \\boldsymbol{f} \\boldsymbol{v} \\\\ \\mathrm{s.t.} \\ & \\boldsymbol{b}_{lower} \\leq \\boldsymbol{A} \\boldsymbol{v} \\leq \\boldsymbol{b}_{upper} \\end{align} Constraints # Steer angle limitation # Steer angle has a certain limitation ( \\delta_{max} \\delta_{max} , \\delta_{min} \\delta_{min} ). Therefore we add linear inequality equations. \\begin{align} \\delta_{min} \\leq \\delta_i \\leq \\delta_{max} \\end{align} \\begin{align} \\delta_{min} \\leq \\delta_i \\leq \\delta_{max} \\end{align} Collision free # To realize collision-free path planning, we have to formulate constraints that the vehicle is inside the road (moreover, a certain meter far from the road boundary) and does not collide with obstacles in linear equations. For linearity, we chose a method to approximate the vehicle shape with a set of circles, that is reliable and easy to implement. Now we formulate the linear constraints where a set of circles on each trajectory point is collision-free. For collision checking, we have a drivable area in the format of an image where walls or obstacles are filled with a color. By using this drivable area, we calculate upper (left) and lower (right) boundaries along reference points so that we can interpolate boundaries on any position on the trajectory. Assuming that upper and lower boundaries are b_l b_l , b_u b_u respectively, and r r is a radius of a circle, lateral deviation of the circle center y' y' has to be b_l + r \\leq y' \\leq b_u - r. b_l + r \\leq y' \\leq b_u - r. Based on the following figure, y' y' can be formulated as follows. \\begin{align} y' & = L \\sin(\\theta + \\beta) + y \\cos \\beta - l \\sin(\\gamma - \\phi_a) \\\\ & = L \\sin \\theta \\cos \\beta + L \\cos \\theta \\sin \\beta + y \\cos \\beta - l \\sin(\\gamma - \\phi_a) \\\\ & \\approx L \\theta \\cos \\beta + L \\sin \\beta + y \\cos \\beta - l \\sin(\\gamma - \\phi_a) \\end{align} \\begin{align} y' & = L \\sin(\\theta + \\beta) + y \\cos \\beta - l \\sin(\\gamma - \\phi_a) \\\\ & = L \\sin \\theta \\cos \\beta + L \\cos \\theta \\sin \\beta + y \\cos \\beta - l \\sin(\\gamma - \\phi_a) \\\\ & \\approx L \\theta \\cos \\beta + L \\sin \\beta + y \\cos \\beta - l \\sin(\\gamma - \\phi_a) \\end{align} b_l + r - \\lambda \\leq y' \\leq b_u - r + \\lambda. b_l + r - \\lambda \\leq y' \\leq b_u - r + \\lambda. \\begin{align} y' & = C_1 \\boldsymbol{x} + C_2 \\\\ & = C_1 (B \\boldsymbol{v} + \\boldsymbol{w}) + C_2 \\\\ & = C_1 B \\boldsymbol{v} + \\boldsymbol{w} + C_2 \\end{align} \\begin{align} y' & = C_1 \\boldsymbol{x} + C_2 \\\\ & = C_1 (B \\boldsymbol{v} + \\boldsymbol{w}) + C_2 \\\\ & = C_1 B \\boldsymbol{v} + \\boldsymbol{w} + C_2 \\end{align} Note that longitudinal position of the circle center and the trajectory point to calculate boundaries are different. But each boundaries are vertical against the trajectory, resulting in less distortion by the longitudinal position difference since road boundaries does not change so much. For example, if the boundaries are not vertical against the trajectory and there is a certain difference of longitudinal position between the circe center and the trajectory point, we can easily guess that there is much more distortion when comparing lateral deviation and boundaries. \\begin{align} A_{blk} & = \\begin{pmatrix} C_1 B & O & \\dots & O & I_{N_{ref} \\times N_{ref}} & O \\dots & O\\\\ -C_1 B & O & \\dots & O & I & O \\dots & O\\\\ O & O & \\dots & O & I & O \\dots & O \\end{pmatrix} \\in \\boldsymbol{R}^{3 N_{ref} \\times D_v + N_{circle} N_{ref}} \\\\ \\boldsymbol{b}_{lower, blk} & = \\begin{pmatrix} \\boldsymbol{b}_{lower} - C_1 \\boldsymbol{w} - C_2 \\\\ -\\boldsymbol{b}_{upper} + C_1 \\boldsymbol{w} + C_2 \\\\ O \\end{pmatrix} \\in \\boldsymbol{R}^{3 N_{ref}} \\\\ \\boldsymbol{b}_{upper, blk} & = \\boldsymbol{\\infty} \\in \\boldsymbol{R}^{3 N_{ref}} \\end{align} \\begin{align} A_{blk} & = \\begin{pmatrix} C_1 B & O & \\dots & O & I_{N_{ref} \\times N_{ref}} & O \\dots & O\\\\ -C_1 B & O & \\dots & O & I & O \\dots & O\\\\ O & O & \\dots & O & I & O \\dots & O \\end{pmatrix} \\in \\boldsymbol{R}^{3 N_{ref} \\times D_v + N_{circle} N_{ref}} \\\\ \\boldsymbol{b}_{lower, blk} & = \\begin{pmatrix} \\boldsymbol{b}_{lower} - C_1 \\boldsymbol{w} - C_2 \\\\ -\\boldsymbol{b}_{upper} + C_1 \\boldsymbol{w} + C_2 \\\\ O \\end{pmatrix} \\in \\boldsymbol{R}^{3 N_{ref}} \\\\ \\boldsymbol{b}_{upper, blk} & = \\boldsymbol{\\infty} \\in \\boldsymbol{R}^{3 N_{ref}} \\end{align} We will explain options for optimization. L-infinity optimization # The above formulation is called L2 norm for slack variables. Instead, if we use L-infinity norm where slack variables are shared by enabling l_inf_norm . \\begin{align} A_{blk} = \\begin{pmatrix} C_1 B & I_{N_{ref} \\times N_{ref}} \\\\ -C_1 B & I \\\\ O & I \\end{pmatrix} \\in \\boldsymbol{R}^{3N_{ref} \\times D_v + N_{ref}} \\end{align} \\begin{align} A_{blk} = \\begin{pmatrix} C_1 B & I_{N_{ref} \\times N_{ref}} \\\\ -C_1 B & I \\\\ O & I \\end{pmatrix} \\in \\boldsymbol{R}^{3N_{ref} \\times D_v + N_{ref}} \\end{align} Two-step soft constraints # \\begin{align} \\boldsymbol{v}' = \\begin{pmatrix} \\boldsymbol{v} \\\\ \\boldsymbol{\\lambda}^{soft_1} \\\\ \\boldsymbol{\\lambda}^{soft_2} \\\\ \\end{pmatrix} \\in \\boldsymbol{R}^{D_v + 2N_{slack}} \\end{align} \\begin{align} \\boldsymbol{v}' = \\begin{pmatrix} \\boldsymbol{v} \\\\ \\boldsymbol{\\lambda}^{soft_1} \\\\ \\boldsymbol{\\lambda}^{soft_2} \\\\ \\end{pmatrix} \\in \\boldsymbol{R}^{D_v + 2N_{slack}} \\end{align} * * depends on whether to use L2 norm or L-infinity optimization. \\begin{align} A_{blk} & = \\begin{pmatrix} A^{soft_1}_{blk} \\\\ A^{soft_2}_{blk} \\\\ \\end{pmatrix}\\\\ & = \\begin{pmatrix} C_1^{soft_1} B & & \\\\ -C_1^{soft_1} B & \\Huge{*} & \\Huge{O} \\\\ O & & \\\\ C_1^{soft_2} B & & \\\\ -C_1^{soft_2} B & \\Huge{O} & \\Huge{*} \\\\ O & & \\end{pmatrix} \\in \\boldsymbol{R}^{6 N_{ref} \\times D_v + 2 N_{slack}} \\end{align} \\begin{align} A_{blk} & = \\begin{pmatrix} A^{soft_1}_{blk} \\\\ A^{soft_2}_{blk} \\\\ \\end{pmatrix}\\\\ & = \\begin{pmatrix} C_1^{soft_1} B & & \\\\ -C_1^{soft_1} B & \\Huge{*} & \\Huge{O} \\\\ O & & \\\\ C_1^{soft_2} B & & \\\\ -C_1^{soft_2} B & \\Huge{O} & \\Huge{*} \\\\ O & & \\end{pmatrix} \\in \\boldsymbol{R}^{6 N_{ref} \\times D_v + 2 N_{slack}} \\end{align} N_{slack} N_{slack} is N_{circle} N_{circle} when L2 optimization, or 1 1 when L-infinity optimization. N_{circle} N_{circle} is the number of circles to check collision. Tuning # Vehicle # max steering wheel degree mpt.kinematics.max_steer_deg Boundary search # advanced.mpt.bounds_search_widths In order to efficiently search precise lateral boundaries on each trajectory point, different resolutions of search widths are defined. By default, [0.45, 0.15, 0.05, 0.01] is used. In this case, the goal is to get the boundaries' length on each trajectory point with 0.01 [m] resolution. Firstly, lateral boundaries are searhed with a rough resolution (= 0.45 [m]). Then, within its 0.45 [m] resolution which boundaries are inside, they are searched again with a bit precise resolution (= 0.15 [m]). Following this rule, finally boundaries with 0.01 [m] will be found. Assumptions # EB optimized trajectory length should be longer than MPT optimized trajectory length since MPT result may be jerky because of non-fixed reference path (= EB optimized trajectory) At least, EB fixed optimized trajectory length must be longer than MPT fixed optimization trajectory length This causes the case that there is a large difference between MPT fixed optimized point and MPT optimized point just after the point. Drivability in narrow roads # set option.drivability_check.use_vehicle_circles true use a set of circles as a shape of the vehicle when checking if the generated trajectory will be outside the drivable area. make mpt.clearance.soft_clearance_from_road smaller make mpt.kinematics.optimization_center_offset different The point on the vehicle, offset forward from the base link` tries to follow the reference path. This may cause the a part of generated trajectory will be outside the drivable area. Computation time # Loose EB optimization make eb.common.delta_arc_length_for_eb large and eb.common.num_sampling_points_for_eb small This makes the number of design variables smaller Be careful about the trajectory length between MPT and EB as shown in Assumptions. However, empirically this causes large turn at the corner (e.g. The vehicle turns a steering wheel to the opposite side (=left) a bit just before the corner turning to right) make eb.qp.eps_abs and eb.qp.eps_rel small This causes very unstable reference path generation for MPT, or turning a steering wheel a little bit larger Enable computation reduction flag set l_inf_norm true (by default) use L-inf norm optimization for MPT w.r.t. slack variables, resulting in lower number of design variables set enable_warm_start true set enable_manual_warm_start true (by default) set steer_limit_constraint false This causes no assumption for trajectory generation where steering angle will not exceeds its hardware limitation make the number of collision-free constraints small How to change parameters depend on the type of collision-free constraints If This may cause the trajectory generation where a part of ego vehicle is out of drivable area Disable publishing debug visualization markers set option.is_publishing_* false Robustness # Check if the trajectory before EB, after EB, or after MPT is not robust if the trajectory before EB is not robust if the trajectory after EB is not robust if the trajectory after MPT is not robust make mpt.weight.steer_input_weight or mpt.weight.steer_rate_weight larger, which are stability of steering wheel along the trajectory. Other options # option.skip_optimization skips EB and MPT optimization. option.enable_pre_smoothing enables EB which is smoothing the trajectory for MPT. EB is not required if the reference path for MPT is smooth enough and does not change its shape suddenly option.is_showing_calculation_time enables showing each calculation time for functions and total calculation time on the terminal. option.is_stopping_if_outside_drivable_area enables stopping just before the generated trajectory point will be outside the drivable area. mpt.option.plan_from_ego enables planning from the ego pose when the ego's velocity is zero. mpt.option.two_step_soft_constraint enables two step of soft constraints for collision free mpt.option.soft_clearance_from_road and mpt.option.soft_second_clearance_from_road are the weight. Limitation # When turning right or left in the intersection, the output trajectory is close to the outside road boundary. Roles of planning for behavior_path_planner and obstacle_avoidance_planner are not decided clearly. High computation cost Comparison to other methods # Planning a trajectory that satisfies kinematic feasibility and collision-free has two main characteristics that makes hard to be solved: one is non-convex and the other is high dimension. According to the characteristics, we investigate pros and cons of the typical planning methods: optimization-based, sampling-based, and learning-based method. Optimization-based method # pros: comparatively fast against high dimension by leveraging the gradient descent cons: often converge to the local minima in the non-convex problem Sampling-based method # pros: realize global optimization cons: high computation cost especially in the complex case Learning-based method # under research yet Based on these pros/cons, we chose the optimization-based planner first. Although it has a cons to converge to the local minima, it can get a good solution by the preprocessing to approximate the convex problem that almost equals to the original non-convex problem. How to debug # Topics for debugging will be explained in this section. Drivable area Drivable area to cover the road. Whether this area is continuous and covers the road can be checked. /planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner/debug/drivable_area , whose type is nav_msgs/msg/OccupancyGrid Path from behavior The input path of obstacle_avoidance_planner. Whether this path is continuous and the curvature is not so high can be checked. Path or PathFootprint rviz plugin. EB trajectory The output trajectory of elastic band. Whether this trajectory is very smooth and a sampling width is constant can be checked. Trajectory or TrajectoryFootprint rviz plugin. MPT reference trajectory The reference trajectory of model predictive trajectory. Whether this trajectory is very smooth and a sampling width is constant can be checked. Trajectory or TrajectoryFootprint rviz plugin. MPT fixed trajectory The fixed trajectory around the ego of model predictive trajectory. Trajectory or TrajectoryFootprint rviz plugin. bounds Lateral Distance to the road or object boundaries to check collision in model predictive trajectory. Whether these lines' ends align the road or obstacle boundaries can be checked. bounds* of /planning/scenario_planning/lane_driving/motion_planning/obstacle_avoidance_planner/debug/marker whose type is visualization_msgs/msg/MarkerArray MPT trajectory The output of model predictive trajectory. Whether this trajectory is smooth enough and inside the drivable area can be checked. Trajectory or TrajectoryFootprint rviz plugin. Output trajectory The output of obstacle_avoidance_planner. Whether this trajectory is smooth enough can be checked. Trajectory or TrajectoryFootprint rviz plugin.","title":"Index"},{"location":"planning/obstacle_avoidance_planner/#purpose","text":"This package generates a trajectory that is feasible to drive and collision free based on a reference path, drivable area, and static/dynamic obstacles. Only position and orientation of trajectory are calculated in this module (velocity is just aligned from the one in the path), and velocity or acceleration will be updated in the latter modules.","title":"Purpose"},{"location":"planning/obstacle_avoidance_planner/#feature","text":"This package is able to follow the behavior path smoothly make the trajectory inside the drivable area as much as possible insert stop point if its trajectory point is outside the drivable area","title":"Feature"},{"location":"planning/obstacle_avoidance_planner/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"planning/obstacle_avoidance_planner/#input","text":"Name Type Description ~/input/path autoware_auto_planning_msgs/Path Reference path and the corresponding drivable area ~/input/objects autoware_auto_perception_msgs/PredictedObjects Recognized objects around the vehicle /localization/kinematic_kinematics nav_msgs/Odometry Current Velocity of ego vehicle /planning/scenario_planning/lane_driving/obstacle_avoidance_approval tier4_planning_msgs/EnableAvoidance Approval to execute obstacle avoidance","title":"input"},{"location":"planning/obstacle_avoidance_planner/#output","text":"Name Type Description ~/output/trajectory autoware_auto_planning_msgs/Trajectory Optimized trajectory that is feasible to drive and collision-free","title":"output"},{"location":"planning/obstacle_avoidance_planner/#flowchart","text":"Flowchart of functions is explained here.","title":"Flowchart"},{"location":"planning/obstacle_avoidance_planner/#checkreplan","text":"When one of the following conditions are realized, callback function to generate a trajectory is called and publish the trajectory. Otherwise, previously generated (optimized) trajectory is published just with aligning the velocity from the latest behavior path. Ego moves a certain distance compared to the previous ego pose (default: 3.0 [m]) Time passes (default: 1.0 [s]) Ego is far from the previously generated trajectory","title":"checkReplan"},{"location":"planning/obstacle_avoidance_planner/#getroadclearancemap","text":"Cost map is generated according to the distance to the road boundaries. These cost maps are used in the optimization to generate a collision-free trajectory.","title":"getRoadClearanceMap"},{"location":"planning/obstacle_avoidance_planner/#drawobstacleonimage","text":"Only obstacles that are static and locate in a shoulder lane is decided to avoid. In detail, this equals to the following three conditions at the same time, and the red obstacles in the figure (id: 3, 4, 5) is to be avoided. Velocity is under a certain value (default: 0.1 [m/s]) CoG of the obstacles is not on the center line so that the ego will not avoid the car in front of the ego in the same lane. At least one point of the obstacle polygon is outside the drivable area.","title":"drawObstacleOnImage"},{"location":"planning/obstacle_avoidance_planner/#getobstacleclearancemap","text":"Cost map is generated according to the distance to the target obstacles to be avoided.","title":"getObstacleClearanceMap"},{"location":"planning/obstacle_avoidance_planner/#getebtrajectory","text":"The latter optimization (MPT) assumes that the reference path is smooth enough. Therefore the path from behavior is made smooth here, and send to the optimization as a format of trajectory. Obstacles are ignored in this function. More details can be seen in the Elastic Band section.","title":"getEBTrajectory"},{"location":"planning/obstacle_avoidance_planner/#getmodelpredictivetrajectory","text":"This module makes the trajectory kinematically-feasible and collision-free. We define vehicle pose in the frenet coordinate, and minimize tracking errors by optimization. This optimization considers vehicle kinematics and collision checking with road boundary and obstacles. To decrease the computation cost, the optimization is applied to the shorter trajectory (default: 50 [m]) than the whole trajectory, and concatenate the remained trajectory with the optimized one at last. The trajectory just in front of the ego must not be changed a lot so that the steering wheel will be stable. Therefore, we use the previously generated trajectory in front of the ego. Optimization center on the vehicle, that tries to locate just on the trajectory, can be tuned along side the vehicle vertical axis. This parameter mpt.kinematics.optimization center offset is defined as the signed length from the back-wheel center to the optimization center. Some examples are shown in the following figure, and it is shown that the trajectory of vehicle shape differs according to the optimization center even if the reference trajectory (green one) is the same. More details can be seen in the Model Predictive Trajectory section.","title":"getModelPredictiveTrajectory"},{"location":"planning/obstacle_avoidance_planner/#insertzerovelocityoutsidedrivablearea","text":"Optimized trajectory is too short for velocity planning, therefore extend the trajectory by concatenating the optimized trajectory and the behavior path considering drivability. Generated trajectory is checked if it is inside the drivable area or not, and if outside drivable area, output a trajectory inside drivable area with the behavior path or the previously generated trajectory. As described above, the behavior path is separated into two paths: one is for optimization and the other is the remain. The first path becomes optimized trajectory, and the second path just is transformed to a trajectory. Then a trajectory inside the drivable area is calculated as follows. If optimized trajectory is inside the drivable area , and the remained trajectory is inside/outside the drivable area, the output trajectory will be just concatenation of those two trajectories. In this case, we do not care if the remained trajectory is inside or outside the drivable area since generally it is outside the drivable area (especially in a narrow road), but we want to pass a trajectory as long as possible to the latter module. If optimized trajectory is outside the drivable area , and the remained trajectory is inside/outside the drivable area, and if the previously generated trajectory is memorized , the output trajectory will be the previously generated trajectory, where zero velocity is inserted to the point firstly going outside the drivable area. and if the previously generated trajectory is not memorized , the output trajectory will be a part of trajectory just transformed from the behavior path, where zero velocity is inserted to the point firstly going outside the drivable area. Optimization failure is dealt with the same as if the optimized trajectory is outside the drivable area. The output trajectory is memorized as a previously generated trajectory for the next cycle. Rationale In the current design, since there are some modelling errors, the constraints are considered to be soft constraints. Therefore, we have to make sure that the optimized trajectory is inside the drivable area or not after optimization.","title":"insertZeroVelocityOutsideDrivableArea"},{"location":"planning/obstacle_avoidance_planner/#alignvelocity","text":"Velocity is assigned in the result trajectory from the velocity in the behavior path. The shapes of the trajectory and the path are different, therefore the each nearest trajectory point to the path is searched and interpolated linearly.","title":"alignVelocity"},{"location":"planning/obstacle_avoidance_planner/#algorithms","text":"In this section, Elastic band (to make the path smooth) and Model Predictive Trajectory (to make the trajectory kinematically feasible and collision-free) will be explained in detail.","title":"Algorithms"},{"location":"planning/obstacle_avoidance_planner/#elastic-band","text":"","title":"Elastic band"},{"location":"planning/obstacle_avoidance_planner/#abstract","text":"Elastic band smooths the path generated in the behavior. Since the latter process of optimization uses the curvature and normal vector of the reference path, smoothing should be applied here so that the optimization will be stable. This smoothing process does not consider collision. Therefore the output path may have a collision with road boundaries or obstacles.","title":"Abstract"},{"location":"planning/obstacle_avoidance_planner/#formulation","text":"We formulate a QP problem minimizing the distance between the previous point and the next point for each point. Conditions that each point can move to a certain extent are used so that the path will not changed a lot but will be smoother. For k k 'th point ( \\boldsymbol{p}_k = (x_k, y_k) \\boldsymbol{p}_k = (x_k, y_k) ), the objective function is as follows. The beginning and end point are fixed during the optimization. \\begin{align} \\ J & = \\min \\sum_{k=1}^{n-1} ||(\\boldsymbol{p}_{k+1} - \\boldsymbol{p}_{k}) - (\\boldsymbol{p}_{k} - \\boldsymbol{p}_{k-1})||^2 \\\\ \\ & = \\min \\sum_{k=1}^{n-1} ||\\boldsymbol{p}_{k+1} - 2 \\boldsymbol{p}_{k} + \\boldsymbol{p}_{k-1}||^2 \\\\ \\ & = \\min \\sum_{k=1}^{n-1} \\{(x_{k+1} - x_k + x_{k-1})^2 + (y_{k+1} - y_k + y_{k-1})^2\\} \\\\ \\ & = \\min \\begin{pmatrix} \\ x_0 \\\\ \\ x_1 \\\\ \\ x_2 \\\\ \\vdots \\\\ \\ x_{n-2}\\\\ \\ x_{n-1} \\\\ \\ x_{n} \\\\ \\ y_0 \\\\ \\ y_1 \\\\ \\ y_2 \\\\ \\vdots \\\\ \\ y_{n-2}\\\\ \\ y_{n-1} \\\\ \\ y_{n} \\\\ \\end{pmatrix}^T \\begin{pmatrix} 1 & -2 & 1 & 0 & \\dots& \\\\ -2 & 5 & -4 & 1 & 0 &\\dots \\\\ 1 & -4 & 6 & -4 & 1 & \\\\ 0 & 1 & -4 & 6 & -4 & \\\\ \\vdots & 0 & \\ddots&\\ddots& \\ddots \\\\ & \\vdots & & & \\\\ & & & 1 & -4 & 6 & -4 & 1 \\\\ & & & & 1 & -4 & 5 & -2 \\\\ & & & & & 1 & -2 & 1& \\\\ & & & & & & & &1 & -2 & 1 & 0 & \\dots& \\\\ & & & & & & & &-2 & 5 & -4 & 1 & 0 &\\dots \\\\ & & & & & & & &1 & -4 & 6 & -4 & 1 & \\\\ & & & & & & & &0 & 1 & -4 & 6 & -4 & \\\\ & & & & & & & &\\vdots & 0 & \\ddots&\\ddots& \\ddots \\\\ & & & & & & & & & \\vdots & & & \\\\ & & & & & & & & & & & 1 & -4 & 6 & -4 & 1 \\\\ & & & & & & & & & & & & 1 & -4 & 5 & -2 \\\\ & & & & & & & & & & & & & 1 & -2 & 1& \\\\ \\end{pmatrix} \\begin{pmatrix} \\ x_0 \\\\ \\ x_1 \\\\ \\ x_2 \\\\ \\vdots \\\\ \\ x_{n-2}\\\\ \\ x_{n-1} \\\\ \\ x_{n} \\\\ \\ y_0 \\\\ \\ y_1 \\\\ \\ y_2 \\\\ \\vdots \\\\ \\ y_{n-2}\\\\ \\ y_{n-1} \\\\ \\ y_{n} \\\\ \\end{pmatrix} \\end{align} \\begin{align} \\ J & = \\min \\sum_{k=1}^{n-1} ||(\\boldsymbol{p}_{k+1} - \\boldsymbol{p}_{k}) - (\\boldsymbol{p}_{k} - \\boldsymbol{p}_{k-1})||^2 \\\\ \\ & = \\min \\sum_{k=1}^{n-1} ||\\boldsymbol{p}_{k+1} - 2 \\boldsymbol{p}_{k} + \\boldsymbol{p}_{k-1}||^2 \\\\ \\ & = \\min \\sum_{k=1}^{n-1} \\{(x_{k+1} - x_k + x_{k-1})^2 + (y_{k+1} - y_k + y_{k-1})^2\\} \\\\ \\ & = \\min \\begin{pmatrix} \\ x_0 \\\\ \\ x_1 \\\\ \\ x_2 \\\\ \\vdots \\\\ \\ x_{n-2}\\\\ \\ x_{n-1} \\\\ \\ x_{n} \\\\ \\ y_0 \\\\ \\ y_1 \\\\ \\ y_2 \\\\ \\vdots \\\\ \\ y_{n-2}\\\\ \\ y_{n-1} \\\\ \\ y_{n} \\\\ \\end{pmatrix}^T \\begin{pmatrix} 1 & -2 & 1 & 0 & \\dots& \\\\ -2 & 5 & -4 & 1 & 0 &\\dots \\\\ 1 & -4 & 6 & -4 & 1 & \\\\ 0 & 1 & -4 & 6 & -4 & \\\\ \\vdots & 0 & \\ddots&\\ddots& \\ddots \\\\ & \\vdots & & & \\\\ & & & 1 & -4 & 6 & -4 & 1 \\\\ & & & & 1 & -4 & 5 & -2 \\\\ & & & & & 1 & -2 & 1& \\\\ & & & & & & & &1 & -2 & 1 & 0 & \\dots& \\\\ & & & & & & & &-2 & 5 & -4 & 1 & 0 &\\dots \\\\ & & & & & & & &1 & -4 & 6 & -4 & 1 & \\\\ & & & & & & & &0 & 1 & -4 & 6 & -4 & \\\\ & & & & & & & &\\vdots & 0 & \\ddots&\\ddots& \\ddots \\\\ & & & & & & & & & \\vdots & & & \\\\ & & & & & & & & & & & 1 & -4 & 6 & -4 & 1 \\\\ & & & & & & & & & & & & 1 & -4 & 5 & -2 \\\\ & & & & & & & & & & & & & 1 & -2 & 1& \\\\ \\end{pmatrix} \\begin{pmatrix} \\ x_0 \\\\ \\ x_1 \\\\ \\ x_2 \\\\ \\vdots \\\\ \\ x_{n-2}\\\\ \\ x_{n-1} \\\\ \\ x_{n} \\\\ \\ y_0 \\\\ \\ y_1 \\\\ \\ y_2 \\\\ \\vdots \\\\ \\ y_{n-2}\\\\ \\ y_{n-1} \\\\ \\ y_{n} \\\\ \\end{pmatrix} \\end{align}","title":"Formulation"},{"location":"planning/obstacle_avoidance_planner/#model-predictive-trajectory","text":"","title":"Model predictive trajectory"},{"location":"planning/obstacle_avoidance_planner/#abstract_1","text":"Model Predictive Trajectory (MPT) calculates the trajectory that realizes the following conditions. Kinematically feasible for linear vehicle kinematics model Collision free with obstacles and road boundaries Conditions for collision free is considered to be not hard constraints but soft constraints. When the optimization failed or the optimized trajectory is not collision free, the output trajectory will be previously generated trajectory. Trajectory near the ego must be stable, therefore the condition where trajectory points near the ego are the same as previously generated trajectory is considered, and this is the only hard constraints in MPT.","title":"Abstract"},{"location":"planning/obstacle_avoidance_planner/#vehicle-kinematics","text":"As the following figure, we consider the bicycle kinematics model in the frenet frame to track the reference path. At time step k k , we define lateral distance to the reference path, heading angle against the reference path, and steer angle as y_k y_k , \\theta_k \\theta_k , and \\delta_k \\delta_k respectively. Assuming that the commanded steer angle is \\delta_{des, k} \\delta_{des, k} , the kinematics model in the frenet frame is formulated as follows. We also assume that the steer angle \\delta_k \\delta_k is first-order lag to the commanded one. \\begin{align} y_{k+1} & = y_{k} + v \\sin \\theta_k dt \\\\ \\theta_{k+1} & = \\theta_k + \\frac{v \\tan \\delta_k}{L}dt - \\kappa_k v \\cos \\theta_k dt \\\\ \\delta_{k+1} & = \\delta_k - \\frac{\\delta_k - \\delta_{des,k}}{\\tau}dt \\end{align} \\begin{align} y_{k+1} & = y_{k} + v \\sin \\theta_k dt \\\\ \\theta_{k+1} & = \\theta_k + \\frac{v \\tan \\delta_k}{L}dt - \\kappa_k v \\cos \\theta_k dt \\\\ \\delta_{k+1} & = \\delta_k - \\frac{\\delta_k - \\delta_{des,k}}{\\tau}dt \\end{align}","title":"Vehicle kinematics"},{"location":"planning/obstacle_avoidance_planner/#linearization","text":"Then we linearize these equations. y_k y_k and \\theta_k \\theta_k are tracking errors, so we assume that those are small enough. Therefore \\sin \\theta_k \\approx \\theta_k \\sin \\theta_k \\approx \\theta_k . Since \\delta_k \\delta_k is a steer angle, it is not always small. By using a reference steer angle \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} calculated by the reference path curvature \\kappa_k \\kappa_k , we express \\delta_k \\delta_k with a small value \\Delta \\delta_k \\Delta \\delta_k . Note that the steer angle \\delta_k \\delta_k is within the steer angle limitation \\delta_{\\max} \\delta_{\\max} . When the reference steer angle \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} is larger than the steer angle limitation \\delta_{\\max} \\delta_{\\max} , and \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} is used to linearize the steer angle, \\Delta \\delta_k \\Delta \\delta_k is \\Delta \\delta_k = \\delta - \\delta_{\\mathrm{ref}, k} = \\delta_{\\max} - \\delta_{\\mathrm{ref}, k} \\Delta \\delta_k = \\delta - \\delta_{\\mathrm{ref}, k} = \\delta_{\\max} - \\delta_{\\mathrm{ref}, k} , and the absolute \\Delta \\delta_k \\Delta \\delta_k gets larger. Therefore, we have to apply the steer angle limitation to \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} as well. \\begin{align} \\delta_{\\mathrm{ref}, k} & = \\mathrm{clamp}(\\arctan(L \\kappa_k), -\\delta_{\\max}, \\delta_{\\max}) \\\\ \\delta_k & = \\delta_{\\mathrm{ref}, k} + \\Delta \\delta_k, \\ \\Delta \\delta_k \\ll 1 \\\\ \\end{align} \\begin{align} \\delta_{\\mathrm{ref}, k} & = \\mathrm{clamp}(\\arctan(L \\kappa_k), -\\delta_{\\max}, \\delta_{\\max}) \\\\ \\delta_k & = \\delta_{\\mathrm{ref}, k} + \\Delta \\delta_k, \\ \\Delta \\delta_k \\ll 1 \\\\ \\end{align} \\mathrm{clamp}(v, v_{\\min}, v_{\\max}) \\mathrm{clamp}(v, v_{\\min}, v_{\\max}) is a function to convert v v to be larger than v_{\\min} v_{\\min} and smaller than v_{\\max} v_{\\max} . Using this \\delta_{\\mathrm{ref}, k} \\delta_{\\mathrm{ref}, k} , \\tan \\delta_k \\tan \\delta_k is linearized as follows. \\begin{align} \\tan \\delta_k & \\approx \\tan \\delta_{\\mathrm{ref}, k} + \\left.\\frac{d \\tan \\delta}{d \\delta}\\right|_{\\delta = \\delta_{\\mathrm{ref}, k}} \\Delta \\delta_k \\\\ & = \\tan \\delta_{\\mathrm{ref}, k} + \\left.\\frac{d \\tan \\delta}{d \\delta}\\right|_{\\delta = \\delta_{\\mathrm{ref}, k}} (\\delta_{\\mathrm{ref}, k} - \\delta_k) \\\\ & = \\tan \\delta_{\\mathrm{ref}, k} - \\frac{\\delta_{\\mathrm{ref}, k}}{\\cos^2 \\delta_{\\mathrm{ref}, k}} + \\frac{1}{\\cos^2 \\delta_{\\mathrm{ref}, k}} \\delta_k \\end{align} \\begin{align} \\tan \\delta_k & \\approx \\tan \\delta_{\\mathrm{ref}, k} + \\left.\\frac{d \\tan \\delta}{d \\delta}\\right|_{\\delta = \\delta_{\\mathrm{ref}, k}} \\Delta \\delta_k \\\\ & = \\tan \\delta_{\\mathrm{ref}, k} + \\left.\\frac{d \\tan \\delta}{d \\delta}\\right|_{\\delta = \\delta_{\\mathrm{ref}, k}} (\\delta_{\\mathrm{ref}, k} - \\delta_k) \\\\ & = \\tan \\delta_{\\mathrm{ref}, k} - \\frac{\\delta_{\\mathrm{ref}, k}}{\\cos^2 \\delta_{\\mathrm{ref}, k}} + \\frac{1}{\\cos^2 \\delta_{\\mathrm{ref}, k}} \\delta_k \\end{align}","title":"Linearization"},{"location":"planning/obstacle_avoidance_planner/#one-step-state-equation","text":"Based on the linearization, the error kinematics is formulated with the following linear equations, \\begin{align} \\begin{pmatrix} y_{k+1} \\\\ \\theta_{k+1} \\end{pmatrix} = \\begin{pmatrix} 1 & v dt \\\\ 0 & 1 \\\\ \\end{pmatrix} \\begin{pmatrix} y_k \\\\ \\theta_k \\\\ \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ \\frac{v dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} \\\\ \\end{pmatrix} \\delta_{k} + \\begin{pmatrix} 0 \\\\ \\frac{v \\tan(\\delta_{\\mathrm{ref}, k}) dt}{L} - \\frac{v \\delta_{\\mathrm{ref}, k} dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} - \\kappa_k v dt\\\\ \\end{pmatrix} \\end{align} \\begin{align} \\begin{pmatrix} y_{k+1} \\\\ \\theta_{k+1} \\end{pmatrix} = \\begin{pmatrix} 1 & v dt \\\\ 0 & 1 \\\\ \\end{pmatrix} \\begin{pmatrix} y_k \\\\ \\theta_k \\\\ \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ \\frac{v dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} \\\\ \\end{pmatrix} \\delta_{k} + \\begin{pmatrix} 0 \\\\ \\frac{v \\tan(\\delta_{\\mathrm{ref}, k}) dt}{L} - \\frac{v \\delta_{\\mathrm{ref}, k} dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} - \\kappa_k v dt\\\\ \\end{pmatrix} \\end{align} which can be formulated as follows with the state \\boldsymbol{x} \\boldsymbol{x} , control input u u and some matrices, where \\boldsymbol{x} = (y_k, \\theta_k) \\boldsymbol{x} = (y_k, \\theta_k) \\begin{align} \\boldsymbol{x}_{k+1} = A_k \\boldsymbol{x}_k + \\boldsymbol{b}_k u_k + \\boldsymbol{w}_k \\end{align} \\begin{align} \\boldsymbol{x}_{k+1} = A_k \\boldsymbol{x}_k + \\boldsymbol{b}_k u_k + \\boldsymbol{w}_k \\end{align}","title":"One-step state equation"},{"location":"planning/obstacle_avoidance_planner/#time-series-state-equation","text":"Then, we formulate time-series state equation by concatenating states, control inputs and matrices respectively as \\begin{align} \\boldsymbol{x} = A \\boldsymbol{x}_0 + B \\boldsymbol{u} + \\boldsymbol{w} \\end{align} \\begin{align} \\boldsymbol{x} = A \\boldsymbol{x}_0 + B \\boldsymbol{u} + \\boldsymbol{w} \\end{align} where \\begin{align} \\boldsymbol{x} = (\\boldsymbol{x}^T_1, \\boldsymbol{x}^T_2, \\boldsymbol{x}^T_3, \\dots, \\boldsymbol{x}^T_{n-1})^T \\\\ \\boldsymbol{u} = (u_0, u_1, u_2, \\dots, u_{n-2})^T \\\\ \\boldsymbol{w} = (\\boldsymbol{w}^T_0, \\boldsymbol{w}^T_1, \\boldsymbol{w}^T_2, \\dots, \\boldsymbol{w}^T_{n-1})^T. \\\\ \\end{align} \\begin{align} \\boldsymbol{x} = (\\boldsymbol{x}^T_1, \\boldsymbol{x}^T_2, \\boldsymbol{x}^T_3, \\dots, \\boldsymbol{x}^T_{n-1})^T \\\\ \\boldsymbol{u} = (u_0, u_1, u_2, \\dots, u_{n-2})^T \\\\ \\boldsymbol{w} = (\\boldsymbol{w}^T_0, \\boldsymbol{w}^T_1, \\boldsymbol{w}^T_2, \\dots, \\boldsymbol{w}^T_{n-1})^T. \\\\ \\end{align} In detail, each matrices are constructed as follows. \\begin{align} \\begin{pmatrix} \\boldsymbol{x}_1 \\\\ \\boldsymbol{x}_2 \\\\ \\boldsymbol{x}_3 \\\\ \\vdots \\\\ \\boldsymbol{x}_{n-1} \\end{pmatrix} = \\begin{pmatrix} A_0 \\\\ A_1 A_0 \\\\ A_2 A_1 A_0\\\\ \\vdots \\\\ \\prod\\limits_{k=0}^{n-1} A_{k} \\end{pmatrix} \\boldsymbol{x}_0 + \\begin{pmatrix} B_0 & 0 & & \\dots & 0 \\\\ A_0 B_0 & B_1 & 0 & \\dots & 0 \\\\ A_1 A_0 B_0 & A_0 B_1 & B_2 & \\dots & 0 \\\\ \\vdots & \\vdots & & \\ddots & 0 \\\\ \\prod\\limits_{k=0}^{n-3} A_k B_0 & \\prod\\limits_{k=0}^{n-4} A_k B_1 & \\dots & A_0 B_{n-3} & B_{n-2} \\end{pmatrix} \\begin{pmatrix} u_0 \\\\ u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_{n-2} \\end{pmatrix} + \\begin{pmatrix} I & 0 & & \\dots & 0 \\\\ A_0 & I & 0 & \\dots & 0 \\\\ A_1 A_0 & A_0 & I & \\dots & 0 \\\\ \\vdots & \\vdots & & \\ddots & 0 \\\\ \\prod\\limits_{k=0}^{n-3} A_k & \\prod\\limits_{k=0}^{n-4} A_k & \\dots & A_0 & I \\end{pmatrix} \\begin{pmatrix} \\boldsymbol{w}_0 \\\\ \\boldsymbol{w}_1 \\\\ \\boldsymbol{w}_2 \\\\ \\vdots \\\\ \\boldsymbol{w}_{n-2} \\end{pmatrix} \\end{align} \\begin{align} \\begin{pmatrix} \\boldsymbol{x}_1 \\\\ \\boldsymbol{x}_2 \\\\ \\boldsymbol{x}_3 \\\\ \\vdots \\\\ \\boldsymbol{x}_{n-1} \\end{pmatrix} = \\begin{pmatrix} A_0 \\\\ A_1 A_0 \\\\ A_2 A_1 A_0\\\\ \\vdots \\\\ \\prod\\limits_{k=0}^{n-1} A_{k} \\end{pmatrix} \\boldsymbol{x}_0 + \\begin{pmatrix} B_0 & 0 & & \\dots & 0 \\\\ A_0 B_0 & B_1 & 0 & \\dots & 0 \\\\ A_1 A_0 B_0 & A_0 B_1 & B_2 & \\dots & 0 \\\\ \\vdots & \\vdots & & \\ddots & 0 \\\\ \\prod\\limits_{k=0}^{n-3} A_k B_0 & \\prod\\limits_{k=0}^{n-4} A_k B_1 & \\dots & A_0 B_{n-3} & B_{n-2} \\end{pmatrix} \\begin{pmatrix} u_0 \\\\ u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_{n-2} \\end{pmatrix} + \\begin{pmatrix} I & 0 & & \\dots & 0 \\\\ A_0 & I & 0 & \\dots & 0 \\\\ A_1 A_0 & A_0 & I & \\dots & 0 \\\\ \\vdots & \\vdots & & \\ddots & 0 \\\\ \\prod\\limits_{k=0}^{n-3} A_k & \\prod\\limits_{k=0}^{n-4} A_k & \\dots & A_0 & I \\end{pmatrix} \\begin{pmatrix} \\boldsymbol{w}_0 \\\\ \\boldsymbol{w}_1 \\\\ \\boldsymbol{w}_2 \\\\ \\vdots \\\\ \\boldsymbol{w}_{n-2} \\end{pmatrix} \\end{align}","title":"Time-series state equation"},{"location":"planning/obstacle_avoidance_planner/#free-boundary-conditioned-time-series-state-equation","text":"For path planning which does not start from the current ego pose, \\boldsymbol{x}_0 \\boldsymbol{x}_0 should be the design variable of optimization. Therefore, we make \\boldsymbol{u}' \\boldsymbol{u}' by concatenating \\boldsymbol{x}_0 \\boldsymbol{x}_0 and \\boldsymbol{u} \\boldsymbol{u} , and redefine \\boldsymbol{x} \\boldsymbol{x} as follows. \\begin{align} \\boldsymbol{u}' & = (\\boldsymbol{x}^T_0, \\boldsymbol{u}^T)^T \\\\ \\boldsymbol{x} & = (\\boldsymbol{x}^T_0, \\boldsymbol{x}^T_1, \\boldsymbol{x}^T_2, \\dots, \\boldsymbol{x}^T_{n-1})^T \\end{align} \\begin{align} \\boldsymbol{u}' & = (\\boldsymbol{x}^T_0, \\boldsymbol{u}^T)^T \\\\ \\boldsymbol{x} & = (\\boldsymbol{x}^T_0, \\boldsymbol{x}^T_1, \\boldsymbol{x}^T_2, \\dots, \\boldsymbol{x}^T_{n-1})^T \\end{align} Then we get the following state equation \\begin{align} \\boldsymbol{x}' = B \\boldsymbol{u}' + \\boldsymbol{w}, \\end{align} \\begin{align} \\boldsymbol{x}' = B \\boldsymbol{u}' + \\boldsymbol{w}, \\end{align} which is in detail \\begin{align} \\begin{pmatrix} \\boldsymbol{x}_0 \\\\ \\boldsymbol{x}_1 \\\\ \\boldsymbol{x}_2 \\\\ \\boldsymbol{x}_3 \\\\ \\vdots \\\\ \\boldsymbol{x}_{n-1} \\end{pmatrix} = \\begin{pmatrix} I & 0 & \\dots & & & 0 \\\\ A_0 & B_0 & 0 & & \\dots & 0 \\\\ A_1 A_0 & A_0 B_0 & B_1 & 0 & \\dots & 0 \\\\ A_2 A_1 A_0 & A_1 A_0 B_0 & A_0 B_1 & B_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\vdots & & \\ddots & 0 \\\\ \\prod\\limits_{k=0}^{n-1} A_k & \\prod\\limits_{k=0}^{n-3} A_k B_0 & \\prod\\limits_{k=0}^{n-4} A_k B_1 & \\dots & A_0 B_{n-3} & B_{n-2} \\end{pmatrix} \\begin{pmatrix} \\boldsymbol{x}_0 \\\\ u_0 \\\\ u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_{n-2} \\end{pmatrix} + \\begin{pmatrix} 0 & \\dots & & & 0 \\\\ I & 0 & & \\dots & 0 \\\\ A_0 & I & 0 & \\dots & 0 \\\\ A_1 A_0 & A_0 & I & \\dots & 0 \\\\ \\vdots & \\vdots & & \\ddots & 0 \\\\ \\prod\\limits_{k=0}^{n-3} A_k & \\prod\\limits_{k=0}^{n-4} A_k & \\dots & A_0 & I \\end{pmatrix} \\begin{pmatrix} \\boldsymbol{w}_0 \\\\ \\boldsymbol{w}_1 \\\\ \\boldsymbol{w}_2 \\\\ \\vdots \\\\ \\boldsymbol{w}_{n-2} \\end{pmatrix}. \\end{align} \\begin{align} \\begin{pmatrix} \\boldsymbol{x}_0 \\\\ \\boldsymbol{x}_1 \\\\ \\boldsymbol{x}_2 \\\\ \\boldsymbol{x}_3 \\\\ \\vdots \\\\ \\boldsymbol{x}_{n-1} \\end{pmatrix} = \\begin{pmatrix} I & 0 & \\dots & & & 0 \\\\ A_0 & B_0 & 0 & & \\dots & 0 \\\\ A_1 A_0 & A_0 B_0 & B_1 & 0 & \\dots & 0 \\\\ A_2 A_1 A_0 & A_1 A_0 B_0 & A_0 B_1 & B_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\vdots & & \\ddots & 0 \\\\ \\prod\\limits_{k=0}^{n-1} A_k & \\prod\\limits_{k=0}^{n-3} A_k B_0 & \\prod\\limits_{k=0}^{n-4} A_k B_1 & \\dots & A_0 B_{n-3} & B_{n-2} \\end{pmatrix} \\begin{pmatrix} \\boldsymbol{x}_0 \\\\ u_0 \\\\ u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_{n-2} \\end{pmatrix} + \\begin{pmatrix} 0 & \\dots & & & 0 \\\\ I & 0 & & \\dots & 0 \\\\ A_0 & I & 0 & \\dots & 0 \\\\ A_1 A_0 & A_0 & I & \\dots & 0 \\\\ \\vdots & \\vdots & & \\ddots & 0 \\\\ \\prod\\limits_{k=0}^{n-3} A_k & \\prod\\limits_{k=0}^{n-4} A_k & \\dots & A_0 & I \\end{pmatrix} \\begin{pmatrix} \\boldsymbol{w}_0 \\\\ \\boldsymbol{w}_1 \\\\ \\boldsymbol{w}_2 \\\\ \\vdots \\\\ \\boldsymbol{w}_{n-2} \\end{pmatrix}. \\end{align}","title":"Free-boundary-conditioned time-series state equation"},{"location":"planning/obstacle_avoidance_planner/#objective-function","text":"The objective function for smoothing and tracking is shown as follows, which can be formulated with value function matrices Q, R Q, R . \\begin{align} J_1 (\\boldsymbol{x}', \\boldsymbol{u}') & = w_y \\sum_{k} y_k^2 + w_{\\theta} \\sum_{k} \\theta_k^2 + w_{\\delta} \\sum_k \\delta_k^2 + w_{\\dot{\\delta}} \\sum_k \\dot{\\delta}_k^2 + w_{\\ddot{\\delta}} \\sum_k \\ddot{\\delta}_k^2 \\\\ & = \\boldsymbol{x}'^T Q \\boldsymbol{x}' + \\boldsymbol{u}'^T R \\boldsymbol{u}' \\\\ & = \\boldsymbol{u}'^T H \\boldsymbol{u}' + \\boldsymbol{u}'^T \\boldsymbol{f} \\end{align} \\begin{align} J_1 (\\boldsymbol{x}', \\boldsymbol{u}') & = w_y \\sum_{k} y_k^2 + w_{\\theta} \\sum_{k} \\theta_k^2 + w_{\\delta} \\sum_k \\delta_k^2 + w_{\\dot{\\delta}} \\sum_k \\dot{\\delta}_k^2 + w_{\\ddot{\\delta}} \\sum_k \\ddot{\\delta}_k^2 \\\\ & = \\boldsymbol{x}'^T Q \\boldsymbol{x}' + \\boldsymbol{u}'^T R \\boldsymbol{u}' \\\\ & = \\boldsymbol{u}'^T H \\boldsymbol{u}' + \\boldsymbol{u}'^T \\boldsymbol{f} \\end{align} As mentioned before, the constraints to be collision free with obstacles and road boundaries are formulated to be soft constraints. Assuming that the lateral distance to the road boundaries or obstacles from the back wheel center, front wheel center, and the point between them are y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k} y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k} respectively, and slack variables for each point are \\lambda_{\\mathrm{base}}, \\lambda_{\\mathrm{top}}, \\lambda_{\\mathrm{mid}} \\lambda_{\\mathrm{base}}, \\lambda_{\\mathrm{top}}, \\lambda_{\\mathrm{mid}} , the soft constraints can be formulated as follows. y_{\\mathrm{base}, k, \\min} - \\lambda_{\\mathrm{base}, k} \\leq y_{\\mathrm{base}, k} (y_k) \\leq y_{\\mathrm{base}, k, \\max} + \\lambda_{\\mathrm{base}, k}\\\\ y_{\\mathrm{top}, k, \\min} - \\lambda_{\\mathrm{top}, k} \\leq y_{\\mathrm{top}, k} (y_k) \\leq y_{\\mathrm{top}, k, \\max} + \\lambda_{\\mathrm{top}, k}\\\\ y_{\\mathrm{mid}, k, \\min} - \\lambda_{\\mathrm{mid}, k} \\leq y_{\\mathrm{mid}, k} (y_k) \\leq y_{\\mathrm{mid}, k, \\max} + \\lambda_{\\mathrm{mid}, k} \\\\ 0 \\leq \\lambda_{\\mathrm{base}, k} \\\\ 0 \\leq \\lambda_{\\mathrm{top}, k} \\\\ 0 \\leq \\lambda_{\\mathrm{mid}, k} y_{\\mathrm{base}, k, \\min} - \\lambda_{\\mathrm{base}, k} \\leq y_{\\mathrm{base}, k} (y_k) \\leq y_{\\mathrm{base}, k, \\max} + \\lambda_{\\mathrm{base}, k}\\\\ y_{\\mathrm{top}, k, \\min} - \\lambda_{\\mathrm{top}, k} \\leq y_{\\mathrm{top}, k} (y_k) \\leq y_{\\mathrm{top}, k, \\max} + \\lambda_{\\mathrm{top}, k}\\\\ y_{\\mathrm{mid}, k, \\min} - \\lambda_{\\mathrm{mid}, k} \\leq y_{\\mathrm{mid}, k} (y_k) \\leq y_{\\mathrm{mid}, k, \\max} + \\lambda_{\\mathrm{mid}, k} \\\\ 0 \\leq \\lambda_{\\mathrm{base}, k} \\\\ 0 \\leq \\lambda_{\\mathrm{top}, k} \\\\ 0 \\leq \\lambda_{\\mathrm{mid}, k} Since y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k} y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k} is formulated as a linear function of y_k y_k , the objective function for soft constraints is formulated as follows. \\begin{align} J_2 & (\\boldsymbol{\\lambda}_\\mathrm{base}, \\boldsymbol{\\lambda}_\\mathrm{top}, \\boldsymbol {\\lambda}_\\mathrm{mid})\\\\ & = w_{\\mathrm{base}} \\sum_{k} \\lambda_{\\mathrm{base}, k} + w_{\\mathrm{mid}} \\sum_k \\lambda_{\\mathrm{mid}, k} + w_{\\mathrm{top}} \\sum_k \\lambda_{\\mathrm{top}, k} \\end{align} \\begin{align} J_2 & (\\boldsymbol{\\lambda}_\\mathrm{base}, \\boldsymbol{\\lambda}_\\mathrm{top}, \\boldsymbol {\\lambda}_\\mathrm{mid})\\\\ & = w_{\\mathrm{base}} \\sum_{k} \\lambda_{\\mathrm{base}, k} + w_{\\mathrm{mid}} \\sum_k \\lambda_{\\mathrm{mid}, k} + w_{\\mathrm{top}} \\sum_k \\lambda_{\\mathrm{top}, k} \\end{align} Slack variables are also design variables for optimization. We define a vector \\boldsymbol{v} \\boldsymbol{v} , that concatenates all the design variables. \\begin{align} \\boldsymbol{v} = \\begin{pmatrix} \\boldsymbol{u}'^T & \\boldsymbol{\\lambda}_\\mathrm{base}^T & \\boldsymbol{\\lambda}_\\mathrm{top}^T & \\boldsymbol{\\lambda}_\\mathrm{mid}^T \\end{pmatrix}^T \\end{align} \\begin{align} \\boldsymbol{v} = \\begin{pmatrix} \\boldsymbol{u}'^T & \\boldsymbol{\\lambda}_\\mathrm{base}^T & \\boldsymbol{\\lambda}_\\mathrm{top}^T & \\boldsymbol{\\lambda}_\\mathrm{mid}^T \\end{pmatrix}^T \\end{align} The summation of these two objective functions is the objective function for the optimization problem. \\begin{align} \\min_{\\boldsymbol{v}} J (\\boldsymbol{v}) = \\min_{\\boldsymbol{v}} J_1 (\\boldsymbol{u}') + J_2 (\\boldsymbol{\\lambda}_\\mathrm{base}, \\boldsymbol{\\lambda}_\\mathrm{top}, \\boldsymbol{\\lambda}_\\mathrm{mid}) \\end{align} \\begin{align} \\min_{\\boldsymbol{v}} J (\\boldsymbol{v}) = \\min_{\\boldsymbol{v}} J_1 (\\boldsymbol{u}') + J_2 (\\boldsymbol{\\lambda}_\\mathrm{base}, \\boldsymbol{\\lambda}_\\mathrm{top}, \\boldsymbol{\\lambda}_\\mathrm{mid}) \\end{align} As mentioned before, we use hard constraints where some trajectory points in front of the ego are the same as the previously generated trajectory points. This hard constraints is formulated as follows. \\begin{align} \\delta_k = \\delta_{k}^{\\mathrm{prev}} (0 \\leq i \\leq N_{\\mathrm{fix}}) \\end{align} \\begin{align} \\delta_k = \\delta_{k}^{\\mathrm{prev}} (0 \\leq i \\leq N_{\\mathrm{fix}}) \\end{align} Finally we transform those objective functions to the following QP problem, and solve it. \\begin{align} \\min_{\\boldsymbol{v}} \\ & \\frac{1}{2} \\boldsymbol{v}^T \\boldsymbol{H} \\boldsymbol{v} + \\boldsymbol{f} \\boldsymbol{v} \\\\ \\mathrm{s.t.} \\ & \\boldsymbol{b}_{lower} \\leq \\boldsymbol{A} \\boldsymbol{v} \\leq \\boldsymbol{b}_{upper} \\end{align} \\begin{align} \\min_{\\boldsymbol{v}} \\ & \\frac{1}{2} \\boldsymbol{v}^T \\boldsymbol{H} \\boldsymbol{v} + \\boldsymbol{f} \\boldsymbol{v} \\\\ \\mathrm{s.t.} \\ & \\boldsymbol{b}_{lower} \\leq \\boldsymbol{A} \\boldsymbol{v} \\leq \\boldsymbol{b}_{upper} \\end{align}","title":"Objective function"},{"location":"planning/obstacle_avoidance_planner/#constraints","text":"","title":"Constraints"},{"location":"planning/obstacle_avoidance_planner/#steer-angle-limitation","text":"Steer angle has a certain limitation ( \\delta_{max} \\delta_{max} , \\delta_{min} \\delta_{min} ). Therefore we add linear inequality equations. \\begin{align} \\delta_{min} \\leq \\delta_i \\leq \\delta_{max} \\end{align} \\begin{align} \\delta_{min} \\leq \\delta_i \\leq \\delta_{max} \\end{align}","title":"Steer angle limitation"},{"location":"planning/obstacle_avoidance_planner/#collision-free","text":"To realize collision-free path planning, we have to formulate constraints that the vehicle is inside the road (moreover, a certain meter far from the road boundary) and does not collide with obstacles in linear equations. For linearity, we chose a method to approximate the vehicle shape with a set of circles, that is reliable and easy to implement. Now we formulate the linear constraints where a set of circles on each trajectory point is collision-free. For collision checking, we have a drivable area in the format of an image where walls or obstacles are filled with a color. By using this drivable area, we calculate upper (left) and lower (right) boundaries along reference points so that we can interpolate boundaries on any position on the trajectory. Assuming that upper and lower boundaries are b_l b_l , b_u b_u respectively, and r r is a radius of a circle, lateral deviation of the circle center y' y' has to be b_l + r \\leq y' \\leq b_u - r. b_l + r \\leq y' \\leq b_u - r. Based on the following figure, y' y' can be formulated as follows. \\begin{align} y' & = L \\sin(\\theta + \\beta) + y \\cos \\beta - l \\sin(\\gamma - \\phi_a) \\\\ & = L \\sin \\theta \\cos \\beta + L \\cos \\theta \\sin \\beta + y \\cos \\beta - l \\sin(\\gamma - \\phi_a) \\\\ & \\approx L \\theta \\cos \\beta + L \\sin \\beta + y \\cos \\beta - l \\sin(\\gamma - \\phi_a) \\end{align} \\begin{align} y' & = L \\sin(\\theta + \\beta) + y \\cos \\beta - l \\sin(\\gamma - \\phi_a) \\\\ & = L \\sin \\theta \\cos \\beta + L \\cos \\theta \\sin \\beta + y \\cos \\beta - l \\sin(\\gamma - \\phi_a) \\\\ & \\approx L \\theta \\cos \\beta + L \\sin \\beta + y \\cos \\beta - l \\sin(\\gamma - \\phi_a) \\end{align} b_l + r - \\lambda \\leq y' \\leq b_u - r + \\lambda. b_l + r - \\lambda \\leq y' \\leq b_u - r + \\lambda. \\begin{align} y' & = C_1 \\boldsymbol{x} + C_2 \\\\ & = C_1 (B \\boldsymbol{v} + \\boldsymbol{w}) + C_2 \\\\ & = C_1 B \\boldsymbol{v} + \\boldsymbol{w} + C_2 \\end{align} \\begin{align} y' & = C_1 \\boldsymbol{x} + C_2 \\\\ & = C_1 (B \\boldsymbol{v} + \\boldsymbol{w}) + C_2 \\\\ & = C_1 B \\boldsymbol{v} + \\boldsymbol{w} + C_2 \\end{align} Note that longitudinal position of the circle center and the trajectory point to calculate boundaries are different. But each boundaries are vertical against the trajectory, resulting in less distortion by the longitudinal position difference since road boundaries does not change so much. For example, if the boundaries are not vertical against the trajectory and there is a certain difference of longitudinal position between the circe center and the trajectory point, we can easily guess that there is much more distortion when comparing lateral deviation and boundaries. \\begin{align} A_{blk} & = \\begin{pmatrix} C_1 B & O & \\dots & O & I_{N_{ref} \\times N_{ref}} & O \\dots & O\\\\ -C_1 B & O & \\dots & O & I & O \\dots & O\\\\ O & O & \\dots & O & I & O \\dots & O \\end{pmatrix} \\in \\boldsymbol{R}^{3 N_{ref} \\times D_v + N_{circle} N_{ref}} \\\\ \\boldsymbol{b}_{lower, blk} & = \\begin{pmatrix} \\boldsymbol{b}_{lower} - C_1 \\boldsymbol{w} - C_2 \\\\ -\\boldsymbol{b}_{upper} + C_1 \\boldsymbol{w} + C_2 \\\\ O \\end{pmatrix} \\in \\boldsymbol{R}^{3 N_{ref}} \\\\ \\boldsymbol{b}_{upper, blk} & = \\boldsymbol{\\infty} \\in \\boldsymbol{R}^{3 N_{ref}} \\end{align} \\begin{align} A_{blk} & = \\begin{pmatrix} C_1 B & O & \\dots & O & I_{N_{ref} \\times N_{ref}} & O \\dots & O\\\\ -C_1 B & O & \\dots & O & I & O \\dots & O\\\\ O & O & \\dots & O & I & O \\dots & O \\end{pmatrix} \\in \\boldsymbol{R}^{3 N_{ref} \\times D_v + N_{circle} N_{ref}} \\\\ \\boldsymbol{b}_{lower, blk} & = \\begin{pmatrix} \\boldsymbol{b}_{lower} - C_1 \\boldsymbol{w} - C_2 \\\\ -\\boldsymbol{b}_{upper} + C_1 \\boldsymbol{w} + C_2 \\\\ O \\end{pmatrix} \\in \\boldsymbol{R}^{3 N_{ref}} \\\\ \\boldsymbol{b}_{upper, blk} & = \\boldsymbol{\\infty} \\in \\boldsymbol{R}^{3 N_{ref}} \\end{align} We will explain options for optimization.","title":"Collision free"},{"location":"planning/obstacle_avoidance_planner/#l-infinity-optimization","text":"The above formulation is called L2 norm for slack variables. Instead, if we use L-infinity norm where slack variables are shared by enabling l_inf_norm . \\begin{align} A_{blk} = \\begin{pmatrix} C_1 B & I_{N_{ref} \\times N_{ref}} \\\\ -C_1 B & I \\\\ O & I \\end{pmatrix} \\in \\boldsymbol{R}^{3N_{ref} \\times D_v + N_{ref}} \\end{align} \\begin{align} A_{blk} = \\begin{pmatrix} C_1 B & I_{N_{ref} \\times N_{ref}} \\\\ -C_1 B & I \\\\ O & I \\end{pmatrix} \\in \\boldsymbol{R}^{3N_{ref} \\times D_v + N_{ref}} \\end{align}","title":"L-infinity optimization"},{"location":"planning/obstacle_avoidance_planner/#two-step-soft-constraints","text":"\\begin{align} \\boldsymbol{v}' = \\begin{pmatrix} \\boldsymbol{v} \\\\ \\boldsymbol{\\lambda}^{soft_1} \\\\ \\boldsymbol{\\lambda}^{soft_2} \\\\ \\end{pmatrix} \\in \\boldsymbol{R}^{D_v + 2N_{slack}} \\end{align} \\begin{align} \\boldsymbol{v}' = \\begin{pmatrix} \\boldsymbol{v} \\\\ \\boldsymbol{\\lambda}^{soft_1} \\\\ \\boldsymbol{\\lambda}^{soft_2} \\\\ \\end{pmatrix} \\in \\boldsymbol{R}^{D_v + 2N_{slack}} \\end{align} * * depends on whether to use L2 norm or L-infinity optimization. \\begin{align} A_{blk} & = \\begin{pmatrix} A^{soft_1}_{blk} \\\\ A^{soft_2}_{blk} \\\\ \\end{pmatrix}\\\\ & = \\begin{pmatrix} C_1^{soft_1} B & & \\\\ -C_1^{soft_1} B & \\Huge{*} & \\Huge{O} \\\\ O & & \\\\ C_1^{soft_2} B & & \\\\ -C_1^{soft_2} B & \\Huge{O} & \\Huge{*} \\\\ O & & \\end{pmatrix} \\in \\boldsymbol{R}^{6 N_{ref} \\times D_v + 2 N_{slack}} \\end{align} \\begin{align} A_{blk} & = \\begin{pmatrix} A^{soft_1}_{blk} \\\\ A^{soft_2}_{blk} \\\\ \\end{pmatrix}\\\\ & = \\begin{pmatrix} C_1^{soft_1} B & & \\\\ -C_1^{soft_1} B & \\Huge{*} & \\Huge{O} \\\\ O & & \\\\ C_1^{soft_2} B & & \\\\ -C_1^{soft_2} B & \\Huge{O} & \\Huge{*} \\\\ O & & \\end{pmatrix} \\in \\boldsymbol{R}^{6 N_{ref} \\times D_v + 2 N_{slack}} \\end{align} N_{slack} N_{slack} is N_{circle} N_{circle} when L2 optimization, or 1 1 when L-infinity optimization. N_{circle} N_{circle} is the number of circles to check collision.","title":"Two-step soft constraints"},{"location":"planning/obstacle_avoidance_planner/#tuning","text":"","title":"Tuning"},{"location":"planning/obstacle_avoidance_planner/#vehicle","text":"max steering wheel degree mpt.kinematics.max_steer_deg","title":"Vehicle"},{"location":"planning/obstacle_avoidance_planner/#boundary-search","text":"advanced.mpt.bounds_search_widths In order to efficiently search precise lateral boundaries on each trajectory point, different resolutions of search widths are defined. By default, [0.45, 0.15, 0.05, 0.01] is used. In this case, the goal is to get the boundaries' length on each trajectory point with 0.01 [m] resolution. Firstly, lateral boundaries are searhed with a rough resolution (= 0.45 [m]). Then, within its 0.45 [m] resolution which boundaries are inside, they are searched again with a bit precise resolution (= 0.15 [m]). Following this rule, finally boundaries with 0.01 [m] will be found.","title":"Boundary search"},{"location":"planning/obstacle_avoidance_planner/#assumptions","text":"EB optimized trajectory length should be longer than MPT optimized trajectory length since MPT result may be jerky because of non-fixed reference path (= EB optimized trajectory) At least, EB fixed optimized trajectory length must be longer than MPT fixed optimization trajectory length This causes the case that there is a large difference between MPT fixed optimized point and MPT optimized point just after the point.","title":"Assumptions"},{"location":"planning/obstacle_avoidance_planner/#drivability-in-narrow-roads","text":"set option.drivability_check.use_vehicle_circles true use a set of circles as a shape of the vehicle when checking if the generated trajectory will be outside the drivable area. make mpt.clearance.soft_clearance_from_road smaller make mpt.kinematics.optimization_center_offset different The point on the vehicle, offset forward from the base link` tries to follow the reference path. This may cause the a part of generated trajectory will be outside the drivable area.","title":"Drivability in narrow roads"},{"location":"planning/obstacle_avoidance_planner/#computation-time","text":"Loose EB optimization make eb.common.delta_arc_length_for_eb large and eb.common.num_sampling_points_for_eb small This makes the number of design variables smaller Be careful about the trajectory length between MPT and EB as shown in Assumptions. However, empirically this causes large turn at the corner (e.g. The vehicle turns a steering wheel to the opposite side (=left) a bit just before the corner turning to right) make eb.qp.eps_abs and eb.qp.eps_rel small This causes very unstable reference path generation for MPT, or turning a steering wheel a little bit larger Enable computation reduction flag set l_inf_norm true (by default) use L-inf norm optimization for MPT w.r.t. slack variables, resulting in lower number of design variables set enable_warm_start true set enable_manual_warm_start true (by default) set steer_limit_constraint false This causes no assumption for trajectory generation where steering angle will not exceeds its hardware limitation make the number of collision-free constraints small How to change parameters depend on the type of collision-free constraints If This may cause the trajectory generation where a part of ego vehicle is out of drivable area Disable publishing debug visualization markers set option.is_publishing_* false","title":"Computation time"},{"location":"planning/obstacle_avoidance_planner/#robustness","text":"Check if the trajectory before EB, after EB, or after MPT is not robust if the trajectory before EB is not robust if the trajectory after EB is not robust if the trajectory after MPT is not robust make mpt.weight.steer_input_weight or mpt.weight.steer_rate_weight larger, which are stability of steering wheel along the trajectory.","title":"Robustness"},{"location":"planning/obstacle_avoidance_planner/#other-options","text":"option.skip_optimization skips EB and MPT optimization. option.enable_pre_smoothing enables EB which is smoothing the trajectory for MPT. EB is not required if the reference path for MPT is smooth enough and does not change its shape suddenly option.is_showing_calculation_time enables showing each calculation time for functions and total calculation time on the terminal. option.is_stopping_if_outside_drivable_area enables stopping just before the generated trajectory point will be outside the drivable area. mpt.option.plan_from_ego enables planning from the ego pose when the ego's velocity is zero. mpt.option.two_step_soft_constraint enables two step of soft constraints for collision free mpt.option.soft_clearance_from_road and mpt.option.soft_second_clearance_from_road are the weight.","title":"Other options"},{"location":"planning/obstacle_avoidance_planner/#limitation","text":"When turning right or left in the intersection, the output trajectory is close to the outside road boundary. Roles of planning for behavior_path_planner and obstacle_avoidance_planner are not decided clearly. High computation cost","title":"Limitation"},{"location":"planning/obstacle_avoidance_planner/#comparison-to-other-methods","text":"Planning a trajectory that satisfies kinematic feasibility and collision-free has two main characteristics that makes hard to be solved: one is non-convex and the other is high dimension. According to the characteristics, we investigate pros and cons of the typical planning methods: optimization-based, sampling-based, and learning-based method.","title":"Comparison to other methods"},{"location":"planning/obstacle_avoidance_planner/#optimization-based-method","text":"pros: comparatively fast against high dimension by leveraging the gradient descent cons: often converge to the local minima in the non-convex problem","title":"Optimization-based method"},{"location":"planning/obstacle_avoidance_planner/#sampling-based-method","text":"pros: realize global optimization cons: high computation cost especially in the complex case","title":"Sampling-based method"},{"location":"planning/obstacle_avoidance_planner/#learning-based-method","text":"under research yet Based on these pros/cons, we chose the optimization-based planner first. Although it has a cons to converge to the local minima, it can get a good solution by the preprocessing to approximate the convex problem that almost equals to the original non-convex problem.","title":"Learning-based method"},{"location":"planning/obstacle_avoidance_planner/#how-to-debug","text":"Topics for debugging will be explained in this section. Drivable area Drivable area to cover the road. Whether this area is continuous and covers the road can be checked. /planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner/debug/drivable_area , whose type is nav_msgs/msg/OccupancyGrid Path from behavior The input path of obstacle_avoidance_planner. Whether this path is continuous and the curvature is not so high can be checked. Path or PathFootprint rviz plugin. EB trajectory The output trajectory of elastic band. Whether this trajectory is very smooth and a sampling width is constant can be checked. Trajectory or TrajectoryFootprint rviz plugin. MPT reference trajectory The reference trajectory of model predictive trajectory. Whether this trajectory is very smooth and a sampling width is constant can be checked. Trajectory or TrajectoryFootprint rviz plugin. MPT fixed trajectory The fixed trajectory around the ego of model predictive trajectory. Trajectory or TrajectoryFootprint rviz plugin. bounds Lateral Distance to the road or object boundaries to check collision in model predictive trajectory. Whether these lines' ends align the road or obstacle boundaries can be checked. bounds* of /planning/scenario_planning/lane_driving/motion_planning/obstacle_avoidance_planner/debug/marker whose type is visualization_msgs/msg/MarkerArray MPT trajectory The output of model predictive trajectory. Whether this trajectory is smooth enough and inside the drivable area can be checked. Trajectory or TrajectoryFootprint rviz plugin. Output trajectory The output of obstacle_avoidance_planner. Whether this trajectory is smooth enough can be checked. Trajectory or TrajectoryFootprint rviz plugin.","title":"How to debug"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/","text":"Obstacle Velocity Planner # Overview # The obstacle_cruise_planner package has following modules. obstacle stop planning inserting a stop point in the trajectory when there is a static obstacle on the trajectory. adaptive cruise planning sending an external velocity limit to motion_velocity_smoother when there is a dynamic obstacle to cruise on the trajectory Interfaces # Input topics # Name Type Description ~/input/trajectory autoware_auto_planning_msgs::Trajectory input trajectory ~/input/smoothed_trajectory autoware_auto_planning_msgs::Trajectory trajectory with smoothed velocity ~/input/objects autoware_auto_perception_msgs::PredictedObjects dynamic objects ~/input/odometry nav_msgs::msg::Odometry ego odometry Output topics # Name Type Description ~output/trajectory autoware_auto_planning_msgs::Trajectory output trajectory ~output/velocity_limit tier4_planning_msgs::VelocityLimit velocity limit for cruising ~output/clear_velocity_limit tier4_planning_msgs::VelocityLimitClearCommand clear command for velocity limit ~output/stop_reasons tier4_planning_msgs::StopReasonArray reasons that make the vehicle to stop Design # Design for the following functions is defined here. Obstacle candidates selection Obstacle stop planning Adaptive cruise planning A data structure for cruise and stop planning is as follows. This planner data is created first, and then sent to the planning algorithm. struct ObstacleCruisePlannerData { rclcpp :: Time current_time ; autoware_auto_planning_msgs :: msg :: Trajectory traj ; geometry_msgs :: msg :: Pose current_pose ; double current_vel ; double current_acc ; std :: vector < TargetObstacle > target_obstacles ; }; struct TargetObstacle { rclcpp :: Time time_stamp ; bool orientation_reliable ; geometry_msgs :: msg :: Pose pose ; bool velocity_reliable ; float velocity ; bool is_classified ; ObjectClassification classification ; Shape shape ; std :: vector < PredictedPath > predicted_paths ; geometry_msgs :: msg :: Point collision_point ; }; Obstacle candidates selection # In this function, target obstacles for stopping or cruising are selected based on their pose and velocity. By default, objects that realize one of the following conditions are considered to be the target obstacle candidates. Some terms will be defined in the following subsections. Vehicle objects \"inside the detection area\" other than \"far crossing vehicles\". non vehicle objects \"inside the detection area\" \"Near cut-in vehicles\" outside the detection area Note that currently the obstacle candidates selection algorithm is for autonomous driving. However, we have following parameters as well for stop and cruise respectively so that we can extend the obstacles candidates selection algorithm for non vehicle robots. By default, unknown and vehicles are obstacles to cruise and stop, and non vehicles are obstacles just to stop. Parameter Type Description cruise_obstacle_type.unknown bool flag to consider unknown objects as being cruised cruise_obstacle_type.car bool flag to consider unknown objects as being cruised cruise_obstacle_type.truck bool flag to consider unknown objects as being cruised ... bool ... stop_obstacle_type.unknown bool flag to consider unknown objects as being stopped ... bool ... Inside the detection area # To calculate obstacles inside the detection area, firstly, obstacles whose distance to the trajectory is less than rough_detection_area_expand_width are selected. Then, the detection area, which is a trajectory with some lateral margin, is calculated as shown in the figure. The detection area width is a vehicle's width + detection_area_expand_width , and it is represented as a polygon resampled with decimate_trajectory_step_length longitudinally. The roughly selected obstacles inside the detection area are considered as inside the detection area. This two-step detection is used for calculation efficiency since collision checking of polygons is heavy. Boost.Geometry is used as a library to check collision among polygons. In the obstacle_filtering namespace, Parameter Type Description rough_detection_area_expand_width double rough lateral margin for rough detection area expansion [m] detection_area_expand_width double lateral margin for precise detection area expansion [m] decimate_trajectory_step_length double longitudinal step length to calculate trajectory polygon for collision checking [m] Far crossing vehicles # Near crossing vehicles (= not far crossing vehicles) are defined as vehicle objects realizing either of following conditions. whose yaw angle against the nearest trajectory point is greater than crossing_obstacle_traj_angle_threshold whose velocity is less than crossing_obstacle_velocity_threshold . Assuming t_1 to be the time for the ego to reach the current crossing obstacle position with the constant velocity motion, and t_2 to be the time for the crossing obstacle to go outside the detection area, if the following condition is realized, the crossing vehicle will be ignored. t_1 - t_2 > \\mathrm{margin\\_for\\_collision\\_time} t_1 - t_2 > \\mathrm{margin\\_for\\_collision\\_time} In the obstacle_filtering namespace, Parameter Type Description crossing_obstacle_velocity_threshold double velocity threshold to decide crossing obstacle [m/s] crossing_obstacle_traj_angle_threshold double yaw threshold of crossing obstacle against the nearest trajectory point [rad] collision_time_margin double time threshold of collision between obstacle and ego [s] Near Cut-in vehicles # Near Cut-in vehicles are defined as vehicle objects whose predicted path's footprints from the current time to max_prediction_time_for_collision_check overlap with the detection area longer than ego_obstacle_overlap_time_threshold . In the obstacle_filtering namespace, Parameter Type Description ego_obstacle_overlap_time_threshold double time threshold to decide cut-in obstacle for cruise or stop [s] max_prediction_time_for_collision_check double prediction time to check collision between obstacle and ego [s] Stop planning # Parameter Type Description common.min_strong_accel double ego's minimum acceleration to stop [m/ss] common.safe_distance_margin double distance with obstacles for stop [m] The role of the stop planning is keeping a safe distance with static vehicle objects or dynamic/static non vehicle objects. The stop planning just inserts the stop point in the trajectory to keep a distance with obstacles inside the detection area. The safe distance is parameterized as common.safe_distance_margin . When inserting the stop point, the required acceleration for the ego to stop in front of the stop point is calculated. If the acceleration is less than common.min_strong_accel , the stop planning will be cancelled since this package does not assume a strong sudden brake for emergency. Adaptive cruise planning # Parameter Type Description common.safe_distance_margin double minimum distance with obstacles for cruise [m] The role of the adaptive cruise planning is keeping a safe distance with dynamic vehicle objects with smoothed velocity transition. This includes not only cruising a front vehicle, but also reacting a cut-in and cut-out vehicle. The safe distance is calculated dynamically based on the Responsibility-Sensitive Safety (RSS) by the following equation. d = v_{ego} t_{idling} + \\frac{1}{2} a_{ego} t_{idling}^2 + \\frac{v_{ego}^2}{2 a_{ego}} - \\frac{v_{obstacle}^2}{2 a_{obstacle}}, d = v_{ego} t_{idling} + \\frac{1}{2} a_{ego} t_{idling}^2 + \\frac{v_{ego}^2}{2 a_{ego}} - \\frac{v_{obstacle}^2}{2 a_{obstacle}}, assuming that d d is the calculated safe distance, t_{idling} t_{idling} is the idling time for the ego to detect the front vehicle's deceleration, v_{ego} v_{ego} is the ego's current velocity, v_{obstacle} v_{obstacle} is the front obstacle's current velocity, a_{ego} a_{ego} is the ego's acceleration, and a_{obstacle} a_{obstacle} is the obstacle's acceleration. These values are parameterized as follows. Other common values such as ego's minimum acceleration is defined in common.param.yaml . Parameter Type Description common.idling_time double idling time for the ego to detect the front vehicle starting deceleration [s] common.min_object_accel_for_rss double front obstacle's acceleration [m/ss] Implementation # Flowchart # Successive functions consist of obstacle_cruise_planner as follows. Various algorithms for stop and cruise planning will be implemented, and one of them is designated depending on the use cases. The core algorithm implementation generateTrajectory depends on the designated algorithm. Algorithm selection # Currently, only a PID-based planner is supported. Each planner will be explained in the following. Parameter Type Description common.planning_method string cruise and stop planning algorithm, selected from \"pid_base\" PID-based planner # Stop planning # In the pid_based_planner namespace, Parameter Type Description obstacle_velocity_threshold_from_cruise_to_stop double obstacle velocity threshold to be stopped from cruised [m/s] Only one obstacle is targeted for the stop planning. It is the obstacle among obstacle candidates whose velocity is less than obstacle_velocity_threshold_from_cruise_to_stop , and which is the nearest to the ego along the trajectory. A stop point is inserted keeping common.safe_distance_margin distance between the ego and obstacle. Note that, as explained in the stop planning design, a stop planning which requires a strong acceleration (less than common.min_strong_accel ) will be canceled. Adaptive cruise planning # In the pid_based_planner namespace, Parameter Type Description kp double p gain for pid control [-] ki double i gain for pid control [-] kd double d gain for pid control [-] output_ratio_during_accel double The output velocity will be multiplied by the ratio during acceleration to follow the front vehicle. [-] vel_to_acc_weight double target acceleration is target velocity * vel_to_acc_weight [-] min_cruise_target_vel double minimum target velocity during cruise [m/s] In order to keep the safe distance, the target velocity and acceleration is calculated and sent as an external velocity limit to the velocity smoothing package ( motion_velocity_smoother by default). The target velocity and acceleration is respectively calculated with the PID controller according to the error between the reference safe distance and the actual distance. Optimization-based planner # under construction Minor functions # Prioritization of behavior module's stop point # When stopping for a pedestrian walking on the crosswalk, the behavior module inserts the zero velocity in the trajectory in front of the crosswalk. Also obstacle_cruise_planner 's stop planning also works, and the ego may not reach the behavior module's stop point since the safe distance defined in obstacle_cruise_planner may be longer than the behavior module's safe distance. To resolve this non-alignment of the stop point between the behavior module and obstacle_cruise_planner , common.min_behavior_stop_margin is defined. In the case of the crosswalk described above, obstacle_cruise_planner inserts the stop point with a distance common.min_behavior_stop_margin at minimum between the ego and obstacle. Parameter Type Description common.min_behavior_stop_margin double minimum stop margin when stopping with the behavior module enabled [m] Visualization for debugging # Detection area # Green polygons which is a detection area is visualized by detection_polygons in the ~/debug/marker topic. Collision point # Red point which is a collision point with obstacle is visualized by collision_points in the ~/debug/marker topic. Obstacle for cruise # Yellow sphere which is a obstacle for cruise is visualized by obstacles_to_cruise in the ~/debug/marker topic. Obstacle for stop # Red sphere which is a obstacle for stop is visualized by obstacles_to_stop in the ~/debug/marker topic. Obstacle cruise wall # Yellow wall which means a safe distance to cruise if the ego's front meets the wall is visualized in the ~/debug/cruise_wall_marker topic. Obstacle stop wall # Red wall which means a safe distance to stop if the ego's front meets the wall is visualized in the ~/debug/stop_wall_marker topic. Known Limits # Common When the obstacle pose or velocity estimation has a delay, the ego sometimes will go close to the front vehicle keeping deceleration. Current implementation only uses predicted objects message for static/dynamic obstacles and does not use pointcloud. Therefore, if object recognition is lost, the ego cannot deal with the lost obstacle. PID-based planner The algorithm strongly depends on the velocity smoothing package ( motion_velocity_smoother by default) whether or not the ego realizes the designated target speed. If the velocity smoothing package is updated, please take care of the vehicle's behavior as much as possible.","title":"Obstacle Velocity Planner"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#obstacle-velocity-planner","text":"","title":"Obstacle Velocity Planner"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#overview","text":"The obstacle_cruise_planner package has following modules. obstacle stop planning inserting a stop point in the trajectory when there is a static obstacle on the trajectory. adaptive cruise planning sending an external velocity limit to motion_velocity_smoother when there is a dynamic obstacle to cruise on the trajectory","title":"Overview"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#interfaces","text":"","title":"Interfaces"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#input-topics","text":"Name Type Description ~/input/trajectory autoware_auto_planning_msgs::Trajectory input trajectory ~/input/smoothed_trajectory autoware_auto_planning_msgs::Trajectory trajectory with smoothed velocity ~/input/objects autoware_auto_perception_msgs::PredictedObjects dynamic objects ~/input/odometry nav_msgs::msg::Odometry ego odometry","title":"Input topics"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#output-topics","text":"Name Type Description ~output/trajectory autoware_auto_planning_msgs::Trajectory output trajectory ~output/velocity_limit tier4_planning_msgs::VelocityLimit velocity limit for cruising ~output/clear_velocity_limit tier4_planning_msgs::VelocityLimitClearCommand clear command for velocity limit ~output/stop_reasons tier4_planning_msgs::StopReasonArray reasons that make the vehicle to stop","title":"Output topics"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#design","text":"Design for the following functions is defined here. Obstacle candidates selection Obstacle stop planning Adaptive cruise planning A data structure for cruise and stop planning is as follows. This planner data is created first, and then sent to the planning algorithm. struct ObstacleCruisePlannerData { rclcpp :: Time current_time ; autoware_auto_planning_msgs :: msg :: Trajectory traj ; geometry_msgs :: msg :: Pose current_pose ; double current_vel ; double current_acc ; std :: vector < TargetObstacle > target_obstacles ; }; struct TargetObstacle { rclcpp :: Time time_stamp ; bool orientation_reliable ; geometry_msgs :: msg :: Pose pose ; bool velocity_reliable ; float velocity ; bool is_classified ; ObjectClassification classification ; Shape shape ; std :: vector < PredictedPath > predicted_paths ; geometry_msgs :: msg :: Point collision_point ; };","title":"Design"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#obstacle-candidates-selection","text":"In this function, target obstacles for stopping or cruising are selected based on their pose and velocity. By default, objects that realize one of the following conditions are considered to be the target obstacle candidates. Some terms will be defined in the following subsections. Vehicle objects \"inside the detection area\" other than \"far crossing vehicles\". non vehicle objects \"inside the detection area\" \"Near cut-in vehicles\" outside the detection area Note that currently the obstacle candidates selection algorithm is for autonomous driving. However, we have following parameters as well for stop and cruise respectively so that we can extend the obstacles candidates selection algorithm for non vehicle robots. By default, unknown and vehicles are obstacles to cruise and stop, and non vehicles are obstacles just to stop. Parameter Type Description cruise_obstacle_type.unknown bool flag to consider unknown objects as being cruised cruise_obstacle_type.car bool flag to consider unknown objects as being cruised cruise_obstacle_type.truck bool flag to consider unknown objects as being cruised ... bool ... stop_obstacle_type.unknown bool flag to consider unknown objects as being stopped ... bool ...","title":"Obstacle candidates selection"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#inside-the-detection-area","text":"To calculate obstacles inside the detection area, firstly, obstacles whose distance to the trajectory is less than rough_detection_area_expand_width are selected. Then, the detection area, which is a trajectory with some lateral margin, is calculated as shown in the figure. The detection area width is a vehicle's width + detection_area_expand_width , and it is represented as a polygon resampled with decimate_trajectory_step_length longitudinally. The roughly selected obstacles inside the detection area are considered as inside the detection area. This two-step detection is used for calculation efficiency since collision checking of polygons is heavy. Boost.Geometry is used as a library to check collision among polygons. In the obstacle_filtering namespace, Parameter Type Description rough_detection_area_expand_width double rough lateral margin for rough detection area expansion [m] detection_area_expand_width double lateral margin for precise detection area expansion [m] decimate_trajectory_step_length double longitudinal step length to calculate trajectory polygon for collision checking [m]","title":"Inside the detection area"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#far-crossing-vehicles","text":"Near crossing vehicles (= not far crossing vehicles) are defined as vehicle objects realizing either of following conditions. whose yaw angle against the nearest trajectory point is greater than crossing_obstacle_traj_angle_threshold whose velocity is less than crossing_obstacle_velocity_threshold . Assuming t_1 to be the time for the ego to reach the current crossing obstacle position with the constant velocity motion, and t_2 to be the time for the crossing obstacle to go outside the detection area, if the following condition is realized, the crossing vehicle will be ignored. t_1 - t_2 > \\mathrm{margin\\_for\\_collision\\_time} t_1 - t_2 > \\mathrm{margin\\_for\\_collision\\_time} In the obstacle_filtering namespace, Parameter Type Description crossing_obstacle_velocity_threshold double velocity threshold to decide crossing obstacle [m/s] crossing_obstacle_traj_angle_threshold double yaw threshold of crossing obstacle against the nearest trajectory point [rad] collision_time_margin double time threshold of collision between obstacle and ego [s]","title":"Far crossing vehicles"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#near-cut-in-vehicles","text":"Near Cut-in vehicles are defined as vehicle objects whose predicted path's footprints from the current time to max_prediction_time_for_collision_check overlap with the detection area longer than ego_obstacle_overlap_time_threshold . In the obstacle_filtering namespace, Parameter Type Description ego_obstacle_overlap_time_threshold double time threshold to decide cut-in obstacle for cruise or stop [s] max_prediction_time_for_collision_check double prediction time to check collision between obstacle and ego [s]","title":"Near Cut-in vehicles"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#stop-planning","text":"Parameter Type Description common.min_strong_accel double ego's minimum acceleration to stop [m/ss] common.safe_distance_margin double distance with obstacles for stop [m] The role of the stop planning is keeping a safe distance with static vehicle objects or dynamic/static non vehicle objects. The stop planning just inserts the stop point in the trajectory to keep a distance with obstacles inside the detection area. The safe distance is parameterized as common.safe_distance_margin . When inserting the stop point, the required acceleration for the ego to stop in front of the stop point is calculated. If the acceleration is less than common.min_strong_accel , the stop planning will be cancelled since this package does not assume a strong sudden brake for emergency.","title":"Stop planning"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#adaptive-cruise-planning","text":"Parameter Type Description common.safe_distance_margin double minimum distance with obstacles for cruise [m] The role of the adaptive cruise planning is keeping a safe distance with dynamic vehicle objects with smoothed velocity transition. This includes not only cruising a front vehicle, but also reacting a cut-in and cut-out vehicle. The safe distance is calculated dynamically based on the Responsibility-Sensitive Safety (RSS) by the following equation. d = v_{ego} t_{idling} + \\frac{1}{2} a_{ego} t_{idling}^2 + \\frac{v_{ego}^2}{2 a_{ego}} - \\frac{v_{obstacle}^2}{2 a_{obstacle}}, d = v_{ego} t_{idling} + \\frac{1}{2} a_{ego} t_{idling}^2 + \\frac{v_{ego}^2}{2 a_{ego}} - \\frac{v_{obstacle}^2}{2 a_{obstacle}}, assuming that d d is the calculated safe distance, t_{idling} t_{idling} is the idling time for the ego to detect the front vehicle's deceleration, v_{ego} v_{ego} is the ego's current velocity, v_{obstacle} v_{obstacle} is the front obstacle's current velocity, a_{ego} a_{ego} is the ego's acceleration, and a_{obstacle} a_{obstacle} is the obstacle's acceleration. These values are parameterized as follows. Other common values such as ego's minimum acceleration is defined in common.param.yaml . Parameter Type Description common.idling_time double idling time for the ego to detect the front vehicle starting deceleration [s] common.min_object_accel_for_rss double front obstacle's acceleration [m/ss]","title":"Adaptive cruise planning"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#implementation","text":"","title":"Implementation"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#flowchart","text":"Successive functions consist of obstacle_cruise_planner as follows. Various algorithms for stop and cruise planning will be implemented, and one of them is designated depending on the use cases. The core algorithm implementation generateTrajectory depends on the designated algorithm.","title":"Flowchart"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#algorithm-selection","text":"Currently, only a PID-based planner is supported. Each planner will be explained in the following. Parameter Type Description common.planning_method string cruise and stop planning algorithm, selected from \"pid_base\"","title":"Algorithm selection"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#pid-based-planner","text":"","title":"PID-based planner"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#stop-planning_1","text":"In the pid_based_planner namespace, Parameter Type Description obstacle_velocity_threshold_from_cruise_to_stop double obstacle velocity threshold to be stopped from cruised [m/s] Only one obstacle is targeted for the stop planning. It is the obstacle among obstacle candidates whose velocity is less than obstacle_velocity_threshold_from_cruise_to_stop , and which is the nearest to the ego along the trajectory. A stop point is inserted keeping common.safe_distance_margin distance between the ego and obstacle. Note that, as explained in the stop planning design, a stop planning which requires a strong acceleration (less than common.min_strong_accel ) will be canceled.","title":"Stop planning"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#adaptive-cruise-planning_1","text":"In the pid_based_planner namespace, Parameter Type Description kp double p gain for pid control [-] ki double i gain for pid control [-] kd double d gain for pid control [-] output_ratio_during_accel double The output velocity will be multiplied by the ratio during acceleration to follow the front vehicle. [-] vel_to_acc_weight double target acceleration is target velocity * vel_to_acc_weight [-] min_cruise_target_vel double minimum target velocity during cruise [m/s] In order to keep the safe distance, the target velocity and acceleration is calculated and sent as an external velocity limit to the velocity smoothing package ( motion_velocity_smoother by default). The target velocity and acceleration is respectively calculated with the PID controller according to the error between the reference safe distance and the actual distance.","title":"Adaptive cruise planning"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#optimization-based-planner","text":"under construction","title":"Optimization-based planner"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#minor-functions","text":"","title":"Minor functions"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#prioritization-of-behavior-modules-stop-point","text":"When stopping for a pedestrian walking on the crosswalk, the behavior module inserts the zero velocity in the trajectory in front of the crosswalk. Also obstacle_cruise_planner 's stop planning also works, and the ego may not reach the behavior module's stop point since the safe distance defined in obstacle_cruise_planner may be longer than the behavior module's safe distance. To resolve this non-alignment of the stop point between the behavior module and obstacle_cruise_planner , common.min_behavior_stop_margin is defined. In the case of the crosswalk described above, obstacle_cruise_planner inserts the stop point with a distance common.min_behavior_stop_margin at minimum between the ego and obstacle. Parameter Type Description common.min_behavior_stop_margin double minimum stop margin when stopping with the behavior module enabled [m]","title":"Prioritization of behavior module's stop point"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#visualization-for-debugging","text":"","title":"Visualization for debugging"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#detection-area","text":"Green polygons which is a detection area is visualized by detection_polygons in the ~/debug/marker topic.","title":"Detection area"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#collision-point","text":"Red point which is a collision point with obstacle is visualized by collision_points in the ~/debug/marker topic.","title":"Collision point"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#obstacle-for-cruise","text":"Yellow sphere which is a obstacle for cruise is visualized by obstacles_to_cruise in the ~/debug/marker topic.","title":"Obstacle for cruise"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#obstacle-for-stop","text":"Red sphere which is a obstacle for stop is visualized by obstacles_to_stop in the ~/debug/marker topic.","title":"Obstacle for stop"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#obstacle-cruise-wall","text":"Yellow wall which means a safe distance to cruise if the ego's front meets the wall is visualized in the ~/debug/cruise_wall_marker topic.","title":"Obstacle cruise wall"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#obstacle-stop-wall","text":"Red wall which means a safe distance to stop if the ego's front meets the wall is visualized in the ~/debug/stop_wall_marker topic.","title":"Obstacle stop wall"},{"location":"planning/obstacle_cruise_planner/obstacle_cruise_planner-design/#known-limits","text":"Common When the obstacle pose or velocity estimation has a delay, the ego sometimes will go close to the front vehicle keeping deceleration. Current implementation only uses predicted objects message for static/dynamic obstacles and does not use pointcloud. Therefore, if object recognition is lost, the ego cannot deal with the lost obstacle. PID-based planner The algorithm strongly depends on the velocity smoothing package ( motion_velocity_smoother by default) whether or not the ego realizes the designated target speed. If the velocity smoothing package is updated, please take care of the vehicle's behavior as much as possible.","title":"Known Limits"},{"location":"planning/obstacle_stop_planner/","text":"Obstacle Stop Planner # Overview # obstacle_stop_planner has following modules Obstacle Stop Planner inserts a stop point in trajectory when there is a static point cloud on the trajectory. Slow Down Planner inserts a deceleration section in trajectory when there is a point cloud near the trajectory. Adaptive Cruise Controller (ACC) embeds target velocity in trajectory when there is a dynamic point cloud on the trajectory. In order to stop with a stop margin from the obstacle exists, the stop point ( v=0 ) is inserted at a distance of baselink to front + stop margin from the obstacle. The baselink to front means the distance between base_link (center of rear-wheel axis) and front of the car. If a stop point has already been inserted by other nodes between the obstacle and a position which is stop margin meters away from the obstacle, the stop point is inserted at a distance of baselink to front + min behavior stop margin from the obstacle. When the deceleration section is inserted, the start point of the section is inserted in front of the target point cloud by the distance of baselink to front + slow down forward margin . the end point of the section is inserted behind the target point cloud by the distance of slow down backward margin + baselink to rear . The baselink to rear means the distance between base_link and rear of the car. The velocities of points in the deceleration section are modified to the deceleration velocity. slow down backward margin and slow down forward margin are determined by the parameters described below. Input topics # Name Type Description ~/input/pointcloud sensor_msgs::PointCloud2 obstacle pointcloud ~/input/trajectory autoware_auto_planning_msgs::Trajectory trajectory ~/input/vector_map autoware_auto_mapping_msgs::HADMapBin vector map ~/input/odometry nav_msgs::Odometry vehicle velocity ~/input/dynamic_objects autoware_auto_perception_msgs::PredictedObjects dynamic objects ~/input/expand_stop_range tier4_planning_msgs::msg::ExpandStopRange expand stop range Output topics # Name Type Description ~output/trajectory autoware_auto_planning_msgs::Trajectory trajectory to be followed ~output/stop_reasons tier4_planning_msgs::StopReasonArray reasons that cause the vehicle to stop Modules # Obstacle Stop Planner # Role # Obstacle Stop Planner module inserts a stop point in trajectory when there is a static point cloud on the trajectory. This module does not work when Adaptive Cruise Controller works. Parameter Type Description stop_planner.stop_margin double stop margin distance from obstacle on the path [m] stop_planner.min_behavior_stop_margin double stop margin distance when any other stop point is inserted in stop margin [m] stop_planner.step_length double step length for pointcloud search range [m] stop_planner.extend_distance double extend trajectory to consider after goal obstacle in the extend_distance [m] stop_planner.expand_stop_range double margin of vehicle footprint [m] Flowchart # First, this module cut off the trajectory behind the car and decimates the points of trajectory for reducing computational costs. Then, a detection area is generated by the decimated trajectory as following figure. The detection area means the area through which the vehicle-body passes. The module searches the obstacle pointcloud within detection area. When the pointcloud is found, Adaptive Cruise Controller modules starts to work. only when Adaptive Cruise Controller modules does not insert target velocity, the stop point is inserted to the trajectory. The stop point means the point with 0 velocity. Slow Down Planner # Role # Slow Down Planner module inserts a deceleration point in trajectory when there is a point cloud near the trajectory. Parameter Type Description slow_down_planner.slow_down_forward_margin double margin distance from slow down point to vehicle front [m] slow_down_planner.slow_down_backward_margin double margin distance from slow down point to vehicle rear [m] slow_down_planner.expand_slow_down_range double offset from vehicle side edge for expanding the search area of the surrounding point cloud [m] slow_down_planner.max_slow_down_vel double max slow down velocity [m/s] slow_down_planner.min_slow_down_vel double min slow down velocity [m/s] Flowchart # First, this module cut off the trajectory behind the car and decimates the points of trajectory for reducing computational costs. ( This is the same process as that of Obstacle Stop planner module. ) Then, a detection area is generated by the decimated trajectory as following figure. The detection area in this module is the extended area of the detection area used in Obstacle Stop Planner module. The distance to be extended depends on the above parameter expand_slow_down_range . The module searches the obstacle pointcloud within detection area. When the pointcloud is found, the deceleration point is inserted to the trajectory. The deceleration point means the point with low velocity; the value of the velocity v_{target} v_{target} is determined as follows. v_{target} = v_{min} + \\frac{l_{ld} - l_{vw}/2}{l_{er}} (v_{max} - v_{min} ) v_{target} = v_{min} + \\frac{l_{ld} - l_{vw}/2}{l_{er}} (v_{max} - v_{min} ) v_{min} v_{min} is minimum target value of Slow Down Planner module. The value of v_{min} v_{min} depends on the parameter min_slow_down_vel . v_{max} v_{max} is maximum target value of Slow Down Planner module. The value of v_{max} v_{max} depends on the parameter max_slow_down_vel . l_{ld} l_{ld} is the lateral deviation of the target pointcloud. l_{vw} l_{vw} is the vehicle width. l_{er} l_{er} is the expand range of detection area. The value of l_{er} l_{er} depends on the parameter expand_slow_down_range The above method means that the smaller the lateral deviation of the pointcloud, the lower the velocity of the deceleration point. Adaptive Cruise Controller # Role # Adaptive Cruise Controller module embeds maximum velocity in trajectory when there is a dynamic point cloud on the trajectory. The value of maximum velocity depends on the own velocity, the velocity of the point cloud ( = velocity of the front car), and the distance to the point cloud (= distance to the front car). Parameter Type Description adaptive_cruise_control.use_object_to_estimate_vel bool use dynamic objects for estimating object velocity or not adaptive_cruise_control.use_pcl_to_estimate_vel bool use raw pointclouds for estimating object velocity or not adaptive_cruise_control.consider_obj_velocity bool consider forward vehicle velocity to calculate target velocity in adaptive cruise or not adaptive_cruise_control.obstacle_velocity_thresh_to_start_acc double start adaptive cruise control when the velocity of the forward obstacle exceeds this value [m/s] adaptive_cruise_control.obstacle_velocity_thresh_to_stop_acc double stop acc when the velocity of the forward obstacle falls below this value [m/s] adaptive_cruise_control.emergency_stop_acceleration double supposed minimum acceleration (deceleration) in emergency stop [m/ss] adaptive_cruise_control.emergency_stop_idling_time double supposed idling time to start emergency stop [s] adaptive_cruise_control.min_dist_stop double minimum distance of emergency stop [m] adaptive_cruise_control.obstacle_emergency_stop_acceleration double supposed minimum acceleration (deceleration) in emergency stop [m/ss] adaptive_cruise_control.max_standard_acceleration double supposed maximum acceleration in active cruise control [m/ss] adaptive_cruise_control.min_standard_acceleration double supposed minimum acceleration (deceleration) in active cruise control [m/ss] adaptive_cruise_control.standard_idling_time double supposed idling time to react object in active cruise control [s] adaptive_cruise_control.min_dist_standard double minimum distance in active cruise control [m] adaptive_cruise_control.obstacle_min_standard_acceleration double supposed minimum acceleration of forward obstacle [m/ss] adaptive_cruise_control.margin_rate_to_change_vel double rate of margin distance to insert target velocity [-] adaptive_cruise_control.use_time_compensation_to_calc_distance bool use time-compensation to calculate distance to forward vehicle adaptive_cruise_control.p_coefficient_positive double coefficient P in PID control (used when target dist -current_dist >=0) [-] adaptive_cruise_control.p_coefficient_negative double coefficient P in PID control (used when target dist -current_dist <0) [-] adaptive_cruise_control.d_coefficient_positive double coefficient D in PID control (used when delta_dist >=0) [-] adaptive_cruise_control.d_coefficient_negative double coefficient D in PID control (used when delta_dist <0) [-] adaptive_cruise_control.object_polygon_length_margin double The distance to extend the polygon length the object in pointcloud-object matching [m] adaptive_cruise_control.object_polygon_width_margin double The distance to extend the polygon width the object in pointcloud-object matching [m] adaptive_cruise_control.valid_estimated_vel_diff_time double Maximum time difference treated as continuous points in speed estimation using a point cloud [s] adaptive_cruise_control.valid_vel_que_time double Time width of information used for speed estimation in speed estimation using a point cloud [s] adaptive_cruise_control.valid_estimated_vel_max double Maximum value of valid speed estimation results in speed estimation using a point cloud [m/s] adaptive_cruise_control.valid_estimated_vel_min double Minimum value of valid speed estimation results in speed estimation using a point cloud [m/s] adaptive_cruise_control.thresh_vel_to_stop double Embed a stop line if the maximum speed calculated by ACC is lower than this speed [m/s] adaptive_cruise_control.lowpass_gain_of_upper_velocity double Lowpass-gain of target velocity adaptive_cruise_control.use_rough_velocity_estimation: bool Use rough estimated velocity if the velocity estimation is failed adaptive_cruise_control.rough_velocity_rate double In the rough velocity estimation, the velocity of front car is estimated as self current velocity * this value Flowchart # (*1) The target vehicle point is calculated as a closest obstacle PointCloud from ego along the trajectory. (*2) The sources of velocity estimation can be changed by the following ROS parameters. adaptive_cruise_control.use_object_to_estimate_vel adaptive_cruise_control.use_pcl_to_estimate_vel This module works only when the target point is found in the detection area of the Obstacle stop planner module. The first process of this module is to estimate the velocity of the target vehicle point. The velocity estimation uses the velocity information of dynamic objects or the travel distance of the target vehicle point from the previous step. The dynamic object information is primal, and the travel distance estimation is used as a backup in case of the perception failure. If the target vehicle point is contained in the bounding box of a dynamic object geometrically, the velocity of the dynamic object is used as the target point velocity. Otherwise, the target point velocity is calculated by the travel distance of the target point from the previous step; that is (current_position - previous_position) / dt . Note that this travel distance based estimation fails when the target point is detected in the first time (it mainly happens in the cut-in situation). To improve the stability of the estimation, the median of the calculation result for several steps is used. If the calculated velocity is within the threshold range, it is used as the target point velocity. Only when the estimation is succeeded and the estimated velocity exceeds the value of obstacle_stop_velocity_thresh_* , the distance to the pointcloud from self-position is calculated. For prevent chattering in the mode transition, obstacle_velocity_thresh_to_start_acc is used for the threshold to start adaptive cruise, and obstacle_velocity_thresh_to_stop_acc is used for the threshold to stop adaptive cruise. When the calculated distance value exceeds the emergency distance d\\_{emergency} d\\_{emergency} calculated by emergency_stop parameters, target velocity to insert is calculated. The emergency distance d\\_{emergency} d\\_{emergency} is calculated as follows. d_{emergency} = d_{margin_{emergency}} + t_{idling_{emergency}} \\cdot v_{ego} + (-\\frac{v_{ego}^2}{2 \\cdot a_{ego_{emergency}}}) - (-\\frac{v_{obj}^2}{2 \\cdot a_{obj_{emergency}}}) d_{emergency} = d_{margin_{emergency}} + t_{idling_{emergency}} \\cdot v_{ego} + (-\\frac{v_{ego}^2}{2 \\cdot a_{ego_{emergency}}}) - (-\\frac{v_{obj}^2}{2 \\cdot a_{obj_{emergency}}}) d_{margin_{emergency}} d_{margin_{emergency}} is a minimum margin to the obstacle pointcloud. The value of d_{margin_{emergency}} d_{margin_{emergency}} depends on the parameter min_dist_stop t_{idling_{emergency}} t_{idling_{emergency}} is a supposed idling time. The value of t_{idling_{emergency}} t_{idling_{emergency}} depends on the parameter emergency_stop_idling_time v_{ego} v_{ego} is a current velocity of own vehicle a_{ego_{_{emergency}}} a_{ego_{_{emergency}}} is a minimum acceleration (maximum deceleration) of own vehicle. The value of a_{ego_{_{emergency}}} a_{ego_{_{emergency}}} depends on the parameter emergency_stop_acceleration v_{obj} v_{obj} is a current velocity of obstacle pointcloud. a_{obj_{_{emergency}}} a_{obj_{_{emergency}}} is a supposed minimum acceleration of obstacle pointcloud. The value of a_{obj_{_{emergency}}} a_{obj_{_{emergency}}} depends on the parameter obstacle_emergency_stop_acceleration *Above X_{_{emergency}} X_{_{emergency}} parameters are used only in emergency situation. The target velocity is determined to keep the distance to the obstacle pointcloud from own vehicle at the standard distance d\\_{standard} d\\_{standard} calculated as following. Therefore, if the distance to the obstacle pointcloud is longer than standard distance, The target velocity becomes higher than the current velocity, and vice versa. For keeping the distance, a PID controller is used. d_{standard} = d_{margin_{standard}} + t_{idling_{standard}} \\cdot v_{ego} + (-\\frac{v_{ego}^2}{2 \\cdot a_{ego_{standard}}}) - (-\\frac{v_{obj}^2}{2 \\cdot a_{obj_{standard}}}) d_{standard} = d_{margin_{standard}} + t_{idling_{standard}} \\cdot v_{ego} + (-\\frac{v_{ego}^2}{2 \\cdot a_{ego_{standard}}}) - (-\\frac{v_{obj}^2}{2 \\cdot a_{obj_{standard}}}) d_{margin_{standard}} d_{margin_{standard}} is a minimum margin to the obstacle pointcloud. The value of d_{margin_{standard}} d_{margin_{standard}} depends on the parameter min_dist_stop t_{idling_{standard}} t_{idling_{standard}} is a supposed idling time. The value of t_{idling_{standard}} t_{idling_{standard}} depends on the parameter standard_stop_idling_time v_{ego} v_{ego} is a current velocity of own vehicle a_{ego_{_{standard}}} a_{ego_{_{standard}}} is a minimum acceleration (maximum deceleration) of own vehicle. The value of a_{ego_{_{standard}}} a_{ego_{_{standard}}} depends on the parameter min_standard_acceleration v_{obj} v_{obj} is a current velocity of obstacle pointcloud. a_{obj_{_{standard}}} a_{obj_{_{standard}}} is a supposed minimum acceleration of obstacle pointcloud. The value of a_{obj_{_{standard}}} a_{obj_{_{standard}}} depends on the parameter obstacle_min_standard_acceleration *Above X_{_{standard}} X_{_{standard}} parameters are used only in non-emergency situation. If the target velocity exceeds the value of thresh_vel_to_stop , the target velocity is embedded in the trajectory. Known Limits # It is strongly depends on velocity planning module whether or not it moves according to the target speed embedded by Adaptive Cruise Controller module. If the velocity planning module is updated, please take care of the vehicle's behavior as much as possible and always be ready for overriding. The velocity estimation algorithm in Adaptive Cruise Controller is depend on object tracking module. Please note that if the object-tracking fails or the tracking result is incorrect, it the possibility that the vehicle behaves dangerously.","title":"Obstacle Stop Planner"},{"location":"planning/obstacle_stop_planner/#obstacle-stop-planner","text":"","title":"Obstacle Stop Planner"},{"location":"planning/obstacle_stop_planner/#overview","text":"obstacle_stop_planner has following modules Obstacle Stop Planner inserts a stop point in trajectory when there is a static point cloud on the trajectory. Slow Down Planner inserts a deceleration section in trajectory when there is a point cloud near the trajectory. Adaptive Cruise Controller (ACC) embeds target velocity in trajectory when there is a dynamic point cloud on the trajectory. In order to stop with a stop margin from the obstacle exists, the stop point ( v=0 ) is inserted at a distance of baselink to front + stop margin from the obstacle. The baselink to front means the distance between base_link (center of rear-wheel axis) and front of the car. If a stop point has already been inserted by other nodes between the obstacle and a position which is stop margin meters away from the obstacle, the stop point is inserted at a distance of baselink to front + min behavior stop margin from the obstacle. When the deceleration section is inserted, the start point of the section is inserted in front of the target point cloud by the distance of baselink to front + slow down forward margin . the end point of the section is inserted behind the target point cloud by the distance of slow down backward margin + baselink to rear . The baselink to rear means the distance between base_link and rear of the car. The velocities of points in the deceleration section are modified to the deceleration velocity. slow down backward margin and slow down forward margin are determined by the parameters described below.","title":"Overview"},{"location":"planning/obstacle_stop_planner/#input-topics","text":"Name Type Description ~/input/pointcloud sensor_msgs::PointCloud2 obstacle pointcloud ~/input/trajectory autoware_auto_planning_msgs::Trajectory trajectory ~/input/vector_map autoware_auto_mapping_msgs::HADMapBin vector map ~/input/odometry nav_msgs::Odometry vehicle velocity ~/input/dynamic_objects autoware_auto_perception_msgs::PredictedObjects dynamic objects ~/input/expand_stop_range tier4_planning_msgs::msg::ExpandStopRange expand stop range","title":"Input topics"},{"location":"planning/obstacle_stop_planner/#output-topics","text":"Name Type Description ~output/trajectory autoware_auto_planning_msgs::Trajectory trajectory to be followed ~output/stop_reasons tier4_planning_msgs::StopReasonArray reasons that cause the vehicle to stop","title":"Output topics"},{"location":"planning/obstacle_stop_planner/#modules","text":"","title":"Modules"},{"location":"planning/obstacle_stop_planner/#obstacle-stop-planner_1","text":"","title":"Obstacle Stop Planner"},{"location":"planning/obstacle_stop_planner/#role","text":"Obstacle Stop Planner module inserts a stop point in trajectory when there is a static point cloud on the trajectory. This module does not work when Adaptive Cruise Controller works. Parameter Type Description stop_planner.stop_margin double stop margin distance from obstacle on the path [m] stop_planner.min_behavior_stop_margin double stop margin distance when any other stop point is inserted in stop margin [m] stop_planner.step_length double step length for pointcloud search range [m] stop_planner.extend_distance double extend trajectory to consider after goal obstacle in the extend_distance [m] stop_planner.expand_stop_range double margin of vehicle footprint [m]","title":"Role"},{"location":"planning/obstacle_stop_planner/#flowchart","text":"First, this module cut off the trajectory behind the car and decimates the points of trajectory for reducing computational costs. Then, a detection area is generated by the decimated trajectory as following figure. The detection area means the area through which the vehicle-body passes. The module searches the obstacle pointcloud within detection area. When the pointcloud is found, Adaptive Cruise Controller modules starts to work. only when Adaptive Cruise Controller modules does not insert target velocity, the stop point is inserted to the trajectory. The stop point means the point with 0 velocity.","title":"Flowchart"},{"location":"planning/obstacle_stop_planner/#slow-down-planner","text":"","title":"Slow Down Planner"},{"location":"planning/obstacle_stop_planner/#role_1","text":"Slow Down Planner module inserts a deceleration point in trajectory when there is a point cloud near the trajectory. Parameter Type Description slow_down_planner.slow_down_forward_margin double margin distance from slow down point to vehicle front [m] slow_down_planner.slow_down_backward_margin double margin distance from slow down point to vehicle rear [m] slow_down_planner.expand_slow_down_range double offset from vehicle side edge for expanding the search area of the surrounding point cloud [m] slow_down_planner.max_slow_down_vel double max slow down velocity [m/s] slow_down_planner.min_slow_down_vel double min slow down velocity [m/s]","title":"Role"},{"location":"planning/obstacle_stop_planner/#flowchart_1","text":"First, this module cut off the trajectory behind the car and decimates the points of trajectory for reducing computational costs. ( This is the same process as that of Obstacle Stop planner module. ) Then, a detection area is generated by the decimated trajectory as following figure. The detection area in this module is the extended area of the detection area used in Obstacle Stop Planner module. The distance to be extended depends on the above parameter expand_slow_down_range . The module searches the obstacle pointcloud within detection area. When the pointcloud is found, the deceleration point is inserted to the trajectory. The deceleration point means the point with low velocity; the value of the velocity v_{target} v_{target} is determined as follows. v_{target} = v_{min} + \\frac{l_{ld} - l_{vw}/2}{l_{er}} (v_{max} - v_{min} ) v_{target} = v_{min} + \\frac{l_{ld} - l_{vw}/2}{l_{er}} (v_{max} - v_{min} ) v_{min} v_{min} is minimum target value of Slow Down Planner module. The value of v_{min} v_{min} depends on the parameter min_slow_down_vel . v_{max} v_{max} is maximum target value of Slow Down Planner module. The value of v_{max} v_{max} depends on the parameter max_slow_down_vel . l_{ld} l_{ld} is the lateral deviation of the target pointcloud. l_{vw} l_{vw} is the vehicle width. l_{er} l_{er} is the expand range of detection area. The value of l_{er} l_{er} depends on the parameter expand_slow_down_range The above method means that the smaller the lateral deviation of the pointcloud, the lower the velocity of the deceleration point.","title":"Flowchart"},{"location":"planning/obstacle_stop_planner/#adaptive-cruise-controller","text":"","title":"Adaptive Cruise Controller"},{"location":"planning/obstacle_stop_planner/#role_2","text":"Adaptive Cruise Controller module embeds maximum velocity in trajectory when there is a dynamic point cloud on the trajectory. The value of maximum velocity depends on the own velocity, the velocity of the point cloud ( = velocity of the front car), and the distance to the point cloud (= distance to the front car). Parameter Type Description adaptive_cruise_control.use_object_to_estimate_vel bool use dynamic objects for estimating object velocity or not adaptive_cruise_control.use_pcl_to_estimate_vel bool use raw pointclouds for estimating object velocity or not adaptive_cruise_control.consider_obj_velocity bool consider forward vehicle velocity to calculate target velocity in adaptive cruise or not adaptive_cruise_control.obstacle_velocity_thresh_to_start_acc double start adaptive cruise control when the velocity of the forward obstacle exceeds this value [m/s] adaptive_cruise_control.obstacle_velocity_thresh_to_stop_acc double stop acc when the velocity of the forward obstacle falls below this value [m/s] adaptive_cruise_control.emergency_stop_acceleration double supposed minimum acceleration (deceleration) in emergency stop [m/ss] adaptive_cruise_control.emergency_stop_idling_time double supposed idling time to start emergency stop [s] adaptive_cruise_control.min_dist_stop double minimum distance of emergency stop [m] adaptive_cruise_control.obstacle_emergency_stop_acceleration double supposed minimum acceleration (deceleration) in emergency stop [m/ss] adaptive_cruise_control.max_standard_acceleration double supposed maximum acceleration in active cruise control [m/ss] adaptive_cruise_control.min_standard_acceleration double supposed minimum acceleration (deceleration) in active cruise control [m/ss] adaptive_cruise_control.standard_idling_time double supposed idling time to react object in active cruise control [s] adaptive_cruise_control.min_dist_standard double minimum distance in active cruise control [m] adaptive_cruise_control.obstacle_min_standard_acceleration double supposed minimum acceleration of forward obstacle [m/ss] adaptive_cruise_control.margin_rate_to_change_vel double rate of margin distance to insert target velocity [-] adaptive_cruise_control.use_time_compensation_to_calc_distance bool use time-compensation to calculate distance to forward vehicle adaptive_cruise_control.p_coefficient_positive double coefficient P in PID control (used when target dist -current_dist >=0) [-] adaptive_cruise_control.p_coefficient_negative double coefficient P in PID control (used when target dist -current_dist <0) [-] adaptive_cruise_control.d_coefficient_positive double coefficient D in PID control (used when delta_dist >=0) [-] adaptive_cruise_control.d_coefficient_negative double coefficient D in PID control (used when delta_dist <0) [-] adaptive_cruise_control.object_polygon_length_margin double The distance to extend the polygon length the object in pointcloud-object matching [m] adaptive_cruise_control.object_polygon_width_margin double The distance to extend the polygon width the object in pointcloud-object matching [m] adaptive_cruise_control.valid_estimated_vel_diff_time double Maximum time difference treated as continuous points in speed estimation using a point cloud [s] adaptive_cruise_control.valid_vel_que_time double Time width of information used for speed estimation in speed estimation using a point cloud [s] adaptive_cruise_control.valid_estimated_vel_max double Maximum value of valid speed estimation results in speed estimation using a point cloud [m/s] adaptive_cruise_control.valid_estimated_vel_min double Minimum value of valid speed estimation results in speed estimation using a point cloud [m/s] adaptive_cruise_control.thresh_vel_to_stop double Embed a stop line if the maximum speed calculated by ACC is lower than this speed [m/s] adaptive_cruise_control.lowpass_gain_of_upper_velocity double Lowpass-gain of target velocity adaptive_cruise_control.use_rough_velocity_estimation: bool Use rough estimated velocity if the velocity estimation is failed adaptive_cruise_control.rough_velocity_rate double In the rough velocity estimation, the velocity of front car is estimated as self current velocity * this value","title":"Role"},{"location":"planning/obstacle_stop_planner/#flowchart_2","text":"(*1) The target vehicle point is calculated as a closest obstacle PointCloud from ego along the trajectory. (*2) The sources of velocity estimation can be changed by the following ROS parameters. adaptive_cruise_control.use_object_to_estimate_vel adaptive_cruise_control.use_pcl_to_estimate_vel This module works only when the target point is found in the detection area of the Obstacle stop planner module. The first process of this module is to estimate the velocity of the target vehicle point. The velocity estimation uses the velocity information of dynamic objects or the travel distance of the target vehicle point from the previous step. The dynamic object information is primal, and the travel distance estimation is used as a backup in case of the perception failure. If the target vehicle point is contained in the bounding box of a dynamic object geometrically, the velocity of the dynamic object is used as the target point velocity. Otherwise, the target point velocity is calculated by the travel distance of the target point from the previous step; that is (current_position - previous_position) / dt . Note that this travel distance based estimation fails when the target point is detected in the first time (it mainly happens in the cut-in situation). To improve the stability of the estimation, the median of the calculation result for several steps is used. If the calculated velocity is within the threshold range, it is used as the target point velocity. Only when the estimation is succeeded and the estimated velocity exceeds the value of obstacle_stop_velocity_thresh_* , the distance to the pointcloud from self-position is calculated. For prevent chattering in the mode transition, obstacle_velocity_thresh_to_start_acc is used for the threshold to start adaptive cruise, and obstacle_velocity_thresh_to_stop_acc is used for the threshold to stop adaptive cruise. When the calculated distance value exceeds the emergency distance d\\_{emergency} d\\_{emergency} calculated by emergency_stop parameters, target velocity to insert is calculated. The emergency distance d\\_{emergency} d\\_{emergency} is calculated as follows. d_{emergency} = d_{margin_{emergency}} + t_{idling_{emergency}} \\cdot v_{ego} + (-\\frac{v_{ego}^2}{2 \\cdot a_{ego_{emergency}}}) - (-\\frac{v_{obj}^2}{2 \\cdot a_{obj_{emergency}}}) d_{emergency} = d_{margin_{emergency}} + t_{idling_{emergency}} \\cdot v_{ego} + (-\\frac{v_{ego}^2}{2 \\cdot a_{ego_{emergency}}}) - (-\\frac{v_{obj}^2}{2 \\cdot a_{obj_{emergency}}}) d_{margin_{emergency}} d_{margin_{emergency}} is a minimum margin to the obstacle pointcloud. The value of d_{margin_{emergency}} d_{margin_{emergency}} depends on the parameter min_dist_stop t_{idling_{emergency}} t_{idling_{emergency}} is a supposed idling time. The value of t_{idling_{emergency}} t_{idling_{emergency}} depends on the parameter emergency_stop_idling_time v_{ego} v_{ego} is a current velocity of own vehicle a_{ego_{_{emergency}}} a_{ego_{_{emergency}}} is a minimum acceleration (maximum deceleration) of own vehicle. The value of a_{ego_{_{emergency}}} a_{ego_{_{emergency}}} depends on the parameter emergency_stop_acceleration v_{obj} v_{obj} is a current velocity of obstacle pointcloud. a_{obj_{_{emergency}}} a_{obj_{_{emergency}}} is a supposed minimum acceleration of obstacle pointcloud. The value of a_{obj_{_{emergency}}} a_{obj_{_{emergency}}} depends on the parameter obstacle_emergency_stop_acceleration *Above X_{_{emergency}} X_{_{emergency}} parameters are used only in emergency situation. The target velocity is determined to keep the distance to the obstacle pointcloud from own vehicle at the standard distance d\\_{standard} d\\_{standard} calculated as following. Therefore, if the distance to the obstacle pointcloud is longer than standard distance, The target velocity becomes higher than the current velocity, and vice versa. For keeping the distance, a PID controller is used. d_{standard} = d_{margin_{standard}} + t_{idling_{standard}} \\cdot v_{ego} + (-\\frac{v_{ego}^2}{2 \\cdot a_{ego_{standard}}}) - (-\\frac{v_{obj}^2}{2 \\cdot a_{obj_{standard}}}) d_{standard} = d_{margin_{standard}} + t_{idling_{standard}} \\cdot v_{ego} + (-\\frac{v_{ego}^2}{2 \\cdot a_{ego_{standard}}}) - (-\\frac{v_{obj}^2}{2 \\cdot a_{obj_{standard}}}) d_{margin_{standard}} d_{margin_{standard}} is a minimum margin to the obstacle pointcloud. The value of d_{margin_{standard}} d_{margin_{standard}} depends on the parameter min_dist_stop t_{idling_{standard}} t_{idling_{standard}} is a supposed idling time. The value of t_{idling_{standard}} t_{idling_{standard}} depends on the parameter standard_stop_idling_time v_{ego} v_{ego} is a current velocity of own vehicle a_{ego_{_{standard}}} a_{ego_{_{standard}}} is a minimum acceleration (maximum deceleration) of own vehicle. The value of a_{ego_{_{standard}}} a_{ego_{_{standard}}} depends on the parameter min_standard_acceleration v_{obj} v_{obj} is a current velocity of obstacle pointcloud. a_{obj_{_{standard}}} a_{obj_{_{standard}}} is a supposed minimum acceleration of obstacle pointcloud. The value of a_{obj_{_{standard}}} a_{obj_{_{standard}}} depends on the parameter obstacle_min_standard_acceleration *Above X_{_{standard}} X_{_{standard}} parameters are used only in non-emergency situation. If the target velocity exceeds the value of thresh_vel_to_stop , the target velocity is embedded in the trajectory.","title":"Flowchart"},{"location":"planning/obstacle_stop_planner/#known-limits","text":"It is strongly depends on velocity planning module whether or not it moves according to the target speed embedded by Adaptive Cruise Controller module. If the velocity planning module is updated, please take care of the vehicle's behavior as much as possible and always be ready for overriding. The velocity estimation algorithm in Adaptive Cruise Controller is depend on object tracking module. Please note that if the object-tracking fails or the tracking result is incorrect, it the possibility that the vehicle behaves dangerously.","title":"Known Limits"},{"location":"planning/planning_error_monitor/","text":"Planning Error Monitor # Purpose # planning_error_monitor checks a trajectory that if it has any invalid numerical values in its positions, twist and accel values. In addition, it also checks the distance between any two consecutive points and curvature value at a certain point. This package basically monitors if a trajectory, which is generated by planning module, has any unexpected errors. Inner-workings / Algorithms # Point Value Checker (onTrajectoryPointValueChecker) # This function checks position, twist and accel values of all points on a trajectory. If they have Nan or Infinity , this function outputs error status. Interval Checker (onTrajectoryIntervalChecker) # This function computes interval distance between two consecutive points, and will output error messages if the distance is over the interval_threshold . Curvature Checker (onTrajectoryCurvatureChecker) # This function checks if the curvature at each point on a trajectory has an appropriate value. Calculation details are described in the following picture. First, we choose one point(green point in the picture) that are 1.0[m] behind the current point. Then we get a point(blue point in the picture) 1.0[m] ahead of the current point. Using these three points, we calculate the curvature by this method . Relative Angle Checker (onTrajectoryRelativeAngleChecker) # This function checks if the relative angle at point1 generated from point2 and 3 on a trajectory has an appropriate value. Inputs / Outputs # Input # Name Type Description ~/input/trajectory autoware_auto_planning_msgs/Trajectory Planned Trajectory by planning modules Output # Name Type Description /diagnostics diagnostic_msgs/DiagnosticArray diagnostics outputs ~/debug/marker visualization_msgs/MarkerArray visualization markers Parameters # Name Type Description Default value error_interval double Error Interval Distance Threshold [m] 100.0 error_curvature double Error Curvature Threshold 1.0 error_sharp_angle double Error Sharp Angle Threshold \\pi \\pi /4 ignore_too_close_points double Ignore Too Close Distance Threshold 0.005 Visualization # When the trajectory error occurs, markers for visualization are published at the topic ~/debug/marker . trajectory_interval: An error occurs when the distance between two points exceeds a certain large value. The two points where the error occurred will be visualized. trajectory_curvature: An error occurs when the curvature exceeds a certain large value. The three points used to calculate the curvature will be visualized. trajectory_relative_angle: An error occurs when the angle in the direction of the path point changes significantly. The three points used to calculate the relative angle will be visualized. Assumptions / Known limits # It cannot compute curvature values at start and end points of the trajectory. If trajectory points are too close, curvature calculation might output incorrect values. Future extensions / Unimplemented parts # Collision checker with obstacles may be implemented in the future. Error detection and handling # For the onsite validation, you can use the invalid_trajectory_publisher node. Please launch the node with the following command when the target trajectory is being published. ros2 launch planning_error_monitor invalid_trajectory_publisher.launch.xml This node subscribes the target trajectory, inserts the invalid point, and publishes it with the same name. The invalid trajectory is supposed to be detected by the planning_error_monitor . Limitation: Once the invalid_trajectory_publisher receives the trajectory, it will turn off the subscriber. This is to prevent the trajectory from looping in the same node, therefore, only the one pattern of invalid trajectory is generated.","title":"Planning Error Monitor"},{"location":"planning/planning_error_monitor/#planning-error-monitor","text":"","title":"Planning Error Monitor"},{"location":"planning/planning_error_monitor/#purpose","text":"planning_error_monitor checks a trajectory that if it has any invalid numerical values in its positions, twist and accel values. In addition, it also checks the distance between any two consecutive points and curvature value at a certain point. This package basically monitors if a trajectory, which is generated by planning module, has any unexpected errors.","title":"Purpose"},{"location":"planning/planning_error_monitor/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"planning/planning_error_monitor/#point-value-checker-ontrajectorypointvaluechecker","text":"This function checks position, twist and accel values of all points on a trajectory. If they have Nan or Infinity , this function outputs error status.","title":"Point Value Checker (onTrajectoryPointValueChecker)"},{"location":"planning/planning_error_monitor/#interval-checker-ontrajectoryintervalchecker","text":"This function computes interval distance between two consecutive points, and will output error messages if the distance is over the interval_threshold .","title":"Interval Checker (onTrajectoryIntervalChecker)"},{"location":"planning/planning_error_monitor/#curvature-checker-ontrajectorycurvaturechecker","text":"This function checks if the curvature at each point on a trajectory has an appropriate value. Calculation details are described in the following picture. First, we choose one point(green point in the picture) that are 1.0[m] behind the current point. Then we get a point(blue point in the picture) 1.0[m] ahead of the current point. Using these three points, we calculate the curvature by this method .","title":"Curvature Checker (onTrajectoryCurvatureChecker)"},{"location":"planning/planning_error_monitor/#relative-angle-checker-ontrajectoryrelativeanglechecker","text":"This function checks if the relative angle at point1 generated from point2 and 3 on a trajectory has an appropriate value.","title":"Relative Angle Checker (onTrajectoryRelativeAngleChecker)"},{"location":"planning/planning_error_monitor/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"planning/planning_error_monitor/#input","text":"Name Type Description ~/input/trajectory autoware_auto_planning_msgs/Trajectory Planned Trajectory by planning modules","title":"Input"},{"location":"planning/planning_error_monitor/#output","text":"Name Type Description /diagnostics diagnostic_msgs/DiagnosticArray diagnostics outputs ~/debug/marker visualization_msgs/MarkerArray visualization markers","title":"Output"},{"location":"planning/planning_error_monitor/#parameters","text":"Name Type Description Default value error_interval double Error Interval Distance Threshold [m] 100.0 error_curvature double Error Curvature Threshold 1.0 error_sharp_angle double Error Sharp Angle Threshold \\pi \\pi /4 ignore_too_close_points double Ignore Too Close Distance Threshold 0.005","title":"Parameters"},{"location":"planning/planning_error_monitor/#visualization","text":"When the trajectory error occurs, markers for visualization are published at the topic ~/debug/marker . trajectory_interval: An error occurs when the distance between two points exceeds a certain large value. The two points where the error occurred will be visualized. trajectory_curvature: An error occurs when the curvature exceeds a certain large value. The three points used to calculate the curvature will be visualized. trajectory_relative_angle: An error occurs when the angle in the direction of the path point changes significantly. The three points used to calculate the relative angle will be visualized.","title":"Visualization"},{"location":"planning/planning_error_monitor/#assumptions-known-limits","text":"It cannot compute curvature values at start and end points of the trajectory. If trajectory points are too close, curvature calculation might output incorrect values.","title":"Assumptions / Known limits"},{"location":"planning/planning_error_monitor/#future-extensions-unimplemented-parts","text":"Collision checker with obstacles may be implemented in the future.","title":"Future extensions / Unimplemented parts"},{"location":"planning/planning_error_monitor/#error-detection-and-handling","text":"For the onsite validation, you can use the invalid_trajectory_publisher node. Please launch the node with the following command when the target trajectory is being published. ros2 launch planning_error_monitor invalid_trajectory_publisher.launch.xml This node subscribes the target trajectory, inserts the invalid point, and publishes it with the same name. The invalid trajectory is supposed to be detected by the planning_error_monitor . Limitation: Once the invalid_trajectory_publisher receives the trajectory, it will turn off the subscriber. This is to prevent the trajectory from looping in the same node, therefore, only the one pattern of invalid trajectory is generated.","title":"Error detection and handling"},{"location":"planning/planning_evaluator/","text":"Planning Evaluator # Purpose # This package provides nodes that generate metrics to evaluate the quality of planning and control. Inner-workings / Algorithms # The evaluation node calculates metrics each time it receives a trajectory T(0) . Metrics are calculated using the following information: the trajectory T(0) itself. the previous trajectory T(-1) . the reference trajectory assumed to be used as the reference to plan T(0) . the current ego pose. the set of objects in the environment. These information are maintained by an instance of class MetricsCalculator which is also responsible for calculating metrics. Stat # Each metric is calculated using a Stat instance which contains the minimum, maximum, and mean values calculated for the metric as well as the number of values measured. Metric calculation and adding more metrics # All possible metrics are defined in the Metric enumeration defined include/planning_evaluator/metrics/metric.hpp . This file also defines conversions from/to string as well as human readable descriptions to be used as header of the output file. The MetricsCalculator is responsible for calculating metric statistics through calls to function: Stat < double > MetricsCalculator :: calculate ( const Metric metric , const Trajectory & traj ) const ; Adding a new metric M requires the following steps: metrics/metric.hpp : add M to the enum , to the from/to string conversion maps, and to the description map. metrics_calculator.cpp : add M to the switch/case statement of the calculate function. Add M to the selected_metrics parameters. Inputs / Outputs # Inputs # Name Type Description ~/input/trajectory autoware_auto_planning_msgs::msg::Trajectory Main trajectory to evaluate ~/input/reference_trajectory autoware_auto_planning_msgs::msg::Trajectory Reference trajectory to use for deviation metrics ~/input/objects autoware_auto_perception_msgs::msg::PredictedObjects Obstacles Outputs # Each metric is published on a topic named after the metric name. Name Type Description ~/metrics diagnostic_msgs::msg::DiagnosticArray DiagnosticArray with a DiagnosticStatus for each metric When shut down, the evaluation node writes the values of the metrics measured during its lifetime to a file as specified by the output_file parameter. Parameters # Name Type Description output_file string file used to write metrics ego_frame string frame used for the ego pose selected_metrics List metrics to measure and publish trajectory.min_point_dist_m double minimum distance between two successive points to use for angle calculation trajectory.lookahead.max_dist_m double maximum distance from ego along the trajectory to use for calculation trajectory.lookahead.max_time_m double maximum time ahead of ego along the trajectory to use for calculation obstacle.dist_thr_m double distance between ego and the obstacle below which a collision is considered Assumptions / Known limits # There is a strong assumption that when receiving a trajectory T(0) , it has been generated using the last received reference trajectory and objects. This can be wrong if a new reference trajectory or objects are published while T(0) is being calculated. Precision is currently limited by the resolution of the trajectories. It is possible to interpolate the trajectory and reference trajectory to increase precision but would make computation significantly more expensive. Future extensions / Unimplemented parts # Use Route or Path messages as reference trajectory. RSS metrics (done in another node https://tier4.atlassian.net/browse/AJD-263 ). Add option to publish the min and max metric values. For now only the mean value is published. motion_evaluator_node . Node which constructs a trajectory over time from the real motion of ego. Only a proof of concept is currently implemented.","title":"Planning Evaluator"},{"location":"planning/planning_evaluator/#planning-evaluator","text":"","title":"Planning Evaluator"},{"location":"planning/planning_evaluator/#purpose","text":"This package provides nodes that generate metrics to evaluate the quality of planning and control.","title":"Purpose"},{"location":"planning/planning_evaluator/#inner-workings-algorithms","text":"The evaluation node calculates metrics each time it receives a trajectory T(0) . Metrics are calculated using the following information: the trajectory T(0) itself. the previous trajectory T(-1) . the reference trajectory assumed to be used as the reference to plan T(0) . the current ego pose. the set of objects in the environment. These information are maintained by an instance of class MetricsCalculator which is also responsible for calculating metrics.","title":"Inner-workings / Algorithms"},{"location":"planning/planning_evaluator/#stat","text":"Each metric is calculated using a Stat instance which contains the minimum, maximum, and mean values calculated for the metric as well as the number of values measured.","title":"Stat"},{"location":"planning/planning_evaluator/#metric-calculation-and-adding-more-metrics","text":"All possible metrics are defined in the Metric enumeration defined include/planning_evaluator/metrics/metric.hpp . This file also defines conversions from/to string as well as human readable descriptions to be used as header of the output file. The MetricsCalculator is responsible for calculating metric statistics through calls to function: Stat < double > MetricsCalculator :: calculate ( const Metric metric , const Trajectory & traj ) const ; Adding a new metric M requires the following steps: metrics/metric.hpp : add M to the enum , to the from/to string conversion maps, and to the description map. metrics_calculator.cpp : add M to the switch/case statement of the calculate function. Add M to the selected_metrics parameters.","title":"Metric calculation and adding more metrics"},{"location":"planning/planning_evaluator/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"planning/planning_evaluator/#inputs","text":"Name Type Description ~/input/trajectory autoware_auto_planning_msgs::msg::Trajectory Main trajectory to evaluate ~/input/reference_trajectory autoware_auto_planning_msgs::msg::Trajectory Reference trajectory to use for deviation metrics ~/input/objects autoware_auto_perception_msgs::msg::PredictedObjects Obstacles","title":"Inputs"},{"location":"planning/planning_evaluator/#outputs","text":"Each metric is published on a topic named after the metric name. Name Type Description ~/metrics diagnostic_msgs::msg::DiagnosticArray DiagnosticArray with a DiagnosticStatus for each metric When shut down, the evaluation node writes the values of the metrics measured during its lifetime to a file as specified by the output_file parameter.","title":"Outputs"},{"location":"planning/planning_evaluator/#parameters","text":"Name Type Description output_file string file used to write metrics ego_frame string frame used for the ego pose selected_metrics List metrics to measure and publish trajectory.min_point_dist_m double minimum distance between two successive points to use for angle calculation trajectory.lookahead.max_dist_m double maximum distance from ego along the trajectory to use for calculation trajectory.lookahead.max_time_m double maximum time ahead of ego along the trajectory to use for calculation obstacle.dist_thr_m double distance between ego and the obstacle below which a collision is considered","title":"Parameters"},{"location":"planning/planning_evaluator/#assumptions-known-limits","text":"There is a strong assumption that when receiving a trajectory T(0) , it has been generated using the last received reference trajectory and objects. This can be wrong if a new reference trajectory or objects are published while T(0) is being calculated. Precision is currently limited by the resolution of the trajectories. It is possible to interpolate the trajectory and reference trajectory to increase precision but would make computation significantly more expensive.","title":"Assumptions / Known limits"},{"location":"planning/planning_evaluator/#future-extensions-unimplemented-parts","text":"Use Route or Path messages as reference trajectory. RSS metrics (done in another node https://tier4.atlassian.net/browse/AJD-263 ). Add option to publish the min and max metric values. For now only the mean value is published. motion_evaluator_node . Node which constructs a trajectory over time from the real motion of ego. Only a proof of concept is currently implemented.","title":"Future extensions / Unimplemented parts"},{"location":"planning/route_handler/","text":"route handler # route_handler is a library for calculating driving route on the lanelet map.","title":"route handler"},{"location":"planning/route_handler/#route-handler","text":"route_handler is a library for calculating driving route on the lanelet map.","title":"route handler"},{"location":"planning/rtc_auto_approver/","text":"RTC Auto Approver # Purpose # RTC Auto Approver is a node to approve request to cooperate from behavior planning modules automatically. Inner-workings / Algorithms # Assumptions / Known limits # Future extensions / Unimplemented parts #","title":"RTC Auto Approver"},{"location":"planning/rtc_auto_approver/#rtc-auto-approver","text":"","title":"RTC Auto Approver"},{"location":"planning/rtc_auto_approver/#purpose","text":"RTC Auto Approver is a node to approve request to cooperate from behavior planning modules automatically.","title":"Purpose"},{"location":"planning/rtc_auto_approver/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"planning/rtc_auto_approver/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"planning/rtc_auto_approver/#future-extensions-unimplemented-parts","text":"","title":"Future extensions / Unimplemented parts"},{"location":"planning/rtc_interface/","text":"RTC Interface # Purpose # RTC Interface is an interface to publish the decision status of behavior planning modules and receive execution command from external of an autonomous driving system. Inner-workings / Algorithms # Usage example # // Generate instance (in this example, \"intersection\" is selected) rtc_interface :: RTCInterface rtc_interface ( node , \"intersection\" ); // Generate UUID const unique_identifier_msgs :: msg :: UUID uuid = generateUUID ( getModuleId ()); // Repeat while module is running while (...) { // Get safety status of the module corresponding to the module id const bool safe = ... // Get distance to the object corresponding to the module id const double distance = ... // Get time stamp const rclcpp :: Time stamp = ... // Update status rtc_interface . updateCooperateStatus ( uuid , safe , distance , stamp ); if ( rtc_interface . isActivated ( uuid )) { // Execute planning } else { // Stop planning } // Get time stamp const rclcpp :: Time stamp = ... // Publish status topic rtc_interface . publishCooperateStatus ( stamp ); } // Remove the status from array rtc_interface . removeCooperateStatus ( uuid ); Inputs / Outputs # RTCInterface (Constructor) # rtc_interface :: RTCInterface ( rclcpp :: Node & node , const std :: string & name ); Description # A constructor for rtc_interface::RTCInterface . Input # node : Node calling this interface name : Name of cooperate status array topic and cooperate commands service Cooperate status array topic name : ~/{name}/cooperate_status Cooperate commands service name : ~/{name}/cooperate_commands Output # An instance of RTCInterface publishCooperateStatus # rtc_interface :: publishCooperateStatus ( const rclcpp :: Time & stamp ) Description # Publish registered cooperate status. Input # stamp : Time stamp Output # Nothing updateCooperateStatus # rtc_interface :: updateCooperateStatus ( const unique_identifier_msgs :: msg :: UUID & uuid , const bool safe , const double distance , const rclcpp :: Time & stamp ) Description # Update cooperate status corresponding to uuid . If cooperate status corresponding to uuid is not registered yet, add new cooperate status. Input # uuid : UUID for requesting module safe : Safety status of requesting module distance : Distance to the object from ego vehicle stamp : Time stamp Output # Nothing removeCooperateStatus # rtc_interface :: removeCooperateStatus ( const unique_identifier_msgs :: msg :: UUID & uuid ) Description # Remove cooperate status corresponding to uuid from registered statuses. Input # uuid : UUID for expired module Output # Nothing clearCooperateStatus # rtc_interface :: clearCooperateStatus () Description # Remove all cooperate statuses. Input # Nothing Output # Nothing isActivated # rtc_interface :: isActivated ( const unique_identifier_msgs :: msg :: UUID & uuid ) Description # Return received command status corresponding to uuid . Input # uuid : UUID for checking module Output # If received command is ACTIVATED , return true . If not, return false . isRegistered # rtc_interface :: isRegistered ( const unique_identifier_msgs :: msg :: UUID & uuid ) Description # Return true if uuid is registered. Input # uuid : UUID for checking module Output # If uuid is registered, return true . If not, return false . Assumptions / Known limits # Future extensions / Unimplemented parts #","title":"RTC Interface"},{"location":"planning/rtc_interface/#rtc-interface","text":"","title":"RTC Interface"},{"location":"planning/rtc_interface/#purpose","text":"RTC Interface is an interface to publish the decision status of behavior planning modules and receive execution command from external of an autonomous driving system.","title":"Purpose"},{"location":"planning/rtc_interface/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"planning/rtc_interface/#usage-example","text":"// Generate instance (in this example, \"intersection\" is selected) rtc_interface :: RTCInterface rtc_interface ( node , \"intersection\" ); // Generate UUID const unique_identifier_msgs :: msg :: UUID uuid = generateUUID ( getModuleId ()); // Repeat while module is running while (...) { // Get safety status of the module corresponding to the module id const bool safe = ... // Get distance to the object corresponding to the module id const double distance = ... // Get time stamp const rclcpp :: Time stamp = ... // Update status rtc_interface . updateCooperateStatus ( uuid , safe , distance , stamp ); if ( rtc_interface . isActivated ( uuid )) { // Execute planning } else { // Stop planning } // Get time stamp const rclcpp :: Time stamp = ... // Publish status topic rtc_interface . publishCooperateStatus ( stamp ); } // Remove the status from array rtc_interface . removeCooperateStatus ( uuid );","title":"Usage example"},{"location":"planning/rtc_interface/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"planning/rtc_interface/#rtcinterface-constructor","text":"rtc_interface :: RTCInterface ( rclcpp :: Node & node , const std :: string & name );","title":"RTCInterface (Constructor)"},{"location":"planning/rtc_interface/#description","text":"A constructor for rtc_interface::RTCInterface .","title":"Description"},{"location":"planning/rtc_interface/#input","text":"node : Node calling this interface name : Name of cooperate status array topic and cooperate commands service Cooperate status array topic name : ~/{name}/cooperate_status Cooperate commands service name : ~/{name}/cooperate_commands","title":"Input"},{"location":"planning/rtc_interface/#output","text":"An instance of RTCInterface","title":"Output"},{"location":"planning/rtc_interface/#publishcooperatestatus","text":"rtc_interface :: publishCooperateStatus ( const rclcpp :: Time & stamp )","title":"publishCooperateStatus"},{"location":"planning/rtc_interface/#description_1","text":"Publish registered cooperate status.","title":"Description"},{"location":"planning/rtc_interface/#input_1","text":"stamp : Time stamp","title":"Input"},{"location":"planning/rtc_interface/#output_1","text":"Nothing","title":"Output"},{"location":"planning/rtc_interface/#updatecooperatestatus","text":"rtc_interface :: updateCooperateStatus ( const unique_identifier_msgs :: msg :: UUID & uuid , const bool safe , const double distance , const rclcpp :: Time & stamp )","title":"updateCooperateStatus"},{"location":"planning/rtc_interface/#description_2","text":"Update cooperate status corresponding to uuid . If cooperate status corresponding to uuid is not registered yet, add new cooperate status.","title":"Description"},{"location":"planning/rtc_interface/#input_2","text":"uuid : UUID for requesting module safe : Safety status of requesting module distance : Distance to the object from ego vehicle stamp : Time stamp","title":"Input"},{"location":"planning/rtc_interface/#output_2","text":"Nothing","title":"Output"},{"location":"planning/rtc_interface/#removecooperatestatus","text":"rtc_interface :: removeCooperateStatus ( const unique_identifier_msgs :: msg :: UUID & uuid )","title":"removeCooperateStatus"},{"location":"planning/rtc_interface/#description_3","text":"Remove cooperate status corresponding to uuid from registered statuses.","title":"Description"},{"location":"planning/rtc_interface/#input_3","text":"uuid : UUID for expired module","title":"Input"},{"location":"planning/rtc_interface/#output_3","text":"Nothing","title":"Output"},{"location":"planning/rtc_interface/#clearcooperatestatus","text":"rtc_interface :: clearCooperateStatus ()","title":"clearCooperateStatus"},{"location":"planning/rtc_interface/#description_4","text":"Remove all cooperate statuses.","title":"Description"},{"location":"planning/rtc_interface/#input_4","text":"Nothing","title":"Input"},{"location":"planning/rtc_interface/#output_4","text":"Nothing","title":"Output"},{"location":"planning/rtc_interface/#isactivated","text":"rtc_interface :: isActivated ( const unique_identifier_msgs :: msg :: UUID & uuid )","title":"isActivated"},{"location":"planning/rtc_interface/#description_5","text":"Return received command status corresponding to uuid .","title":"Description"},{"location":"planning/rtc_interface/#input_5","text":"uuid : UUID for checking module","title":"Input"},{"location":"planning/rtc_interface/#output_5","text":"If received command is ACTIVATED , return true . If not, return false .","title":"Output"},{"location":"planning/rtc_interface/#isregistered","text":"rtc_interface :: isRegistered ( const unique_identifier_msgs :: msg :: UUID & uuid )","title":"isRegistered"},{"location":"planning/rtc_interface/#description_6","text":"Return true if uuid is registered.","title":"Description"},{"location":"planning/rtc_interface/#input_6","text":"uuid : UUID for checking module","title":"Input"},{"location":"planning/rtc_interface/#output_6","text":"If uuid is registered, return true . If not, return false .","title":"Output"},{"location":"planning/rtc_interface/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"planning/rtc_interface/#future-extensions-unimplemented-parts","text":"","title":"Future extensions / Unimplemented parts"},{"location":"planning/scenario_selector/","text":"scenario_selector # scenario_selector_node # scenario_selector_node is a node that switches trajectories from each scenario. Input topics # Name Type Description ~input/lane_driving/trajectory autoware_auto_planning_msgs::Trajectory trajectory of LaneDriving scenario ~input/parking/trajectory autoware_auto_planning_msgs::Trajectory trajectory of Parking scenario ~input/lanelet_map autoware_auto_mapping_msgs::HADMapBin ~input/route autoware_auto_planning_msgs::HADMapRoute route and goal pose ~input/odometry nav_msgs::Odometry for checking whether vehicle is stopped is_parking_completed bool (implemented as rosparam) whether all split trajectory of Parking are published Output topics # Name Type Description ~output/scenario tier4_planning_msgs::Scenario current scenario and scenarios to be activated ~output/trajectory autoware_auto_planning_msgs::Trajectory trajectory to be followed Output TFs # None How to launch # Write your remapping info in scenario_selector.launch or add args when executing roslaunch roslaunch scenario_selector scenario_selector.launch If you would like to use only a single scenario, roslaunch scenario_selector dummy_scenario_selector_{scenario_name}.launch Parameters # Parameter Type Description update_rate double timer's update rate th_max_message_delay_sec double threshold time of input messages' maximum delay th_arrived_distance_m double threshold distance to check if vehicle has arrived at the trajectory's endpoint th_stopped_time_sec double threshold time to check if vehicle is stopped th_stopped_velocity_mps double threshold velocity to check if vehicle is stopped Flowchart #","title":"scenario_selector"},{"location":"planning/scenario_selector/#scenario_selector","text":"","title":"scenario_selector"},{"location":"planning/scenario_selector/#scenario_selector_node","text":"scenario_selector_node is a node that switches trajectories from each scenario.","title":"scenario_selector_node"},{"location":"planning/scenario_selector/#input-topics","text":"Name Type Description ~input/lane_driving/trajectory autoware_auto_planning_msgs::Trajectory trajectory of LaneDriving scenario ~input/parking/trajectory autoware_auto_planning_msgs::Trajectory trajectory of Parking scenario ~input/lanelet_map autoware_auto_mapping_msgs::HADMapBin ~input/route autoware_auto_planning_msgs::HADMapRoute route and goal pose ~input/odometry nav_msgs::Odometry for checking whether vehicle is stopped is_parking_completed bool (implemented as rosparam) whether all split trajectory of Parking are published","title":"Input topics"},{"location":"planning/scenario_selector/#output-topics","text":"Name Type Description ~output/scenario tier4_planning_msgs::Scenario current scenario and scenarios to be activated ~output/trajectory autoware_auto_planning_msgs::Trajectory trajectory to be followed","title":"Output topics"},{"location":"planning/scenario_selector/#output-tfs","text":"None","title":"Output TFs"},{"location":"planning/scenario_selector/#how-to-launch","text":"Write your remapping info in scenario_selector.launch or add args when executing roslaunch roslaunch scenario_selector scenario_selector.launch If you would like to use only a single scenario, roslaunch scenario_selector dummy_scenario_selector_{scenario_name}.launch","title":"How to launch"},{"location":"planning/scenario_selector/#parameters","text":"Parameter Type Description update_rate double timer's update rate th_max_message_delay_sec double threshold time of input messages' maximum delay th_arrived_distance_m double threshold distance to check if vehicle has arrived at the trajectory's endpoint th_stopped_time_sec double threshold time to check if vehicle is stopped th_stopped_velocity_mps double threshold velocity to check if vehicle is stopped","title":"Parameters"},{"location":"planning/scenario_selector/#flowchart","text":"","title":"Flowchart"},{"location":"planning/surround_obstacle_checker/","text":"Surround Obstacle Checker # Purpose # This module subscribes required data (ego-pose, obstacles, etc), and publishes zero velocity limit to keep stopping if any of stop conditions are satisfied. Inner-workings / Algorithms # Flow chart # Algorithms # Check data # Check that surround_obstacle_checker receives no ground pointcloud, dynamic objects and current velocity data. Get distance to nearest object # Calculate distance between ego vehicle and the nearest object. In this function, it calculates the minimum distance between the polygon of ego vehicle and all points in pointclouds and the polygons of dynamic objects. Stop requirement # If it satisfies all following conditions, it plans stopping. Ego vehicle is stopped It satisfies any following conditions The distance to nearest obstacle satisfies following conditions If state is State::PASS , the distance is less than surround_check_distance If state is State::STOP , the distance is less than surround_check_recover_distance If it does not satisfies the condition in 1, elapsed time from the time it satisfies the condition in 1 is less than state_clear_time States # To prevent chattering, surround_obstacle_checker manages two states. As mentioned in stop condition section, it prevents chattering by changing threshold to find surround obstacle depending on the states. State::PASS : Stop planning is released State::STOP \uff1aWhile stop planning Inputs / Outputs # Input # Name Type Description /perception/obstacle_segmentation/pointcloud sensor_msgs::msg::PointCloud2 Pointcloud of obstacles which the ego-vehicle should stop or avoid /perception/object_recognition/objects autoware_auto_perception_msgs::msg::PredictedObjects Dynamic objects /localization/kinematic_state nav_msgs::msg::Odometry Current twist /tf tf2_msgs::msg::TFMessage TF /tf_static tf2_msgs::msg::TFMessage TF static Output # Name Type Description ~/output/velocity_limit_clear_command tier4_planning_msgs::msg::VelocityLimitClearCommand Velocity limit clear command ~/output/max_velocity tier4_planning_msgs::msg::VelocityLimit Velocity limit command ~/output/no_start_reason diagnostic_msgs::msg::DiagnosticStatus No start reason ~/output/stop_reasons tier4_planning_msgs::msg::StopReasonArray Stop reasons ~/debug/marker visualization_msgs::msg::MarkerArray Marker for visualization Parameters # Name Type Description Default value use_pointcloud bool Use pointcloud as obstacle check true use_dynamic_object bool Use dynamic object as obstacle check true surround_check_distance double If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status [m] 0.5 surround_check_recover_distance double If no object exists in this distance, transit to \"non-surrounding-obstacle\" status [m] 0.8 state_clear_time double Threshold to clear stop state [s] 2.0 stop_state_ego_speed double Threshold to check ego vehicle stopped [m/s] 0.1 stop_state_entry_duration_time double Threshold to check ego vehicle stopped [s] 0.1 Assumptions / Known limits # To perform stop planning, it is necessary to get obstacle pointclouds data. Hence, it does not plan stopping if the obstacle is in blind spot.","title":"Surround Obstacle Checker"},{"location":"planning/surround_obstacle_checker/#surround-obstacle-checker","text":"","title":"Surround Obstacle Checker"},{"location":"planning/surround_obstacle_checker/#purpose","text":"This module subscribes required data (ego-pose, obstacles, etc), and publishes zero velocity limit to keep stopping if any of stop conditions are satisfied.","title":"Purpose"},{"location":"planning/surround_obstacle_checker/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"planning/surround_obstacle_checker/#flow-chart","text":"","title":"Flow chart"},{"location":"planning/surround_obstacle_checker/#algorithms","text":"","title":"Algorithms"},{"location":"planning/surround_obstacle_checker/#check-data","text":"Check that surround_obstacle_checker receives no ground pointcloud, dynamic objects and current velocity data.","title":"Check data"},{"location":"planning/surround_obstacle_checker/#get-distance-to-nearest-object","text":"Calculate distance between ego vehicle and the nearest object. In this function, it calculates the minimum distance between the polygon of ego vehicle and all points in pointclouds and the polygons of dynamic objects.","title":"Get distance to nearest object"},{"location":"planning/surround_obstacle_checker/#stop-requirement","text":"If it satisfies all following conditions, it plans stopping. Ego vehicle is stopped It satisfies any following conditions The distance to nearest obstacle satisfies following conditions If state is State::PASS , the distance is less than surround_check_distance If state is State::STOP , the distance is less than surround_check_recover_distance If it does not satisfies the condition in 1, elapsed time from the time it satisfies the condition in 1 is less than state_clear_time","title":"Stop requirement"},{"location":"planning/surround_obstacle_checker/#states","text":"To prevent chattering, surround_obstacle_checker manages two states. As mentioned in stop condition section, it prevents chattering by changing threshold to find surround obstacle depending on the states. State::PASS : Stop planning is released State::STOP \uff1aWhile stop planning","title":"States"},{"location":"planning/surround_obstacle_checker/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"planning/surround_obstacle_checker/#input","text":"Name Type Description /perception/obstacle_segmentation/pointcloud sensor_msgs::msg::PointCloud2 Pointcloud of obstacles which the ego-vehicle should stop or avoid /perception/object_recognition/objects autoware_auto_perception_msgs::msg::PredictedObjects Dynamic objects /localization/kinematic_state nav_msgs::msg::Odometry Current twist /tf tf2_msgs::msg::TFMessage TF /tf_static tf2_msgs::msg::TFMessage TF static","title":"Input"},{"location":"planning/surround_obstacle_checker/#output","text":"Name Type Description ~/output/velocity_limit_clear_command tier4_planning_msgs::msg::VelocityLimitClearCommand Velocity limit clear command ~/output/max_velocity tier4_planning_msgs::msg::VelocityLimit Velocity limit command ~/output/no_start_reason diagnostic_msgs::msg::DiagnosticStatus No start reason ~/output/stop_reasons tier4_planning_msgs::msg::StopReasonArray Stop reasons ~/debug/marker visualization_msgs::msg::MarkerArray Marker for visualization","title":"Output"},{"location":"planning/surround_obstacle_checker/#parameters","text":"Name Type Description Default value use_pointcloud bool Use pointcloud as obstacle check true use_dynamic_object bool Use dynamic object as obstacle check true surround_check_distance double If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status [m] 0.5 surround_check_recover_distance double If no object exists in this distance, transit to \"non-surrounding-obstacle\" status [m] 0.8 state_clear_time double Threshold to clear stop state [s] 2.0 stop_state_ego_speed double Threshold to check ego vehicle stopped [m/s] 0.1 stop_state_entry_duration_time double Threshold to check ego vehicle stopped [s] 0.1","title":"Parameters"},{"location":"planning/surround_obstacle_checker/#assumptions-known-limits","text":"To perform stop planning, it is necessary to get obstacle pointclouds data. Hence, it does not plan stopping if the obstacle is in blind spot.","title":"Assumptions / Known limits"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/","text":"Surround Obstacle Checker # Purpose # surround_obstacle_checker \u306f\u3001\u81ea\u8eca\u304c\u505c\u8eca\u4e2d\u3001\u81ea\u8eca\u306e\u5468\u56f2\u306b\u969c\u5bb3\u7269\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306b\u767a\u9032\u3057\u306a\u3044\u3088\u3046\u306b\u505c\u6b62\u8a08\u753b\u3092\u884c\u3046\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3042\u308b\u3002 Inner-workings / Algorithms # Flow chart # Algorithms # Check data # \u70b9\u7fa4\u3001\u52d5\u7684\u7269\u4f53\u3001\u81ea\u8eca\u901f\u5ea6\u306e\u30c7\u30fc\u30bf\u304c\u53d6\u5f97\u3067\u304d\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3059\u308b\u3002 Get distance to nearest object # \u81ea\u8eca\u3068\u6700\u8fd1\u508d\u306e\u969c\u5bb3\u7269\u3068\u306e\u8ddd\u96e2\u3092\u8a08\u7b97\u3059\u308b\u3002 \u3053\u3053\u3067\u306f\u3001\u81ea\u8eca\u306e\u30dd\u30ea\u30b4\u30f3\u3092\u8a08\u7b97\u3057\u3001\u70b9\u7fa4\u306e\u5404\u70b9\u304a\u3088\u3073\u5404\u52d5\u7684\u7269\u4f53\u306e\u30dd\u30ea\u30b4\u30f3\u3068\u306e\u8ddd\u96e2\u3092\u305d\u308c\u305e\u308c\u8a08\u7b97\u3059\u308b\u3053\u3068\u3067\u6700\u8fd1\u508d\u306e\u969c\u5bb3\u7269\u3068\u306e\u8ddd\u96e2\u3092\u6c42\u3081\u308b\u3002 Stop condition # \u6b21\u306e\u6761\u4ef6\u3092\u3059\u3079\u3066\u6e80\u305f\u3059\u3068\u304d\u3001\u81ea\u8eca\u306f\u505c\u6b62\u8a08\u753b\u3092\u884c\u3046\u3002 \u81ea\u8eca\u304c\u505c\u8eca\u3057\u3066\u3044\u308b\u3053\u3068 \u6b21\u306e\u3046\u3061\u3044\u305a\u308c\u304b\u3092\u6e80\u305f\u3059\u3053\u3068 \u6700\u8fd1\u508d\u306e\u969c\u5bb3\u7269\u3068\u306e\u8ddd\u96e2\u304c\u6b21\u306e\u6761\u4ef6\u3092\u307f\u305f\u3059\u3053\u3068 State::PASS \u306e\u3068\u304d\u3001 surround_check_distance \u672a\u6e80\u3067\u3042\u308b State::STOP \u306e\u3068\u304d\u3001 surround_check_recover_distance \u4ee5\u4e0b\u3067\u3042\u308b 1 \u3092\u6e80\u305f\u3057\u3066\u3044\u306a\u3044\u3068\u304d\u30011 \u306e\u6761\u4ef6\u3092\u6e80\u305f\u3057\u305f\u6642\u523b\u304b\u3089\u306e\u7d4c\u904e\u6642\u9593\u304c state_clear_time \u4ee5\u4e0b\u3067\u3042\u308b\u3053\u3068 States # \u30c1\u30e3\u30bf\u30ea\u30f3\u30b0\u9632\u6b62\u306e\u305f\u3081\u3001 surround_obstacle_checker \u3067\u306f\u72b6\u614b\u3092\u7ba1\u7406\u3057\u3066\u3044\u308b\u3002 Stop condition \u306e\u9805\u3067\u8ff0\u3079\u305f\u3088\u3046\u306b\u3001\u72b6\u614b\u306b\u3088\u3063\u3066\u969c\u5bb3\u7269\u5224\u5b9a\u306e\u3057\u304d\u3044\u5024\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u3067\u30c1\u30e3\u30bf\u30ea\u30f3\u30b0\u3092\u9632\u6b62\u3057\u3066\u3044\u308b\u3002 State::PASS \uff1a\u505c\u6b62\u8a08\u753b\u89e3\u9664\u4e2d State::STOP \uff1a\u505c\u6b62\u8a08\u753b\u4e2d Inputs / Outputs # Input # Name Type Description /perception/obstacle_segmentation/pointcloud sensor_msgs::msg::PointCloud2 Pointcloud of obstacles which the ego-vehicle should stop or avoid /perception/object_recognition/objects autoware_auto_perception_msgs::msg::PredictedObjects Dynamic objects /localization/kinematic_state nav_msgs::msg::Odometry Current twist /tf tf2_msgs::msg::TFMessage TF /tf_static tf2_msgs::msg::TFMessage TF static Output # Name Type Description ~/output/velocity_limit_clear_command tier4_planning_msgs::msg::VelocityLimitClearCommand Velocity limit clear command ~/output/max_velocity tier4_planning_msgs::msg::VelocityLimit Velocity limit command ~/output/no_start_reason diagnostic_msgs::msg::DiagnosticStatus No start reason ~/output/stop_reasons tier4_planning_msgs::msg::StopReasonArray Stop reasons ~/debug/marker visualization_msgs::msg::MarkerArray Marker for visualization Parameters # Name Type Description Default value use_pointcloud bool Use pointcloud as obstacle check true use_dynamic_object bool Use dynamic object as obstacle check true surround_check_distance double If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status [m] 0.5 surround_check_recover_distance double If no object exists in this distance, transit to \"non-surrounding-obstacle\" status [m] 0.8 state_clear_time double Threshold to clear stop state [s] 2.0 stop_state_ego_speed double Threshold to check ego vehicle stopped [m/s] 0.1 stop_state_entry_duration_time double Threshold to check ego vehicle stopped [s] 0.1 Assumptions / Known limits # \u3053\u306e\u6a5f\u80fd\u304c\u52d5\u4f5c\u3059\u308b\u305f\u3081\u306b\u306f\u969c\u5bb3\u7269\u70b9\u7fa4\u306e\u89b3\u6e2c\u304c\u5fc5\u8981\u306a\u305f\u3081\u3001\u969c\u5bb3\u7269\u304c\u6b7b\u89d2\u306b\u5165\u3063\u3066\u3044\u308b\u5834\u5408\u306f\u505c\u6b62\u8a08\u753b\u3092\u884c\u308f\u306a\u3044\u3002","title":"Surround Obstacle Checker"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#surround-obstacle-checker","text":"","title":"Surround Obstacle Checker"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#purpose","text":"surround_obstacle_checker \u306f\u3001\u81ea\u8eca\u304c\u505c\u8eca\u4e2d\u3001\u81ea\u8eca\u306e\u5468\u56f2\u306b\u969c\u5bb3\u7269\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306b\u767a\u9032\u3057\u306a\u3044\u3088\u3046\u306b\u505c\u6b62\u8a08\u753b\u3092\u884c\u3046\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3042\u308b\u3002","title":"Purpose"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#flow-chart","text":"","title":"Flow chart"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#algorithms","text":"","title":"Algorithms"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#check-data","text":"\u70b9\u7fa4\u3001\u52d5\u7684\u7269\u4f53\u3001\u81ea\u8eca\u901f\u5ea6\u306e\u30c7\u30fc\u30bf\u304c\u53d6\u5f97\u3067\u304d\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3059\u308b\u3002","title":"Check data"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#get-distance-to-nearest-object","text":"\u81ea\u8eca\u3068\u6700\u8fd1\u508d\u306e\u969c\u5bb3\u7269\u3068\u306e\u8ddd\u96e2\u3092\u8a08\u7b97\u3059\u308b\u3002 \u3053\u3053\u3067\u306f\u3001\u81ea\u8eca\u306e\u30dd\u30ea\u30b4\u30f3\u3092\u8a08\u7b97\u3057\u3001\u70b9\u7fa4\u306e\u5404\u70b9\u304a\u3088\u3073\u5404\u52d5\u7684\u7269\u4f53\u306e\u30dd\u30ea\u30b4\u30f3\u3068\u306e\u8ddd\u96e2\u3092\u305d\u308c\u305e\u308c\u8a08\u7b97\u3059\u308b\u3053\u3068\u3067\u6700\u8fd1\u508d\u306e\u969c\u5bb3\u7269\u3068\u306e\u8ddd\u96e2\u3092\u6c42\u3081\u308b\u3002","title":"Get distance to nearest object"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#stop-condition","text":"\u6b21\u306e\u6761\u4ef6\u3092\u3059\u3079\u3066\u6e80\u305f\u3059\u3068\u304d\u3001\u81ea\u8eca\u306f\u505c\u6b62\u8a08\u753b\u3092\u884c\u3046\u3002 \u81ea\u8eca\u304c\u505c\u8eca\u3057\u3066\u3044\u308b\u3053\u3068 \u6b21\u306e\u3046\u3061\u3044\u305a\u308c\u304b\u3092\u6e80\u305f\u3059\u3053\u3068 \u6700\u8fd1\u508d\u306e\u969c\u5bb3\u7269\u3068\u306e\u8ddd\u96e2\u304c\u6b21\u306e\u6761\u4ef6\u3092\u307f\u305f\u3059\u3053\u3068 State::PASS \u306e\u3068\u304d\u3001 surround_check_distance \u672a\u6e80\u3067\u3042\u308b State::STOP \u306e\u3068\u304d\u3001 surround_check_recover_distance \u4ee5\u4e0b\u3067\u3042\u308b 1 \u3092\u6e80\u305f\u3057\u3066\u3044\u306a\u3044\u3068\u304d\u30011 \u306e\u6761\u4ef6\u3092\u6e80\u305f\u3057\u305f\u6642\u523b\u304b\u3089\u306e\u7d4c\u904e\u6642\u9593\u304c state_clear_time \u4ee5\u4e0b\u3067\u3042\u308b\u3053\u3068","title":"Stop condition"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#states","text":"\u30c1\u30e3\u30bf\u30ea\u30f3\u30b0\u9632\u6b62\u306e\u305f\u3081\u3001 surround_obstacle_checker \u3067\u306f\u72b6\u614b\u3092\u7ba1\u7406\u3057\u3066\u3044\u308b\u3002 Stop condition \u306e\u9805\u3067\u8ff0\u3079\u305f\u3088\u3046\u306b\u3001\u72b6\u614b\u306b\u3088\u3063\u3066\u969c\u5bb3\u7269\u5224\u5b9a\u306e\u3057\u304d\u3044\u5024\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u3067\u30c1\u30e3\u30bf\u30ea\u30f3\u30b0\u3092\u9632\u6b62\u3057\u3066\u3044\u308b\u3002 State::PASS \uff1a\u505c\u6b62\u8a08\u753b\u89e3\u9664\u4e2d State::STOP \uff1a\u505c\u6b62\u8a08\u753b\u4e2d","title":"States"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#input","text":"Name Type Description /perception/obstacle_segmentation/pointcloud sensor_msgs::msg::PointCloud2 Pointcloud of obstacles which the ego-vehicle should stop or avoid /perception/object_recognition/objects autoware_auto_perception_msgs::msg::PredictedObjects Dynamic objects /localization/kinematic_state nav_msgs::msg::Odometry Current twist /tf tf2_msgs::msg::TFMessage TF /tf_static tf2_msgs::msg::TFMessage TF static","title":"Input"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#output","text":"Name Type Description ~/output/velocity_limit_clear_command tier4_planning_msgs::msg::VelocityLimitClearCommand Velocity limit clear command ~/output/max_velocity tier4_planning_msgs::msg::VelocityLimit Velocity limit command ~/output/no_start_reason diagnostic_msgs::msg::DiagnosticStatus No start reason ~/output/stop_reasons tier4_planning_msgs::msg::StopReasonArray Stop reasons ~/debug/marker visualization_msgs::msg::MarkerArray Marker for visualization","title":"Output"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#parameters","text":"Name Type Description Default value use_pointcloud bool Use pointcloud as obstacle check true use_dynamic_object bool Use dynamic object as obstacle check true surround_check_distance double If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status [m] 0.5 surround_check_recover_distance double If no object exists in this distance, transit to \"non-surrounding-obstacle\" status [m] 0.8 state_clear_time double Threshold to clear stop state [s] 2.0 stop_state_ego_speed double Threshold to check ego vehicle stopped [m/s] 0.1 stop_state_entry_duration_time double Threshold to check ego vehicle stopped [s] 0.1","title":"Parameters"},{"location":"planning/surround_obstacle_checker/surround_obstacle_checker-design.ja/#assumptions-known-limits","text":"\u3053\u306e\u6a5f\u80fd\u304c\u52d5\u4f5c\u3059\u308b\u305f\u3081\u306b\u306f\u969c\u5bb3\u7269\u70b9\u7fa4\u306e\u89b3\u6e2c\u304c\u5fc5\u8981\u306a\u305f\u3081\u3001\u969c\u5bb3\u7269\u304c\u6b7b\u89d2\u306b\u5165\u3063\u3066\u3044\u308b\u5834\u5408\u306f\u505c\u6b62\u8a08\u753b\u3092\u884c\u308f\u306a\u3044\u3002","title":"Assumptions / Known limits"},{"location":"sensing/geo_pos_conv/","text":"geo_pos_conv # Purpose # The geo_pos_conv is a library to calculate the conversion between x, y positions on the plane rectangular coordinate and latitude/longitude on the earth . Inner-workings / Algorithms # Inputs / Outputs # Parameters # Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"geo_pos_conv"},{"location":"sensing/geo_pos_conv/#geo_pos_conv","text":"","title":"geo_pos_conv"},{"location":"sensing/geo_pos_conv/#purpose","text":"The geo_pos_conv is a library to calculate the conversion between x, y positions on the plane rectangular coordinate and latitude/longitude on the earth .","title":"Purpose"},{"location":"sensing/geo_pos_conv/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/geo_pos_conv/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/geo_pos_conv/#parameters","text":"","title":"Parameters"},{"location":"sensing/geo_pos_conv/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/geo_pos_conv/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/geo_pos_conv/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/geo_pos_conv/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/geo_pos_conv/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/gnss_poser/","text":"gnss_poser # Purpose # The gnss_poser is a node that subscribes gnss sensing messages and calculates vehicle pose with covariance. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description ~/input/fix sensor_msgs::msg::NavSatFix gnss status message ~/input/navpvt ublox_msgs::msg::NavPVT position, velocity and time solution (You can see detail description in reference document [1]) Output # Name Type Description ~/output/pose geometry_msgs::msg::PoseStamped vehicle pose calculated from gnss sensing data ~/output/gnss_pose_cov geometry_msgs::msg::PoseWithCovarianceStamped vehicle pose with covariance calculated from gnss sensing data ~/output/gnss_fixed tier4_debug_msgs::msg::BoolStamped gnss fix status Parameters # Core Parameters # Name Type Default Value Description base_frame string \"base_link\" frame d gnss_frame string \"gnss\" frame id gnss_base_frame string \"gnss_base_link\" frame id map_frame string \"map\" frame id use_ublox_receiver bool false flag to use ublox receiver plane_zone int 9 identification number of the plane rectangular coordinate systems (See, reference document [2]) Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # [1] https://github.com/KumarRobotics/ublox.git [2] https://www.gsi.go.jp/LAW/heimencho.html (Optional) Future extensions / Unimplemented parts #","title":"gnss_poser"},{"location":"sensing/gnss_poser/#gnss_poser","text":"","title":"gnss_poser"},{"location":"sensing/gnss_poser/#purpose","text":"The gnss_poser is a node that subscribes gnss sensing messages and calculates vehicle pose with covariance.","title":"Purpose"},{"location":"sensing/gnss_poser/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/gnss_poser/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/gnss_poser/#input","text":"Name Type Description ~/input/fix sensor_msgs::msg::NavSatFix gnss status message ~/input/navpvt ublox_msgs::msg::NavPVT position, velocity and time solution (You can see detail description in reference document [1])","title":"Input"},{"location":"sensing/gnss_poser/#output","text":"Name Type Description ~/output/pose geometry_msgs::msg::PoseStamped vehicle pose calculated from gnss sensing data ~/output/gnss_pose_cov geometry_msgs::msg::PoseWithCovarianceStamped vehicle pose with covariance calculated from gnss sensing data ~/output/gnss_fixed tier4_debug_msgs::msg::BoolStamped gnss fix status","title":"Output"},{"location":"sensing/gnss_poser/#parameters","text":"","title":"Parameters"},{"location":"sensing/gnss_poser/#core-parameters","text":"Name Type Default Value Description base_frame string \"base_link\" frame d gnss_frame string \"gnss\" frame id gnss_base_frame string \"gnss_base_link\" frame id map_frame string \"map\" frame id use_ublox_receiver bool false flag to use ublox receiver plane_zone int 9 identification number of the plane rectangular coordinate systems (See, reference document [2])","title":"Core Parameters"},{"location":"sensing/gnss_poser/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/gnss_poser/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/gnss_poser/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/gnss_poser/#optional-referencesexternal-links","text":"[1] https://github.com/KumarRobotics/ublox.git [2] https://www.gsi.go.jp/LAW/heimencho.html","title":"(Optional) References/External links"},{"location":"sensing/gnss_poser/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/image_diagnostics/","text":"image_diagnostics # Purpose # The image_diagnostics is a node that check the status of the input raw image. Inner-workings / Algorithms # Below figure shows the flowchart of image diagnostics node. Each image is divided into small blocks for block state assessment. Each small image block state is assessed as below figure. After all image's blocks state are evaluated, the whole image status is summarized as below. Inputs / Outputs # Input # Name Type Description input/raw_image sensor_msgs::msg::Image raw image Output # Name Type Description image_diag/debug/gray_image sensor_msgs::msg::Image gray image image_diag/debug/dft_image sensor_msgs::msg::Image discrete Fourier transformation image image_diag/debug/diag_block_image sensor_msgs::msg::Image each block state colorization image_diag/image_state_diag tier4_debug_msgs::msg::Int32Stamped image diagnostics status value /diagnostics diagnostic_msgs::msg::DiagnosticArray diagnostics Parameters # Assumptions / Known limits # This is proof of concept for image diagnostics and the algorithms still under further improvement. (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts # Consider more specific image distortion/occlusion type, for instance raindrop or dust. Consider degraded visibility under fog or rain condition from optical point of view","title":"image_diagnostics"},{"location":"sensing/image_diagnostics/#image_diagnostics","text":"","title":"image_diagnostics"},{"location":"sensing/image_diagnostics/#purpose","text":"The image_diagnostics is a node that check the status of the input raw image.","title":"Purpose"},{"location":"sensing/image_diagnostics/#inner-workings-algorithms","text":"Below figure shows the flowchart of image diagnostics node. Each image is divided into small blocks for block state assessment. Each small image block state is assessed as below figure. After all image's blocks state are evaluated, the whole image status is summarized as below.","title":"Inner-workings / Algorithms"},{"location":"sensing/image_diagnostics/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/image_diagnostics/#input","text":"Name Type Description input/raw_image sensor_msgs::msg::Image raw image","title":"Input"},{"location":"sensing/image_diagnostics/#output","text":"Name Type Description image_diag/debug/gray_image sensor_msgs::msg::Image gray image image_diag/debug/dft_image sensor_msgs::msg::Image discrete Fourier transformation image image_diag/debug/diag_block_image sensor_msgs::msg::Image each block state colorization image_diag/image_state_diag tier4_debug_msgs::msg::Int32Stamped image diagnostics status value /diagnostics diagnostic_msgs::msg::DiagnosticArray diagnostics","title":"Output"},{"location":"sensing/image_diagnostics/#parameters","text":"","title":"Parameters"},{"location":"sensing/image_diagnostics/#assumptions-known-limits","text":"This is proof of concept for image diagnostics and the algorithms still under further improvement.","title":"Assumptions / Known limits"},{"location":"sensing/image_diagnostics/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/image_diagnostics/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/image_diagnostics/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/image_diagnostics/#optional-future-extensions-unimplemented-parts","text":"Consider more specific image distortion/occlusion type, for instance raindrop or dust. Consider degraded visibility under fog or rain condition from optical point of view","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/image_transport_decompressor/","text":"image_transport_decompressor # Purpose # The image_transport_decompressor is a node that decompresses images. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description ~/input/compressed_image sensor_msgs::msg::CompressedImage compressed image Output # Name Type Description ~/output/raw_image sensor_msgs::msg::Image decompressed image Parameters # Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"image_transport_decompressor"},{"location":"sensing/image_transport_decompressor/#image_transport_decompressor","text":"","title":"image_transport_decompressor"},{"location":"sensing/image_transport_decompressor/#purpose","text":"The image_transport_decompressor is a node that decompresses images.","title":"Purpose"},{"location":"sensing/image_transport_decompressor/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/image_transport_decompressor/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/image_transport_decompressor/#input","text":"Name Type Description ~/input/compressed_image sensor_msgs::msg::CompressedImage compressed image","title":"Input"},{"location":"sensing/image_transport_decompressor/#output","text":"Name Type Description ~/output/raw_image sensor_msgs::msg::Image decompressed image","title":"Output"},{"location":"sensing/image_transport_decompressor/#parameters","text":"","title":"Parameters"},{"location":"sensing/image_transport_decompressor/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/image_transport_decompressor/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/image_transport_decompressor/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/image_transport_decompressor/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/image_transport_decompressor/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/imu_corrector/","text":"imu_corrector # Purpose # imu_corrector_node is a node that correct imu data. Correct yaw rate offset b b by reading the parameter. Correct yaw rate standard deviation \\sigma \\sigma by reading the parameter. Mathematically, we assume the following equation: \\tilde{\\omega}(t) = \\omega(t) + b(t) + n(t) \\tilde{\\omega}(t) = \\omega(t) + b(t) + n(t) where \\tilde{\\omega} \\tilde{\\omega} denotes observed angular velocity, \\omega \\omega denotes true angular velocity, b b denotes an offset, and n n denotes a gaussian noise. We also assume that n\\sim\\mathcal{N}(0, \\sigma^2) n\\sim\\mathcal{N}(0, \\sigma^2) . Inputs / Outputs # Input # Name Type Description ~input sensor_msgs::msg::Imu raw imu data Output # Name Type Description ~output sensor_msgs::msg::Imu corrected imu data Parameters # Core Parameters # Name Type Description angular_velocity_offset_x double roll rate offset [rad/s] angular_velocity_offset_y double pitch rate offset [rad/s] angular_velocity_offset_z double yaw rate offset [rad/s] angular_velocity_stddev_xx double roll rate standard deviation [rad/s] angular_velocity_stddev_yy double pitch rate standard deviation [rad/s] angular_velocity_stddev_zz double yaw rate standard deviation [rad/s] Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"imu_corrector"},{"location":"sensing/imu_corrector/#imu_corrector","text":"","title":"imu_corrector"},{"location":"sensing/imu_corrector/#purpose","text":"imu_corrector_node is a node that correct imu data. Correct yaw rate offset b b by reading the parameter. Correct yaw rate standard deviation \\sigma \\sigma by reading the parameter. Mathematically, we assume the following equation: \\tilde{\\omega}(t) = \\omega(t) + b(t) + n(t) \\tilde{\\omega}(t) = \\omega(t) + b(t) + n(t) where \\tilde{\\omega} \\tilde{\\omega} denotes observed angular velocity, \\omega \\omega denotes true angular velocity, b b denotes an offset, and n n denotes a gaussian noise. We also assume that n\\sim\\mathcal{N}(0, \\sigma^2) n\\sim\\mathcal{N}(0, \\sigma^2) .","title":"Purpose"},{"location":"sensing/imu_corrector/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/imu_corrector/#input","text":"Name Type Description ~input sensor_msgs::msg::Imu raw imu data","title":"Input"},{"location":"sensing/imu_corrector/#output","text":"Name Type Description ~output sensor_msgs::msg::Imu corrected imu data","title":"Output"},{"location":"sensing/imu_corrector/#parameters","text":"","title":"Parameters"},{"location":"sensing/imu_corrector/#core-parameters","text":"Name Type Description angular_velocity_offset_x double roll rate offset [rad/s] angular_velocity_offset_y double pitch rate offset [rad/s] angular_velocity_offset_z double yaw rate offset [rad/s] angular_velocity_stddev_xx double roll rate standard deviation [rad/s] angular_velocity_stddev_yy double pitch rate standard deviation [rad/s] angular_velocity_stddev_zz double yaw rate standard deviation [rad/s]","title":"Core Parameters"},{"location":"sensing/imu_corrector/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/imu_corrector/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/imu_corrector/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/imu_corrector/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/imu_corrector/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/livox/livox_tag_filter/","text":"livox_tag_filter # Purpose # The livox_tag_filter is a node that removes noise from pointcloud by using the following tags: Point property based on spatial position Point property based on intensity Return number Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description ~/input sensor_msgs::msg::PointCloud2 reference points Output # Name Type Description ~/output sensor_msgs::msg::PointCloud2 filtered points Parameters # Node Parameters # Name Type Description ignore_tags vector ignored tags (See the following table) Tag Parameters # Bit Description Options 0~1 Point property based on spatial position 00: Normal 01: High confidence level of the noise 10: Moderate confidence level of the noise 11: Low confidence level of the noise 2~3 Point property based on intensity 00: Normal 01: High confidence level of the noise 10: Moderate confidence level of the noise 11: Reserved 4~5 Return number 00: return 0 01: return 1 10: return 2 11: return 3 6~7 Reserved You can download more detail description about the livox from external link [1]. Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # [1] https://www.livoxtech.com/downloads (Optional) Future extensions / Unimplemented parts #","title":"livox_tag_filter"},{"location":"sensing/livox/livox_tag_filter/#livox_tag_filter","text":"","title":"livox_tag_filter"},{"location":"sensing/livox/livox_tag_filter/#purpose","text":"The livox_tag_filter is a node that removes noise from pointcloud by using the following tags: Point property based on spatial position Point property based on intensity Return number","title":"Purpose"},{"location":"sensing/livox/livox_tag_filter/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/livox/livox_tag_filter/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/livox/livox_tag_filter/#input","text":"Name Type Description ~/input sensor_msgs::msg::PointCloud2 reference points","title":"Input"},{"location":"sensing/livox/livox_tag_filter/#output","text":"Name Type Description ~/output sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"sensing/livox/livox_tag_filter/#parameters","text":"","title":"Parameters"},{"location":"sensing/livox/livox_tag_filter/#node-parameters","text":"Name Type Description ignore_tags vector ignored tags (See the following table)","title":"Node Parameters"},{"location":"sensing/livox/livox_tag_filter/#tag-parameters","text":"Bit Description Options 0~1 Point property based on spatial position 00: Normal 01: High confidence level of the noise 10: Moderate confidence level of the noise 11: Low confidence level of the noise 2~3 Point property based on intensity 00: Normal 01: High confidence level of the noise 10: Moderate confidence level of the noise 11: Reserved 4~5 Return number 00: return 0 01: return 1 10: return 2 11: return 3 6~7 Reserved You can download more detail description about the livox from external link [1].","title":"Tag Parameters"},{"location":"sensing/livox/livox_tag_filter/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/livox/livox_tag_filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/livox/livox_tag_filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/livox/livox_tag_filter/#optional-referencesexternal-links","text":"[1] https://www.livoxtech.com/downloads","title":"(Optional) References/External links"},{"location":"sensing/livox/livox_tag_filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/","text":"pointcloud_preprocessor # Purpose # The pointcloud_preprocessor is a package that includes the following filters: removing outlier points cropping concatenating pointclouds correcting distortion downsampling Inner-workings / Algorithms # Detail description of each filter's algorithm is in the following links. Filter Name Description Detail concatenate_data subscribe multiple pointclouds and concatenate them into a pointcloud link crop_box_filter remove points within a given box link distortion_corrector compensate pointcloud distortion caused by ego vehicle's movement during 1 scan link downsample_filter downsampling input pointcloud link outlier_filter remove points caused by hardware problems, rain drops and small insects as a noise link passthrough_filter remove points on the outside of a range in given field (e.g. x, y, z, intensity) link pointcloud_accumulator accumulate pointclouds for a given amount of time link vector_map_filter remove points on the outside of lane by using vector map link Inputs / Outputs # Input # Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/indices pcl_msgs::msg::Indices reference indices Output # Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points Parameters # Node Parameters # Name Type Default Value Description input_frame string \" \" input frame id output_frame string \" \" output frame id max_queue_size int 5 max queue size of input/output topics use_indices bool false flag to use pointcloud indices latched_indices bool false flag to latch pointcloud indices approximate_sync bool false flag to use approximate sync option Assumptions / Known limits # pointcloud_preprocessor::Filter is implemented based on pcl_perception [1] because of this issue . (Optional) Error detection and handling # (Optional) Performance characterization # References/External links # [1] https://github.com/ros-perception/perception_pcl/blob/ros2/pcl_ros/src/pcl_ros/filters/filter.cpp (Optional) Future extensions / Unimplemented parts #","title":"pointcloud_preprocessor"},{"location":"sensing/pointcloud_preprocessor/#pointcloud_preprocessor","text":"","title":"pointcloud_preprocessor"},{"location":"sensing/pointcloud_preprocessor/#purpose","text":"The pointcloud_preprocessor is a package that includes the following filters: removing outlier points cropping concatenating pointclouds correcting distortion downsampling","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/#inner-workings-algorithms","text":"Detail description of each filter's algorithm is in the following links. Filter Name Description Detail concatenate_data subscribe multiple pointclouds and concatenate them into a pointcloud link crop_box_filter remove points within a given box link distortion_corrector compensate pointcloud distortion caused by ego vehicle's movement during 1 scan link downsample_filter downsampling input pointcloud link outlier_filter remove points caused by hardware problems, rain drops and small insects as a noise link passthrough_filter remove points on the outside of a range in given field (e.g. x, y, z, intensity) link pointcloud_accumulator accumulate pointclouds for a given amount of time link vector_map_filter remove points on the outside of lane by using vector map link","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/#input","text":"Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/indices pcl_msgs::msg::Indices reference indices","title":"Input"},{"location":"sensing/pointcloud_preprocessor/#output","text":"Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"sensing/pointcloud_preprocessor/#parameters","text":"","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/#node-parameters","text":"Name Type Default Value Description input_frame string \" \" input frame id output_frame string \" \" output frame id max_queue_size int 5 max queue size of input/output topics use_indices bool false flag to use pointcloud indices latched_indices bool false flag to latch pointcloud indices approximate_sync bool false flag to use approximate sync option","title":"Node Parameters"},{"location":"sensing/pointcloud_preprocessor/#assumptions-known-limits","text":"pointcloud_preprocessor::Filter is implemented based on pcl_perception [1] because of this issue .","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/#referencesexternal-links","text":"[1] https://github.com/ros-perception/perception_pcl/blob/ros2/pcl_ros/src/pcl_ros/filters/filter.cpp","title":"References/External links"},{"location":"sensing/pointcloud_preprocessor/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/","text":"blockage_diag # Purpose # To ensure the performance of LiDAR and safety for autonomous driving, the abnormal condition diagnostics feature is needed. LiDAR blockage is abnormal condition of LiDAR when some unwanted objects stitch to and block the light pulses and return signal. This node's purpose is to detect the existing of blockage on LiDAR and its related size and location. Inner-workings / Algorithms # This node bases on the no-return region and its location to decide if it is a blockage. The logic is showed as below Inputs / Outputs # This implementation inherits pointcloud_preprocessor::Filter class, please refer README . Input # Name Type Description ~/input/pointcloud_raw_ex sensor_msgs::msg::PointCloud2 The raw point cloud data is used to detect the no-return region Output # Name Type Description ~/output/blockage_diag/debug/blockage_mask_image sensor_msgs::msg::Image The mask image of detected blockage ~/output/blockage_diag/debug/ground_blockage_ratio tier4_debug_msgs::msg::Float32Stamped The area ratio of blockage region in ground region ~/output/blockage_diag/debug/sky_blockage_ratio tier4_debug_msgs::msg::Float32Stamped The area ratio of blockage region in sky region ~/output/blockage_diag/debug/lidar_depth_map sensor_msgs::msg::Image The depth map image of input point cloud Parameters # Name Type Description blockage_ratio_threshold float The threshold of blockage area ratio blockage_count_threshold float The threshold of number continuous blockage frames horizontal_ring_id int The id of horizontal ring of the LiDAR angle_range vector The effective range of LiDAR vertical_bins int The LiDAR channel number model string The LiDAR model Assumptions / Known limits # Only Hesai Pandar40P and Hesai PandarQT were tested. For a new LiDAR, it is neccessary to check order of channel id in vertical distribution manually and modifiy the code. The current method is still limited for dust type of blockage when dust particles are sparsely distributed. (Optional) Error detection and handling # (Optional) Performance characterization # References/External links # (Optional) Future extensions / Unimplemented parts #","title":"blockage_diag"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#blockage_diag","text":"","title":"blockage_diag"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#purpose","text":"To ensure the performance of LiDAR and safety for autonomous driving, the abnormal condition diagnostics feature is needed. LiDAR blockage is abnormal condition of LiDAR when some unwanted objects stitch to and block the light pulses and return signal. This node's purpose is to detect the existing of blockage on LiDAR and its related size and location.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#inner-workings-algorithms","text":"This node bases on the no-return region and its location to decide if it is a blockage. The logic is showed as below","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#inputs-outputs","text":"This implementation inherits pointcloud_preprocessor::Filter class, please refer README .","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#input","text":"Name Type Description ~/input/pointcloud_raw_ex sensor_msgs::msg::PointCloud2 The raw point cloud data is used to detect the no-return region","title":"Input"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#output","text":"Name Type Description ~/output/blockage_diag/debug/blockage_mask_image sensor_msgs::msg::Image The mask image of detected blockage ~/output/blockage_diag/debug/ground_blockage_ratio tier4_debug_msgs::msg::Float32Stamped The area ratio of blockage region in ground region ~/output/blockage_diag/debug/sky_blockage_ratio tier4_debug_msgs::msg::Float32Stamped The area ratio of blockage region in sky region ~/output/blockage_diag/debug/lidar_depth_map sensor_msgs::msg::Image The depth map image of input point cloud","title":"Output"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#parameters","text":"Name Type Description blockage_ratio_threshold float The threshold of blockage area ratio blockage_count_threshold float The threshold of number continuous blockage frames horizontal_ring_id int The id of horizontal ring of the LiDAR angle_range vector The effective range of LiDAR vertical_bins int The LiDAR channel number model string The LiDAR model","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#assumptions-known-limits","text":"Only Hesai Pandar40P and Hesai PandarQT were tested. For a new LiDAR, it is neccessary to check order of channel id in vertical distribution manually and modifiy the code. The current method is still limited for dust type of blockage when dust particles are sparsely distributed.","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#referencesexternal-links","text":"","title":"References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/blockage_diag/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/","text":"concatenate_data # Purpose # Many self-driving cars combine multiple LiDARs to expand the sensing range. Therefore, a function to combine a plurality of point clouds is required. To combine multiple sensor data with a similar timestamp, the message_filters is often used in the ROS-based system, but this requires the assumption that all inputs can be received. Since safety must be strongly considered in autonomous driving, the point clouds concatenate node must be designed so that even if one sensor fails, the remaining sensor information can be output. Inner-workings / Algorithms # The figure below represents the reception time of each sensor data and how it is combined in the case. Inputs / Outputs # Input # Name Type Description ~/input/twist autoware_auto_vehicle_msgs::msg::VelocityReport The vehicle odometry is used to interpolate the timestamp of each sensor data Output # Name Type Description ~/output/points sensor_msgs::msg::Pointcloud2 concatenated point clouds Parameters # Name Type Default Value Description input/points vector of string [] input topic names that type must be sensor_msgs::msg::Pointcloud2 input_frame string \"\" input frame id output_frame string \"\" output frame id max_queue_size int 5 max queue size of input/output topics Core Parameters # Name Type Default Value Description timeout_sec double 0.1 tolerance of time to publish next pointcloud [s] When this time limit is exceeded, the filter concatenates and publishes pointcloud, even if not all the point clouds are subscribed. Assumptions / Known limits # It is necessary to assume that the vehicle odometry value exists, the sensor data and odometry timestamp are correct, and the TF from base_link to sensor_frame is also correct.","title":"concatenate_data"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#concatenate_data","text":"","title":"concatenate_data"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#purpose","text":"Many self-driving cars combine multiple LiDARs to expand the sensing range. Therefore, a function to combine a plurality of point clouds is required. To combine multiple sensor data with a similar timestamp, the message_filters is often used in the ROS-based system, but this requires the assumption that all inputs can be received. Since safety must be strongly considered in autonomous driving, the point clouds concatenate node must be designed so that even if one sensor fails, the remaining sensor information can be output.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#inner-workings-algorithms","text":"The figure below represents the reception time of each sensor data and how it is combined in the case.","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#input","text":"Name Type Description ~/input/twist autoware_auto_vehicle_msgs::msg::VelocityReport The vehicle odometry is used to interpolate the timestamp of each sensor data","title":"Input"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#output","text":"Name Type Description ~/output/points sensor_msgs::msg::Pointcloud2 concatenated point clouds","title":"Output"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#parameters","text":"Name Type Default Value Description input/points vector of string [] input topic names that type must be sensor_msgs::msg::Pointcloud2 input_frame string \"\" input frame id output_frame string \"\" output frame id max_queue_size int 5 max queue size of input/output topics","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#core-parameters","text":"Name Type Default Value Description timeout_sec double 0.1 tolerance of time to publish next pointcloud [s] When this time limit is exceeded, the filter concatenates and publishes pointcloud, even if not all the point clouds are subscribed.","title":"Core Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/concatenate-data/#assumptions-known-limits","text":"It is necessary to assume that the vehicle odometry value exists, the sensor data and odometry timestamp are correct, and the TF from base_link to sensor_frame is also correct.","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/","text":"crop_box_filter # Purpose # The crop_box_filter is a node that removes points with in a given box region. This filter is used to remove the points that hit the vehicle itself. Inner-workings / Algorithms # pcl::CropBox is used, which filters all points inside a given box. Inputs / Outputs # This implementation inherit pointcloud_preprocessor::Filter class, please refer README . Parameters # Node Parameters # This implementation inherit pointcloud_preprocessor::Filter class, please refer README . Core Parameters # Name Type Default Value Description min_x double -1.0 x-coordinate minimum value for crop range max_x double 1.0 x-coordinate maximum value for crop range min_y double -1.0 y-coordinate minimum value for crop range max_y double 1.0 y-coordinate maximum value for crop range min_z double -1.0 z-coordinate minimum value for crop range max_z double 1.0 z-coordinate maximum value for crop range Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"crop_box_filter"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#crop_box_filter","text":"","title":"crop_box_filter"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#purpose","text":"The crop_box_filter is a node that removes points with in a given box region. This filter is used to remove the points that hit the vehicle itself.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#inner-workings-algorithms","text":"pcl::CropBox is used, which filters all points inside a given box.","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#inputs-outputs","text":"This implementation inherit pointcloud_preprocessor::Filter class, please refer README .","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#parameters","text":"","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#node-parameters","text":"This implementation inherit pointcloud_preprocessor::Filter class, please refer README .","title":"Node Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#core-parameters","text":"Name Type Default Value Description min_x double -1.0 x-coordinate minimum value for crop range max_x double 1.0 x-coordinate maximum value for crop range min_y double -1.0 y-coordinate minimum value for crop range max_y double 1.0 y-coordinate maximum value for crop range min_z double -1.0 z-coordinate minimum value for crop range max_z double 1.0 z-coordinate maximum value for crop range","title":"Core Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/crop-box-filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/","text":"distortion_corrector # Purpose # The distortion_corrector is a node that compensates pointcloud distortion caused by ego vehicle's movement during 1 scan. Since the LiDAR sensor scans by rotating an internal laser, the resulting point cloud will be distorted if the ego-vehicle moves during a single scan (as shown by the figure below). The node corrects this by interpolating sensor data using odometry of ego-vehicle. Inner-workings / Algorithms # Use the equations below (specific to the Velodyne 32C sensor) to obtain an accurate timestamp for each scan data point. Use twist information to determine the distance the ego-vehicle has travelled between the time that the scan started and the corrected timestamp of each point, and then correct the position of the point. The offset equation is given by $ TimeOffset = (55.296 \\mu s SequenceIndex) + (2.304 \\mu s DataPointIndex) $ To calculate the exact point time, add the TimeOffset to the timestamp. $ ExactPointTime = TimeStamp + TimeOffset $ Inputs / Outputs # Input # Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/twist geometry_msgs::msg::TwistWithCovarianceStamped twist ~/input/imu sensor_msgs::msg::Imu imu data Output # Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points Parameters # Core Parameters # Name Type Default Value Description timestamp_field_name string \"time_stamp\" time stamp field name use_imu bool true use gyroscope for yaw rate if true, else use vehicle status Assumptions / Known limits #","title":"distortion_corrector"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#distortion_corrector","text":"","title":"distortion_corrector"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#purpose","text":"The distortion_corrector is a node that compensates pointcloud distortion caused by ego vehicle's movement during 1 scan. Since the LiDAR sensor scans by rotating an internal laser, the resulting point cloud will be distorted if the ego-vehicle moves during a single scan (as shown by the figure below). The node corrects this by interpolating sensor data using odometry of ego-vehicle.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#inner-workings-algorithms","text":"Use the equations below (specific to the Velodyne 32C sensor) to obtain an accurate timestamp for each scan data point. Use twist information to determine the distance the ego-vehicle has travelled between the time that the scan started and the corrected timestamp of each point, and then correct the position of the point. The offset equation is given by $ TimeOffset = (55.296 \\mu s SequenceIndex) + (2.304 \\mu s DataPointIndex) $ To calculate the exact point time, add the TimeOffset to the timestamp. $ ExactPointTime = TimeStamp + TimeOffset $","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#input","text":"Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/twist geometry_msgs::msg::TwistWithCovarianceStamped twist ~/input/imu sensor_msgs::msg::Imu imu data","title":"Input"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#output","text":"Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#parameters","text":"","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#core-parameters","text":"Name Type Default Value Description timestamp_field_name string \"time_stamp\" time stamp field name use_imu bool true use gyroscope for yaw rate if true, else use vehicle status","title":"Core Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/distortion-corrector/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/","text":"downsample_filter # Purpose # The downsample_filter is a node that reduces the number of points. Inner-workings / Algorithms # Approximate Downsample Filter # pcl::VoxelGridNearestCentroid is used. The algorithm is described in tier4_pcl_extensions Random Downsample Filter # pcl::RandomSample is used, which points are sampled with uniform probability. Voxel Grid Downsample Filter # pcl::VoxelGrid is used, which points in each voxel are approximated with their centroid. Inputs / Outputs # These implementations inherit pointcloud_preprocessor::Filter class, please refer README . Parameters # Note Parameters # These implementations inherit pointcloud_preprocessor::Filter class, please refer README . Core Parameters # Approximate Downsample Filter # Name Type Default Value Description voxel_size_x double 0.3 voxel size x [m] voxel_size_y double 0.3 voxel size y [m] voxel_size_z double 0.1 voxel size z [m] Random Downsample Filter # Name Type Default Value Description sample_num int 1500 number of indices to be sampled Voxel Grid Downsample Filter # Name Type Default Value Description voxel_size_x double 0.3 voxel size x [m] voxel_size_y double 0.3 voxel size y [m] voxel_size_z double 0.1 voxel size z [m] Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"downsample_filter"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#downsample_filter","text":"","title":"downsample_filter"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#purpose","text":"The downsample_filter is a node that reduces the number of points.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#approximate-downsample-filter","text":"pcl::VoxelGridNearestCentroid is used. The algorithm is described in tier4_pcl_extensions","title":"Approximate Downsample Filter"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#random-downsample-filter","text":"pcl::RandomSample is used, which points are sampled with uniform probability.","title":"Random Downsample Filter"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#voxel-grid-downsample-filter","text":"pcl::VoxelGrid is used, which points in each voxel are approximated with their centroid.","title":"Voxel Grid Downsample Filter"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#inputs-outputs","text":"These implementations inherit pointcloud_preprocessor::Filter class, please refer README .","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#parameters","text":"","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#note-parameters","text":"These implementations inherit pointcloud_preprocessor::Filter class, please refer README .","title":"Note Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#core-parameters","text":"","title":"Core Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#approximate-downsample-filter_1","text":"Name Type Default Value Description voxel_size_x double 0.3 voxel size x [m] voxel_size_y double 0.3 voxel size y [m] voxel_size_z double 0.1 voxel size z [m]","title":"Approximate Downsample Filter"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#random-downsample-filter_1","text":"Name Type Default Value Description sample_num int 1500 number of indices to be sampled","title":"Random Downsample Filter"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#voxel-grid-downsample-filter_1","text":"Name Type Default Value Description voxel_size_x double 0.3 voxel size x [m] voxel_size_y double 0.3 voxel size y [m] voxel_size_z double 0.1 voxel size z [m]","title":"Voxel Grid Downsample Filter"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/downsample-filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/","text":"dual_return_outlier_filter # Purpose # The purpose is to remove point cloud noise such as fog and rain and publish visibility as a diagnostic topic. Inner-workings / Algorithms # This node can remove rain and fog by considering the light reflected from the object in two stages according to the attenuation factor. The dual_return_outlier_filter is named because it removes noise using data that contains two types of return values separated by attenuation factor, as shown in the figure below. Therefore, in order to use this node, the sensor driver must publish custom data including return_type . please refer to PointXYZIRADRT data structure. Another feature of this node is that it publishes visibility as a diagnostic topic. With this function, for example, in heavy rain, the sensing module can notify that the processing performance has reached its limit, which can lead to ensuring the safety of the vehicle. In some complicated road scenes where normal objects also reflect the light in two stages, for instance plants, leaves, some plastic net etc, the visibility faces some drop in fine weather condition. To deal with that, optional settings of a region of interest (ROI) are added. Fixed_xyz_ROI mode: Visibility estimation based on the weak points in a fixed cuboid surrounding region of ego-vehicle, defined by x, y, z in base_link perspective. Fixed_azimuth_ROI mode: Visibility estimation based on the weak points in a fixed surrounding region of ego-vehicle, defined by azimuth and distance of LiDAR perspective. When select 2 fixed ROI modes, due to the range of weak points is shrink, the sensitivity of visibility is decrease so that a trade of between weak_first_local_noise_threshold and visibility_threshold is needed. The figure below describe how the node works. The below picture shows the ROI options. Inputs / Outputs # This implementation inherits pointcloud_preprocessor::Filter class, please refer README . Output # Name Type Description /dual_return_outlier_filter/frequency_image sensor_msgs::msg::Image The histogram image that represent visibility /dual_return_outlier_filter/visibility tier4_debug_msgs::msg::Float32Stamped A representation of visibility with a value from 0 to 1 /dual_return_outlier_filter/pointcloud_noise sensor_msgs::msg::Pointcloud2 The pointcloud removed as noise Parameters # Node Parameters # This implementation inherits pointcloud_preprocessor::Filter class, please refer README . Core Parameters # Name Type Description vertical_bins int The number of vertical bin for visibility histogram max_azimuth_diff float Threshold for ring_outlier_filter weak_first_distance_ratio double Threshold for ring_outlier_filter general_distance_ratio double Threshold for ring_outlier_filter weak_first_local_noise_threshold int The parameter for determining whether it is noise visibility_error_threshold float When the percentage of white pixels in the binary histogram falls below this parameter the diagnostic status becomes ERR visibility_warn_threshold float When the percentage of white pixels in the binary histogram falls below this parameter the diagnostic status becomes WARN roi_mode string The name of ROI mode for switching min_azimuth_deg float The left limit of azimuth for Fixed_azimuth_ROI mode max_azimuth_deg float The right limit of azimuth for Fixed_azimuth_ROI mode max_distance float The limit distance for for Fixed_azimuth_ROI mode x_max float Maximum of x for Fixed_xyz_ROI mode x_min float Minimum of x for Fixed_xyz_ROI mode y_max float Maximum of y for Fixed_xyz_ROI mode y_min float Minimum of y for Fixed_xyz_ROI mode z_max float Maximum of z for Fixed_xyz_ROI mode z_min float Minimum of z for Fixed_xyz_ROI mode Assumptions / Known limits # Not recommended for use as it is under development. Input data must be PointXYZIRADRT type data including return_type . (Optional) Error detection and handling # (Optional) Performance characterization # References/External links # (Optional) Future extensions / Unimplemented parts #","title":"dual_return_outlier_filter"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#dual_return_outlier_filter","text":"","title":"dual_return_outlier_filter"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#purpose","text":"The purpose is to remove point cloud noise such as fog and rain and publish visibility as a diagnostic topic.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#inner-workings-algorithms","text":"This node can remove rain and fog by considering the light reflected from the object in two stages according to the attenuation factor. The dual_return_outlier_filter is named because it removes noise using data that contains two types of return values separated by attenuation factor, as shown in the figure below. Therefore, in order to use this node, the sensor driver must publish custom data including return_type . please refer to PointXYZIRADRT data structure. Another feature of this node is that it publishes visibility as a diagnostic topic. With this function, for example, in heavy rain, the sensing module can notify that the processing performance has reached its limit, which can lead to ensuring the safety of the vehicle. In some complicated road scenes where normal objects also reflect the light in two stages, for instance plants, leaves, some plastic net etc, the visibility faces some drop in fine weather condition. To deal with that, optional settings of a region of interest (ROI) are added. Fixed_xyz_ROI mode: Visibility estimation based on the weak points in a fixed cuboid surrounding region of ego-vehicle, defined by x, y, z in base_link perspective. Fixed_azimuth_ROI mode: Visibility estimation based on the weak points in a fixed surrounding region of ego-vehicle, defined by azimuth and distance of LiDAR perspective. When select 2 fixed ROI modes, due to the range of weak points is shrink, the sensitivity of visibility is decrease so that a trade of between weak_first_local_noise_threshold and visibility_threshold is needed. The figure below describe how the node works. The below picture shows the ROI options.","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#inputs-outputs","text":"This implementation inherits pointcloud_preprocessor::Filter class, please refer README .","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#output","text":"Name Type Description /dual_return_outlier_filter/frequency_image sensor_msgs::msg::Image The histogram image that represent visibility /dual_return_outlier_filter/visibility tier4_debug_msgs::msg::Float32Stamped A representation of visibility with a value from 0 to 1 /dual_return_outlier_filter/pointcloud_noise sensor_msgs::msg::Pointcloud2 The pointcloud removed as noise","title":"Output"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#parameters","text":"","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#node-parameters","text":"This implementation inherits pointcloud_preprocessor::Filter class, please refer README .","title":"Node Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#core-parameters","text":"Name Type Description vertical_bins int The number of vertical bin for visibility histogram max_azimuth_diff float Threshold for ring_outlier_filter weak_first_distance_ratio double Threshold for ring_outlier_filter general_distance_ratio double Threshold for ring_outlier_filter weak_first_local_noise_threshold int The parameter for determining whether it is noise visibility_error_threshold float When the percentage of white pixels in the binary histogram falls below this parameter the diagnostic status becomes ERR visibility_warn_threshold float When the percentage of white pixels in the binary histogram falls below this parameter the diagnostic status becomes WARN roi_mode string The name of ROI mode for switching min_azimuth_deg float The left limit of azimuth for Fixed_azimuth_ROI mode max_azimuth_deg float The right limit of azimuth for Fixed_azimuth_ROI mode max_distance float The limit distance for for Fixed_azimuth_ROI mode x_max float Maximum of x for Fixed_xyz_ROI mode x_min float Minimum of x for Fixed_xyz_ROI mode y_max float Maximum of y for Fixed_xyz_ROI mode y_min float Minimum of y for Fixed_xyz_ROI mode z_max float Maximum of z for Fixed_xyz_ROI mode z_min float Minimum of z for Fixed_xyz_ROI mode","title":"Core Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#assumptions-known-limits","text":"Not recommended for use as it is under development. Input data must be PointXYZIRADRT type data including return_type .","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#referencesexternal-links","text":"","title":"References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/dual-return-outlier-filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/","text":"outlier_filter # Purpose # The outlier_filter is a package for filtering outlier of points. Inner-workings / Algorithms # Filter Name Description Detail radius search 2d outlier filter A method of removing point cloud noise based on the number of points existing within a certain radius link ring outlier filter A method of operating scan in chronological order and removing noise based on the rate of change in the distance between points link voxel grid outlier filter A method of removing point cloud noise based on the number of points existing within a voxel link dual return outlier filter (under development) A method of removing rain and fog by considering the light reflected from the object in two stages according to the attenuation factor. link Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"outlier_filter"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#outlier_filter","text":"","title":"outlier_filter"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#purpose","text":"The outlier_filter is a package for filtering outlier of points.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#inner-workings-algorithms","text":"Filter Name Description Detail radius search 2d outlier filter A method of removing point cloud noise based on the number of points existing within a certain radius link ring outlier filter A method of operating scan in chronological order and removing noise based on the rate of change in the distance between points link voxel grid outlier filter A method of removing point cloud noise based on the number of points existing within a voxel link dual return outlier filter (under development) A method of removing rain and fog by considering the light reflected from the object in two stages according to the attenuation factor. link","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/outlier-filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/","text":"passthrough_filter # Purpose # The passthrough_filter is a node that removes points on the outside of a range in a given field (e.g. x, y, z, intensity, ring, etc). Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/indices pcl_msgs::msg::Indices reference indices Output # Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points Parameters # Core Parameters # Name Type Default Value Description filter_limit_min int 0 minimum allowed field value filter_limit_max int 127 maximum allowed field value filter_field_name string \"ring\" filtering field name keep_organized bool false flag to keep indices structure filter_limit_negative bool false flag to return whether the data is inside limit or not Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"passthrough_filter"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#passthrough_filter","text":"","title":"passthrough_filter"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#purpose","text":"The passthrough_filter is a node that removes points on the outside of a range in a given field (e.g. x, y, z, intensity, ring, etc).","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#input","text":"Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/indices pcl_msgs::msg::Indices reference indices","title":"Input"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#output","text":"Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#parameters","text":"","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#core-parameters","text":"Name Type Default Value Description filter_limit_min int 0 minimum allowed field value filter_limit_max int 127 maximum allowed field value filter_field_name string \"ring\" filtering field name keep_organized bool false flag to keep indices structure filter_limit_negative bool false flag to return whether the data is inside limit or not","title":"Core Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/passthrough-filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/","text":"pointcloud_accumulator # Purpose # The pointcloud_accumulator is a node that accumulates pointclouds for a given amount of time. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points Output # Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points Parameters # Core Parameters # Name Type Default Value Description accumulation_time_sec double 2.0 accumulation period [s] pointcloud_buffer_size int 50 buffer size Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"pointcloud_accumulator"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#pointcloud_accumulator","text":"","title":"pointcloud_accumulator"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#purpose","text":"The pointcloud_accumulator is a node that accumulates pointclouds for a given amount of time.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#input","text":"Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points","title":"Input"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#output","text":"Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#parameters","text":"","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#core-parameters","text":"Name Type Default Value Description accumulation_time_sec double 2.0 accumulation period [s] pointcloud_buffer_size int 50 buffer size","title":"Core Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/pointcloud-accumulator/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/","text":"radius_search_2d_outlier_filter # Purpose # The purpose is to remove point cloud noise such as insects and rain. Inner-workings / Algorithms # RadiusOutlierRemoval filter which removes all indices in its input cloud that don\u2019t have at least some number of neighbors within a certain range. The description above is quoted from [1]. pcl::search::KdTree [2] is used to implement this package. Inputs / Outputs # This implementation inherits pointcloud_preprocessor::Filter class, please refer README . Parameters # Node Parameters # This implementation inherits pointcloud_preprocessor::Filter class, please refer README . Core Parameters # Name Type Description min_neighbors int If points in the circle centered on reference point is less than min_neighbors , a reference point is judged as outlier search_radius double Searching number of points included in search_radius Assumptions / Known limits # Since the method is to count the number of points contained in the cylinder with the direction of gravity as the direction of the cylinder axis, it is a prerequisite that the ground has been removed. (Optional) Error detection and handling # (Optional) Performance characterization # References/External links # [1] https://pcl.readthedocs.io/projects/tutorials/en/latest/remove_outliers.html [2] https://pcl.readthedocs.io/projects/tutorials/en/latest/kdtree_search.html#kdtree-search (Optional) Future extensions / Unimplemented parts #","title":"radius_search_2d_outlier_filter"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#radius_search_2d_outlier_filter","text":"","title":"radius_search_2d_outlier_filter"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#purpose","text":"The purpose is to remove point cloud noise such as insects and rain.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#inner-workings-algorithms","text":"RadiusOutlierRemoval filter which removes all indices in its input cloud that don\u2019t have at least some number of neighbors within a certain range. The description above is quoted from [1]. pcl::search::KdTree [2] is used to implement this package.","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#inputs-outputs","text":"This implementation inherits pointcloud_preprocessor::Filter class, please refer README .","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#parameters","text":"","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#node-parameters","text":"This implementation inherits pointcloud_preprocessor::Filter class, please refer README .","title":"Node Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#core-parameters","text":"Name Type Description min_neighbors int If points in the circle centered on reference point is less than min_neighbors , a reference point is judged as outlier search_radius double Searching number of points included in search_radius","title":"Core Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#assumptions-known-limits","text":"Since the method is to count the number of points contained in the cylinder with the direction of gravity as the direction of the cylinder axis, it is a prerequisite that the ground has been removed.","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#referencesexternal-links","text":"[1] https://pcl.readthedocs.io/projects/tutorials/en/latest/remove_outliers.html [2] https://pcl.readthedocs.io/projects/tutorials/en/latest/kdtree_search.html#kdtree-search","title":"References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/","text":"ring_outlier_filter # Purpose # The purpose is to remove point cloud noise such as insects and rain. Inner-workings / Algorithms # A method of operating scan in chronological order and removing noise based on the rate of change in the distance between points Inputs / Outputs # This implementation inherits pointcloud_preprocessor::Filter class, please refer README . Parameters # Node Parameters # This implementation inherits pointcloud_preprocessor::Filter class, please refer README . Core Parameters # Name Type Default Value Description distance_ratio double 1.03 object_length_threshold double 0.1 num_points_threshold int 4 Assumptions / Known limits # It is a prerequisite to input a scan point cloud in chronological order. In this repository it is defined as blow structure (please refer to PointXYZIRADT ). X: x Y: y z: z I: intensity R: ring A :azimuth D: distance T: time_stamp (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"ring_outlier_filter"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#ring_outlier_filter","text":"","title":"ring_outlier_filter"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#purpose","text":"The purpose is to remove point cloud noise such as insects and rain.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#inner-workings-algorithms","text":"A method of operating scan in chronological order and removing noise based on the rate of change in the distance between points","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#inputs-outputs","text":"This implementation inherits pointcloud_preprocessor::Filter class, please refer README .","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#parameters","text":"","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#node-parameters","text":"This implementation inherits pointcloud_preprocessor::Filter class, please refer README .","title":"Node Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#core-parameters","text":"Name Type Default Value Description distance_ratio double 1.03 object_length_threshold double 0.1 num_points_threshold int 4","title":"Core Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#assumptions-known-limits","text":"It is a prerequisite to input a scan point cloud in chronological order. In this repository it is defined as blow structure (please refer to PointXYZIRADT ). X: x Y: y z: z I: intensity R: ring A :azimuth D: distance T: time_stamp","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/ring-outlier-filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/","text":"vector_map_filter # Purpose # The vector_map_filter is a node that removes points on the outside of lane by using vector map. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/vector_map autoware_auto_mapping_msgs::msg::HADMapBin vector map Output # Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points Parameters # Core Parameters # Name Type Default Value Description voxel_size_x double 0.04 voxel size voxel_size_y double 0.04 voxel size Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"vector_map_filter"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#vector_map_filter","text":"","title":"vector_map_filter"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#purpose","text":"The vector_map_filter is a node that removes points on the outside of lane by using vector map.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#input","text":"Name Type Description ~/input/points sensor_msgs::msg::PointCloud2 reference points ~/input/vector_map autoware_auto_mapping_msgs::msg::HADMapBin vector map","title":"Input"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#output","text":"Name Type Description ~/output/points sensor_msgs::msg::PointCloud2 filtered points","title":"Output"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#parameters","text":"","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#core-parameters","text":"Name Type Default Value Description voxel_size_x double 0.04 voxel size voxel_size_y double 0.04 voxel size","title":"Core Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/vector-map-filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/","text":"voxel_grid_outlier_filter # Purpose # The purpose is to remove point cloud noise such as insects and rain. Inner-workings / Algorithms # Removing point cloud noise based on the number of points existing within a voxel. The radius_search_2d_outlier_filter is better for accuracy, but this method has the advantage of low calculation cost. Inputs / Outputs # This implementation inherits pointcloud_preprocessor::Filter class, please refer README . Parameters # Node Parameters # This implementation inherits pointcloud_preprocessor::Filter class, please refer README . Core Parameters # Name Type Default Value Description voxel_size_x double 0.3 the voxel size along x-axis [m] voxel_size_y double 0.3 the voxel size along y-axis [m] voxel_size_z double 0.1 the voxel size along z-axis [m] voxel_points_threshold int 2 the minimum number of points in each voxel Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts #","title":"voxel_grid_outlier_filter"},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#voxel_grid_outlier_filter","text":"","title":"voxel_grid_outlier_filter"},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#purpose","text":"The purpose is to remove point cloud noise such as insects and rain.","title":"Purpose"},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#inner-workings-algorithms","text":"Removing point cloud noise based on the number of points existing within a voxel. The radius_search_2d_outlier_filter is better for accuracy, but this method has the advantage of low calculation cost.","title":"Inner-workings / Algorithms"},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#inputs-outputs","text":"This implementation inherits pointcloud_preprocessor::Filter class, please refer README .","title":"Inputs / Outputs"},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#parameters","text":"","title":"Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#node-parameters","text":"This implementation inherits pointcloud_preprocessor::Filter class, please refer README .","title":"Node Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#core-parameters","text":"Name Type Default Value Description voxel_size_x double 0.3 the voxel size along x-axis [m] voxel_size_y double 0.3 the voxel size along y-axis [m] voxel_size_z double 0.1 the voxel size along z-axis [m] voxel_points_threshold int 2 the minimum number of points in each voxel","title":"Core Parameters"},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/probabilistic_occupancy_grid_map/","text":"probabilistic_occupancy_grid_map # Purpose # This package outputs the probability of having an obstacle as occupancy grid map. References/External links # Pointcloud based occupancy grid map Laserscan based occupancy grid map","title":"probabilistic_occupancy_grid_map"},{"location":"sensing/probabilistic_occupancy_grid_map/#probabilistic_occupancy_grid_map","text":"","title":"probabilistic_occupancy_grid_map"},{"location":"sensing/probabilistic_occupancy_grid_map/#purpose","text":"This package outputs the probability of having an obstacle as occupancy grid map.","title":"Purpose"},{"location":"sensing/probabilistic_occupancy_grid_map/#referencesexternal-links","text":"Pointcloud based occupancy grid map Laserscan based occupancy grid map","title":"References/External links"},{"location":"sensing/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/","text":"laserscan based occupancy grid map # Inner-workings / Algorithms # The basic idea is to take a 2D laserscan and ray trace it to create a time-series processed occupancy grid map. the node take a laserscan and make an occupancy grid map with one frame. ray trace is done by Bresenham's line algorithm. Optionally, obstacle point clouds and raw point clouds can be received and reflected in the occupancy grid map. The reason is that laserscan only uses the most foreground point in the polar coordinate system, so it throws away a lot of information. As a result, the occupancy grid map is almost an UNKNOWN cell. Therefore, the obstacle point cloud and the raw point cloud are used to reflect what is judged to be the ground and what is judged to be an obstacle in the occupancy grid map. The black and red dots represent raw point clouds, and the red dots represent obstacle point clouds. In other words, the black points are determined as the ground, and the red point cloud is the points determined as obstacles. The gray cells are represented as UNKNOWN cells. Using the previous occupancy grid map, update the existence probability using a binary Bayesian filter (1). Also, the unobserved cells are time-decayed like the system noise of the Kalman filter (2). \\hat{P_{o}} = \\frac{(P_{o} * P_{z})}{(P_{o} * P_{z} + (1 - P_{o}) * \\bar{P_{z}})} \\tag{1} \\hat{P_{o}} = \\frac{(P_{o} + 0.5 * \\frac{1}{ratio})}{(\\frac{1}{ratio} + 1)} \\tag{2} Inputs / Outputs # Input # Name Type Description ~/input/laserscan sensor_msgs::LaserScan laserscan ~/input/obstacle_pointcloud sensor_msgs::PointCloud2 obstacle pointcloud ~/input/raw_pointcloud sensor_msgs::PointCloud2 The overall point cloud used to input the obstacle point cloud Output # Name Type Description ~/output/occupancy_grid_map nav_msgs::OccupancyGrid occupancy grid map Parameters # Node Parameters # Name Type Description map_frame string map frame base_link_frame string base_link frame input_obstacle_pointcloud bool whether to use the optional obstacle point cloud? If this is true, ~/input/obstacle_pointcloud topics will be received. input_obstacle_and_raw_pointcloud bool whether to use the optional obstacle and raw point cloud? If this is true, ~/input/obstacle_pointcloud and ~/input/raw_pointcloud topics will be received. use_height_filter bool whether to height filter for ~/input/obstacle_pointcloud and ~/input/raw_pointcloud ? By default, the height is set to -1~2m. map_length double The length of the map. -100 if it is 50~50[m] map_resolution double The map cell resolution [m] Assumptions / Known limits # In several places we have modified the external code written in BSD3 license. occupancy_grid_map.hpp cost_value.hpp occupancy_grid_map.cpp (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # Bresenham's_line_algorithm https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm https://web.archive.org/web/20080528040104/http://www.research.ibm.com/journal/sj/041/ibmsjIVRIC.pdf (Optional) Future extensions / Unimplemented parts # The update probability of the binary Bayesian filter is currently hard-coded and requires a code change to be modified. Since there is no special support for moving objects, the probability of existence is not increased for fast objects.","title":"laserscan based occupancy grid map"},{"location":"sensing/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#laserscan-based-occupancy-grid-map","text":"","title":"laserscan based occupancy grid map"},{"location":"sensing/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#inner-workings-algorithms","text":"The basic idea is to take a 2D laserscan and ray trace it to create a time-series processed occupancy grid map. the node take a laserscan and make an occupancy grid map with one frame. ray trace is done by Bresenham's line algorithm. Optionally, obstacle point clouds and raw point clouds can be received and reflected in the occupancy grid map. The reason is that laserscan only uses the most foreground point in the polar coordinate system, so it throws away a lot of information. As a result, the occupancy grid map is almost an UNKNOWN cell. Therefore, the obstacle point cloud and the raw point cloud are used to reflect what is judged to be the ground and what is judged to be an obstacle in the occupancy grid map. The black and red dots represent raw point clouds, and the red dots represent obstacle point clouds. In other words, the black points are determined as the ground, and the red point cloud is the points determined as obstacles. The gray cells are represented as UNKNOWN cells. Using the previous occupancy grid map, update the existence probability using a binary Bayesian filter (1). Also, the unobserved cells are time-decayed like the system noise of the Kalman filter (2). \\hat{P_{o}} = \\frac{(P_{o} * P_{z})}{(P_{o} * P_{z} + (1 - P_{o}) * \\bar{P_{z}})} \\tag{1} \\hat{P_{o}} = \\frac{(P_{o} + 0.5 * \\frac{1}{ratio})}{(\\frac{1}{ratio} + 1)} \\tag{2}","title":"Inner-workings / Algorithms"},{"location":"sensing/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#input","text":"Name Type Description ~/input/laserscan sensor_msgs::LaserScan laserscan ~/input/obstacle_pointcloud sensor_msgs::PointCloud2 obstacle pointcloud ~/input/raw_pointcloud sensor_msgs::PointCloud2 The overall point cloud used to input the obstacle point cloud","title":"Input"},{"location":"sensing/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#output","text":"Name Type Description ~/output/occupancy_grid_map nav_msgs::OccupancyGrid occupancy grid map","title":"Output"},{"location":"sensing/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#parameters","text":"","title":"Parameters"},{"location":"sensing/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#node-parameters","text":"Name Type Description map_frame string map frame base_link_frame string base_link frame input_obstacle_pointcloud bool whether to use the optional obstacle point cloud? If this is true, ~/input/obstacle_pointcloud topics will be received. input_obstacle_and_raw_pointcloud bool whether to use the optional obstacle and raw point cloud? If this is true, ~/input/obstacle_pointcloud and ~/input/raw_pointcloud topics will be received. use_height_filter bool whether to height filter for ~/input/obstacle_pointcloud and ~/input/raw_pointcloud ? By default, the height is set to -1~2m. map_length double The length of the map. -100 if it is 50~50[m] map_resolution double The map cell resolution [m]","title":"Node Parameters"},{"location":"sensing/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#assumptions-known-limits","text":"In several places we have modified the external code written in BSD3 license. occupancy_grid_map.hpp cost_value.hpp occupancy_grid_map.cpp","title":"Assumptions / Known limits"},{"location":"sensing/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#optional-referencesexternal-links","text":"Bresenham's_line_algorithm https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm https://web.archive.org/web/20080528040104/http://www.research.ibm.com/journal/sj/041/ibmsjIVRIC.pdf","title":"(Optional) References/External links"},{"location":"sensing/probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#optional-future-extensions-unimplemented-parts","text":"The update probability of the binary Bayesian filter is currently hard-coded and requires a code change to be modified. Since there is no special support for moving objects, the probability of existence is not increased for fast objects.","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/","text":"pointcloud based occupancy grid map # Inner-workings / Algorithms # 1st step # First of all, obstacle and raw pointcloud as input are transformed into a polar coordinate system and divided into bin per angle_increment. At this time, each point belonging to each bin is stored as range data. In addition, the x,y information in the map coordinate is also stored for ray trace on map coordinate. The bin contains the following information for each point range data from origin of raytrace x on map coordinate y on map coordinate The following figure shows each of the bins from side view. 2nd step # The ray trace is performed in three steps for each cell. The ray trace is done by Bresenham's line algorithm. Initialize freespace to the farthest point of each bin. Fill in the unknown cells. Assume that unknown is behind the obstacle, since the back of the obstacle is a blind spot. Therefore, the unknown are assumed to be the cells that are more than a distance margin from each obstacle point. There are three reasons for setting a distance margin. It is unlikely that a point on the ground will be immediately behind an obstacle. The obstacle point cloud is processed and may not match the raw pointcloud. The input may be inaccurate and obstacle points may not be determined as obstacles. Fill in the occupied cells. Fill in the point where the obstacle point is located with occupied. In addition, If the distance between obstacle points is less than or equal to the distance margin, it is filled with occupied because the input may be inaccurate and obstacle points may not be determined as obstacles. 3rd step # Using the previous occupancy grid map, update the existence probability using a binary Bayesian filter (1). Also, the unobserved cells are time-decayed like the system noise of the Kalman filter (2). \\hat{P_{o}} = \\frac{(P_{o} * P_{z})}{(P_{o} * P_{z} + (1 - P_{o}) * \\bar{P_{z}})} \\tag{1} \\hat{P_{o}} = \\frac{(P_{o} + 0.5 * \\frac{1}{ratio})}{(\\frac{1}{ratio} + 1)} \\tag{2} Inputs / Outputs # Input # Name Type Description ~/input/obstacle_pointcloud sensor_msgs::PointCloud2 obstacle pointcloud ~/input/raw_pointcloud sensor_msgs::PointCloud2 The overall point cloud used to input the obstacle point cloud Output # Name Type Description ~/output/occupancy_grid_map nav_msgs::OccupancyGrid occupancy grid map Parameters # Node Parameters # Name Type Description map_frame string map frame base_link_frame string base_link frame use_height_filter bool whether to height filter for ~/input/obstacle_pointcloud and ~/input/raw_pointcloud ? By default, the height is set to -1~2m. map_length double The length of the map. -100 if it is 50~50[m] map_resolution double The map cell resolution [m] Assumptions / Known limits # In several places we have modified the external code written in BSD3 license. occupancy_grid_map.hpp cost_value.hpp occupancy_grid_map.cpp (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # (Optional) Future extensions / Unimplemented parts # The update probability of the binary Bayesian filter is currently hard-coded and requires a code change to be modified. Since there is no special support for moving objects, the probability of existence is not increased for fast objects.","title":"pointcloud based occupancy grid map"},{"location":"sensing/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#pointcloud-based-occupancy-grid-map","text":"","title":"pointcloud based occupancy grid map"},{"location":"sensing/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#1st-step","text":"First of all, obstacle and raw pointcloud as input are transformed into a polar coordinate system and divided into bin per angle_increment. At this time, each point belonging to each bin is stored as range data. In addition, the x,y information in the map coordinate is also stored for ray trace on map coordinate. The bin contains the following information for each point range data from origin of raytrace x on map coordinate y on map coordinate The following figure shows each of the bins from side view.","title":"1st step"},{"location":"sensing/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#2nd-step","text":"The ray trace is performed in three steps for each cell. The ray trace is done by Bresenham's line algorithm. Initialize freespace to the farthest point of each bin. Fill in the unknown cells. Assume that unknown is behind the obstacle, since the back of the obstacle is a blind spot. Therefore, the unknown are assumed to be the cells that are more than a distance margin from each obstacle point. There are three reasons for setting a distance margin. It is unlikely that a point on the ground will be immediately behind an obstacle. The obstacle point cloud is processed and may not match the raw pointcloud. The input may be inaccurate and obstacle points may not be determined as obstacles. Fill in the occupied cells. Fill in the point where the obstacle point is located with occupied. In addition, If the distance between obstacle points is less than or equal to the distance margin, it is filled with occupied because the input may be inaccurate and obstacle points may not be determined as obstacles.","title":"2nd step"},{"location":"sensing/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#3rd-step","text":"Using the previous occupancy grid map, update the existence probability using a binary Bayesian filter (1). Also, the unobserved cells are time-decayed like the system noise of the Kalman filter (2). \\hat{P_{o}} = \\frac{(P_{o} * P_{z})}{(P_{o} * P_{z} + (1 - P_{o}) * \\bar{P_{z}})} \\tag{1} \\hat{P_{o}} = \\frac{(P_{o} + 0.5 * \\frac{1}{ratio})}{(\\frac{1}{ratio} + 1)} \\tag{2}","title":"3rd step"},{"location":"sensing/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#input","text":"Name Type Description ~/input/obstacle_pointcloud sensor_msgs::PointCloud2 obstacle pointcloud ~/input/raw_pointcloud sensor_msgs::PointCloud2 The overall point cloud used to input the obstacle point cloud","title":"Input"},{"location":"sensing/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#output","text":"Name Type Description ~/output/occupancy_grid_map nav_msgs::OccupancyGrid occupancy grid map","title":"Output"},{"location":"sensing/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#parameters","text":"","title":"Parameters"},{"location":"sensing/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#node-parameters","text":"Name Type Description map_frame string map frame base_link_frame string base_link frame use_height_filter bool whether to height filter for ~/input/obstacle_pointcloud and ~/input/raw_pointcloud ? By default, the height is set to -1~2m. map_length double The length of the map. -100 if it is 50~50[m] map_resolution double The map cell resolution [m]","title":"Node Parameters"},{"location":"sensing/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#assumptions-known-limits","text":"In several places we have modified the external code written in BSD3 license. occupancy_grid_map.hpp cost_value.hpp occupancy_grid_map.cpp","title":"Assumptions / Known limits"},{"location":"sensing/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#optional-referencesexternal-links","text":"","title":"(Optional) References/External links"},{"location":"sensing/probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#optional-future-extensions-unimplemented-parts","text":"The update probability of the binary Bayesian filter is currently hard-coded and requires a code change to be modified. Since there is no special support for moving objects, the probability of existence is not increased for fast objects.","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/tier4_pcl_extensions/","text":"tier4_pcl_extensions # Purpose # The tier4_pcl_extensions is a pcl extension library. The voxel grid filter in this package works with a different algorithm than the original one. Inner-workings / Algorithms # Original Algorithm [1] # create a 3D voxel grid over the input pointcloud data calculate centroid in each voxel all the points are approximated with their centroid Extended Algorithm # create a 3D voxel grid over the input pointcloud data calculate centroid in each voxel all the points are approximated with the closest point to their centroid Inputs / Outputs # Parameters # Assumptions / Known limits # (Optional) Error detection and handling # (Optional) Performance characterization # (Optional) References/External links # [1] https://pointclouds.org/documentation/tutorials/voxel_grid.html (Optional) Future extensions / Unimplemented parts #","title":"tier4_pcl_extensions"},{"location":"sensing/tier4_pcl_extensions/#tier4_pcl_extensions","text":"","title":"tier4_pcl_extensions"},{"location":"sensing/tier4_pcl_extensions/#purpose","text":"The tier4_pcl_extensions is a pcl extension library. The voxel grid filter in this package works with a different algorithm than the original one.","title":"Purpose"},{"location":"sensing/tier4_pcl_extensions/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"sensing/tier4_pcl_extensions/#original-algorithm-1","text":"create a 3D voxel grid over the input pointcloud data calculate centroid in each voxel all the points are approximated with their centroid","title":"Original Algorithm [1]"},{"location":"sensing/tier4_pcl_extensions/#extended-algorithm","text":"create a 3D voxel grid over the input pointcloud data calculate centroid in each voxel all the points are approximated with the closest point to their centroid","title":"Extended Algorithm"},{"location":"sensing/tier4_pcl_extensions/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/tier4_pcl_extensions/#parameters","text":"","title":"Parameters"},{"location":"sensing/tier4_pcl_extensions/#assumptions-known-limits","text":"","title":"Assumptions / Known limits"},{"location":"sensing/tier4_pcl_extensions/#optional-error-detection-and-handling","text":"","title":"(Optional) Error detection and handling"},{"location":"sensing/tier4_pcl_extensions/#optional-performance-characterization","text":"","title":"(Optional) Performance characterization"},{"location":"sensing/tier4_pcl_extensions/#optional-referencesexternal-links","text":"[1] https://pointclouds.org/documentation/tutorials/voxel_grid.html","title":"(Optional) References/External links"},{"location":"sensing/tier4_pcl_extensions/#optional-future-extensions-unimplemented-parts","text":"","title":"(Optional) Future extensions / Unimplemented parts"},{"location":"sensing/vehicle_velocity_converter/","text":"vehicle_velocity_converter # Purpose # This package converts autoware_auto_vehicle_msgs::msg::VehicleReport message to geometry_msgs::msg::TwistWithCovarianceStamped for gyro odometer node. Inputs / Outputs # Input # Name Type Description velocity_status autoware_auto_vehicle_msgs::msg::VehicleReport vehicle velocity Output # Name Type Description twist_with_covariance geometry_msgs::msg::TwistWithCovarianceStamped twist with covariance converted from VehicleReport Parameters # Name Type Description frame_id string frame id for output message covariance double set covariance value to the twist message","title":"vehicle_velocity_converter"},{"location":"sensing/vehicle_velocity_converter/#vehicle_velocity_converter","text":"","title":"vehicle_velocity_converter"},{"location":"sensing/vehicle_velocity_converter/#purpose","text":"This package converts autoware_auto_vehicle_msgs::msg::VehicleReport message to geometry_msgs::msg::TwistWithCovarianceStamped for gyro odometer node.","title":"Purpose"},{"location":"sensing/vehicle_velocity_converter/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"sensing/vehicle_velocity_converter/#input","text":"Name Type Description velocity_status autoware_auto_vehicle_msgs::msg::VehicleReport vehicle velocity","title":"Input"},{"location":"sensing/vehicle_velocity_converter/#output","text":"Name Type Description twist_with_covariance geometry_msgs::msg::TwistWithCovarianceStamped twist with covariance converted from VehicleReport","title":"Output"},{"location":"sensing/vehicle_velocity_converter/#parameters","text":"Name Type Description frame_id string frame id for output message covariance double set covariance value to the twist message","title":"Parameters"},{"location":"simulator/dummy_perception_publisher/","text":"dummy_perception_publisher # Purpose # This node publishes the result of the dummy detection with the type of perception. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description /tf tf2_msgs/TFMessage TF (self-pose) input/object dummy_perception_publisher::msg::Object dummy detection objects Output # Name Type Description output/dynamic_object tier4_perception_msgs::msg::DetectedObjectsWithFeature Publishes objects output/points_raw sensor_msgs::msg::PointCloud2 point cloud of objects Parameters # Name Type Default Value Explanation visible_range double 100.0 sensor visible range [m] detection_successful_rate double 0.8 sensor detection rate. (min) 0.0 - 1.0(max) enable_ray_tracing bool true if True, use ray tracking use_object_recognition bool true if True, publish objects topic Node Parameters # None. Core Parameters # None. Assumptions / Known limits # TBD.","title":"dummy_perception_publisher"},{"location":"simulator/dummy_perception_publisher/#dummy_perception_publisher","text":"","title":"dummy_perception_publisher"},{"location":"simulator/dummy_perception_publisher/#purpose","text":"This node publishes the result of the dummy detection with the type of perception.","title":"Purpose"},{"location":"simulator/dummy_perception_publisher/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"simulator/dummy_perception_publisher/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"simulator/dummy_perception_publisher/#input","text":"Name Type Description /tf tf2_msgs/TFMessage TF (self-pose) input/object dummy_perception_publisher::msg::Object dummy detection objects","title":"Input"},{"location":"simulator/dummy_perception_publisher/#output","text":"Name Type Description output/dynamic_object tier4_perception_msgs::msg::DetectedObjectsWithFeature Publishes objects output/points_raw sensor_msgs::msg::PointCloud2 point cloud of objects","title":"Output"},{"location":"simulator/dummy_perception_publisher/#parameters","text":"Name Type Default Value Explanation visible_range double 100.0 sensor visible range [m] detection_successful_rate double 0.8 sensor detection rate. (min) 0.0 - 1.0(max) enable_ray_tracing bool true if True, use ray tracking use_object_recognition bool true if True, publish objects topic","title":"Parameters"},{"location":"simulator/dummy_perception_publisher/#node-parameters","text":"None.","title":"Node Parameters"},{"location":"simulator/dummy_perception_publisher/#core-parameters","text":"None.","title":"Core Parameters"},{"location":"simulator/dummy_perception_publisher/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"simulator/fault_injection/","text":"fault_injection # Purpose # This package is used to convert pseudo system faults from PSim to Diagnostics and notify Autoware. The component diagram is as follows: Test # source install/setup.bash cd fault_injection launch_test test/test_fault_injection_node.test.py Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description ~/input/simulation_events tier4_simulation_msgs::msg::SimulationEvents simulation events Output # None. Parameters # None. Node Parameters # None. Core Parameters # None. Assumptions / Known limits # TBD.","title":"fault_injection"},{"location":"simulator/fault_injection/#fault_injection","text":"","title":"fault_injection"},{"location":"simulator/fault_injection/#purpose","text":"This package is used to convert pseudo system faults from PSim to Diagnostics and notify Autoware. The component diagram is as follows:","title":"Purpose"},{"location":"simulator/fault_injection/#test","text":"source install/setup.bash cd fault_injection launch_test test/test_fault_injection_node.test.py","title":"Test"},{"location":"simulator/fault_injection/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"simulator/fault_injection/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"simulator/fault_injection/#input","text":"Name Type Description ~/input/simulation_events tier4_simulation_msgs::msg::SimulationEvents simulation events","title":"Input"},{"location":"simulator/fault_injection/#output","text":"None.","title":"Output"},{"location":"simulator/fault_injection/#parameters","text":"None.","title":"Parameters"},{"location":"simulator/fault_injection/#node-parameters","text":"None.","title":"Node Parameters"},{"location":"simulator/fault_injection/#core-parameters","text":"None.","title":"Core Parameters"},{"location":"simulator/fault_injection/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"simulator/simple_planning_simulator/design/simple_planning_simulator-design/","text":"simple_planning_simulator # Purpose / Use cases # This node simulates the vehicle motion for a vehicle command in 2D using a simple vehicle model. Design # The purpose of this simulator is for the integration test of planning and control modules. This does not simulate sensing or perception, but is implemented in pure c++ only and works without GPU. Assumptions / Known limits # It simulates only in 2D motion. It does not perform physical operations such as collision and sensing, but only calculates the integral results of vehicle dynamics. Inputs / Outputs / API # input # input/vehicle_control_command [ autoware_auto_msgs/msg/VehicleControlCommand ] : target command to drive a vehicle. input/ackermann_control_command [ autoware_auto_msgs/msg/AckermannControlCommand ] : target command to drive a vehicle. input/vehicle_state_command [ autoware_auto_msgs/msg/VehicleStateCommand ] : target state command (e.g. gear). /initialpose [ geometry_msgs/msg/PoseWithCovarianceStamped ] : for initial pose output # /tf [ tf2_msgs/msg/TFMessage ] : simulated vehicle pose (base_link) /vehicle/vehicle_kinematic_state [ autoware_auto_msgs/msg/VehicleKinematicState ] : simulated kinematic state (defined in CoM) /vehicle/state_report [ autoware_auto_msgs/msg/VehicleStateReport ] : current vehicle state (e.g. gear, mode, etc.) Inner-workings / Algorithms # Common Parameters # Name Type Description Default value simulated_frame_id string set to the child_frame_id in output tf \"base_link\" origin_frame_id string set to the frame_id in output tf \"odom\" initialize_source string If \"ORIGIN\", the initial pose is set at (0,0,0). If \"INITIAL_POSE_TOPIC\", node will wait until the /initialpose topic is published. \"INITIAL_POSE_TOPIC\" add_measurement_noise bool If true, the Gaussian noise is added to the simulated results. true pos_noise_stddev double Standard deviation for position noise 0.01 rpy_noise_stddev double Standard deviation for Euler angle noise 0.0001 vel_noise_stddev double Standard deviation for longitudinal velocity noise 0.0 angvel_noise_stddev double Standard deviation for angular velocity noise 0.0 steer_noise_stddev double Standard deviation for steering angle noise 0.0001 Vehicle Model Parameters # vehicle_model_type options # IDEAL_STEER_VEL IDEAL_STEER_ACC IDEAL_STEER_ACC_GEARED DELAY_STEER_VEL DELAY_STEER_ACC DELAY_STEER_ACC_GEARED The IDEAL model moves ideally as commanded, while the DELAY model moves based on a 1st-order with time delay model. The STEER means the model receives the steer command. The VEL means the model receives the target velocity command, while the ACC model receives the target acceleration command. The GEARED suffix means that the motion will consider the gear command: the vehicle moves only one direction following the gear. The table below shows which models correspond to what parameters. The model names are written in abbreviated form (e.g. IDEAL_STEER_VEL = I_ST_V). Name Type Description I_ST_V I_ST_A I_ST_A_G D_ST_V D_ST_A D_ST_A_G Default value unit acc_time_delay double dead time for the acceleration input x x x x o o 0.1 [s] steer_time_delay double dead time for the steering input x x x o o o 0.24 [s] vel_time_delay double dead time for the velocity input x x x o x x 0.25 [s] acc_time_constant double time constant of the 1st-order acceleration dynamics x x x x o o 0.1 [s] steer_time_constant double time constant of the 1st-order steering dynamics x x x o o o 0.27 [s] vel_time_constant double time constant of the 1st-order velocity dynamics x x x o x x 0.5 [s] vel_lim double limit of velocity x x x o o o 50.0 [m/s] vel_rate_lim double limit of acceleration x x x o o o 7.0 [m/ss] steer_lim double limit of steering angle x x x o o o 1.0 [rad] steer_rate_lim double limit of steering angle change rate x x x o o o 5.0 [rad/s] Note : The steering/velocity/acceleration dynamics is modeled by a first order system with a deadtime in a delay model. The definition of the time constant is the time it takes for the step response to rise up to 63% of its final value. The deadtime is a delay in the response to a control input. Default TF configuration # Since the vehicle outputs odom -> base_link tf, this simulator outputs the tf with the same frame_id configuration. In the simple_planning_simulator.launch.py, the node that outputs the map -> odom tf, that usually estimated by the localization module (e.g. NDT), will be launched as well. Since the tf output by this simulator module is an ideal value, odom -> map will always be 0. Error detection and handling # The only validation on inputs being done is testing for a valid vehicle model type. Security considerations # References / External links # This is originally developed in the Autoware.AI. See the link below. https://github.com/Autoware-AI/simulation/tree/master/wf_simulator Future extensions / Unimplemented parts # Improving the accuracy of vehicle models (e.g., adding steering dead zones and slip behavior) Cooperation with modules that output pseudo pointcloud or pseudo perception results","title":"simple_planning_simulator"},{"location":"simulator/simple_planning_simulator/design/simple_planning_simulator-design/#simple_planning_simulator","text":"","title":"simple_planning_simulator"},{"location":"simulator/simple_planning_simulator/design/simple_planning_simulator-design/#purpose-use-cases","text":"This node simulates the vehicle motion for a vehicle command in 2D using a simple vehicle model.","title":"Purpose / Use cases"},{"location":"simulator/simple_planning_simulator/design/simple_planning_simulator-design/#design","text":"The purpose of this simulator is for the integration test of planning and control modules. This does not simulate sensing or perception, but is implemented in pure c++ only and works without GPU.","title":"Design"},{"location":"simulator/simple_planning_simulator/design/simple_planning_simulator-design/#assumptions-known-limits","text":"It simulates only in 2D motion. It does not perform physical operations such as collision and sensing, but only calculates the integral results of vehicle dynamics.","title":"Assumptions / Known limits"},{"location":"simulator/simple_planning_simulator/design/simple_planning_simulator-design/#inputs-outputs-api","text":"","title":"Inputs / Outputs / API"},{"location":"simulator/simple_planning_simulator/design/simple_planning_simulator-design/#input","text":"input/vehicle_control_command [ autoware_auto_msgs/msg/VehicleControlCommand ] : target command to drive a vehicle. input/ackermann_control_command [ autoware_auto_msgs/msg/AckermannControlCommand ] : target command to drive a vehicle. input/vehicle_state_command [ autoware_auto_msgs/msg/VehicleStateCommand ] : target state command (e.g. gear). /initialpose [ geometry_msgs/msg/PoseWithCovarianceStamped ] : for initial pose","title":"input"},{"location":"simulator/simple_planning_simulator/design/simple_planning_simulator-design/#output","text":"/tf [ tf2_msgs/msg/TFMessage ] : simulated vehicle pose (base_link) /vehicle/vehicle_kinematic_state [ autoware_auto_msgs/msg/VehicleKinematicState ] : simulated kinematic state (defined in CoM) /vehicle/state_report [ autoware_auto_msgs/msg/VehicleStateReport ] : current vehicle state (e.g. gear, mode, etc.)","title":"output"},{"location":"simulator/simple_planning_simulator/design/simple_planning_simulator-design/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"simulator/simple_planning_simulator/design/simple_planning_simulator-design/#common-parameters","text":"Name Type Description Default value simulated_frame_id string set to the child_frame_id in output tf \"base_link\" origin_frame_id string set to the frame_id in output tf \"odom\" initialize_source string If \"ORIGIN\", the initial pose is set at (0,0,0). If \"INITIAL_POSE_TOPIC\", node will wait until the /initialpose topic is published. \"INITIAL_POSE_TOPIC\" add_measurement_noise bool If true, the Gaussian noise is added to the simulated results. true pos_noise_stddev double Standard deviation for position noise 0.01 rpy_noise_stddev double Standard deviation for Euler angle noise 0.0001 vel_noise_stddev double Standard deviation for longitudinal velocity noise 0.0 angvel_noise_stddev double Standard deviation for angular velocity noise 0.0 steer_noise_stddev double Standard deviation for steering angle noise 0.0001","title":"Common Parameters"},{"location":"simulator/simple_planning_simulator/design/simple_planning_simulator-design/#vehicle-model-parameters","text":"","title":"Vehicle Model Parameters"},{"location":"simulator/simple_planning_simulator/design/simple_planning_simulator-design/#vehicle_model_type-options","text":"IDEAL_STEER_VEL IDEAL_STEER_ACC IDEAL_STEER_ACC_GEARED DELAY_STEER_VEL DELAY_STEER_ACC DELAY_STEER_ACC_GEARED The IDEAL model moves ideally as commanded, while the DELAY model moves based on a 1st-order with time delay model. The STEER means the model receives the steer command. The VEL means the model receives the target velocity command, while the ACC model receives the target acceleration command. The GEARED suffix means that the motion will consider the gear command: the vehicle moves only one direction following the gear. The table below shows which models correspond to what parameters. The model names are written in abbreviated form (e.g. IDEAL_STEER_VEL = I_ST_V). Name Type Description I_ST_V I_ST_A I_ST_A_G D_ST_V D_ST_A D_ST_A_G Default value unit acc_time_delay double dead time for the acceleration input x x x x o o 0.1 [s] steer_time_delay double dead time for the steering input x x x o o o 0.24 [s] vel_time_delay double dead time for the velocity input x x x o x x 0.25 [s] acc_time_constant double time constant of the 1st-order acceleration dynamics x x x x o o 0.1 [s] steer_time_constant double time constant of the 1st-order steering dynamics x x x o o o 0.27 [s] vel_time_constant double time constant of the 1st-order velocity dynamics x x x o x x 0.5 [s] vel_lim double limit of velocity x x x o o o 50.0 [m/s] vel_rate_lim double limit of acceleration x x x o o o 7.0 [m/ss] steer_lim double limit of steering angle x x x o o o 1.0 [rad] steer_rate_lim double limit of steering angle change rate x x x o o o 5.0 [rad/s] Note : The steering/velocity/acceleration dynamics is modeled by a first order system with a deadtime in a delay model. The definition of the time constant is the time it takes for the step response to rise up to 63% of its final value. The deadtime is a delay in the response to a control input.","title":"vehicle_model_type options"},{"location":"simulator/simple_planning_simulator/design/simple_planning_simulator-design/#default-tf-configuration","text":"Since the vehicle outputs odom -> base_link tf, this simulator outputs the tf with the same frame_id configuration. In the simple_planning_simulator.launch.py, the node that outputs the map -> odom tf, that usually estimated by the localization module (e.g. NDT), will be launched as well. Since the tf output by this simulator module is an ideal value, odom -> map will always be 0.","title":"Default TF configuration"},{"location":"simulator/simple_planning_simulator/design/simple_planning_simulator-design/#error-detection-and-handling","text":"The only validation on inputs being done is testing for a valid vehicle model type.","title":"Error detection and handling"},{"location":"simulator/simple_planning_simulator/design/simple_planning_simulator-design/#security-considerations","text":"","title":"Security considerations"},{"location":"simulator/simple_planning_simulator/design/simple_planning_simulator-design/#references-external-links","text":"This is originally developed in the Autoware.AI. See the link below. https://github.com/Autoware-AI/simulation/tree/master/wf_simulator","title":"References / External links"},{"location":"simulator/simple_planning_simulator/design/simple_planning_simulator-design/#future-extensions-unimplemented-parts","text":"Improving the accuracy of vehicle models (e.g., adding steering dead zones and slip behavior) Cooperation with modules that output pseudo pointcloud or pseudo perception results","title":"Future extensions / Unimplemented parts"},{"location":"system/ad_service_state_monitor/","text":"ad_service_state_monitor # Purpose # This node manages AutowareState transitions. Inner-workings / Algorithms # Inputs / Outputs # Input # Name Type Description /planning/mission_planning/route autoware_auto_planning_msgs::msg::HADMapRoute Subscribe route /localization/kinematic_state nav_msgs::msg::Odometry Used to decide whether vehicle is stopped or not /vehicle/state_report autoware_auto_vehicle_msgs::msg::ControlModeReport Used to check vehicle mode: autonomous or manual. Output # Name Type Description /autoware/engage autoware_auto_vehicle_msgs::msg::Engage publish disengage flag on AutowareState transition /autoware/state autoware_auto_system_msgs::msg::AutowareState publish AutowareState Parameters # Node Parameters # Name Type Default Value Explanation update_rate int 10 Timer callback period. Core Parameters # Name Type Default Value Explanation th_arrived_distance_m double 1.0 threshold distance to check if vehicle has arrived at the route's endpoint th_stopped_time_sec double 1.0 threshold time to check if vehicle is stopped th_stopped_velocity_mps double 0.01 threshold velocity to check if vehicle is stopped disengage_on_route bool true send disengage flag or not when the route is subscribed disengage_on_goal bool true send disengage flag or not when the vehicle is arrived goal Assumptions / Known limits # TBD.","title":"ad_service_state_monitor"},{"location":"system/ad_service_state_monitor/#ad_service_state_monitor","text":"","title":"ad_service_state_monitor"},{"location":"system/ad_service_state_monitor/#purpose","text":"This node manages AutowareState transitions.","title":"Purpose"},{"location":"system/ad_service_state_monitor/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"system/ad_service_state_monitor/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"system/ad_service_state_monitor/#input","text":"Name Type Description /planning/mission_planning/route autoware_auto_planning_msgs::msg::HADMapRoute Subscribe route /localization/kinematic_state nav_msgs::msg::Odometry Used to decide whether vehicle is stopped or not /vehicle/state_report autoware_auto_vehicle_msgs::msg::ControlModeReport Used to check vehicle mode: autonomous or manual.","title":"Input"},{"location":"system/ad_service_state_monitor/#output","text":"Name Type Description /autoware/engage autoware_auto_vehicle_msgs::msg::Engage publish disengage flag on AutowareState transition /autoware/state autoware_auto_system_msgs::msg::AutowareState publish AutowareState","title":"Output"},{"location":"system/ad_service_state_monitor/#parameters","text":"","title":"Parameters"},{"location":"system/ad_service_state_monitor/#node-parameters","text":"Name Type Default Value Explanation update_rate int 10 Timer callback period.","title":"Node Parameters"},{"location":"system/ad_service_state_monitor/#core-parameters","text":"Name Type Default Value Explanation th_arrived_distance_m double 1.0 threshold distance to check if vehicle has arrived at the route's endpoint th_stopped_time_sec double 1.0 threshold time to check if vehicle is stopped th_stopped_velocity_mps double 0.01 threshold velocity to check if vehicle is stopped disengage_on_route bool true send disengage flag or not when the route is subscribed disengage_on_goal bool true send disengage flag or not when the vehicle is arrived goal","title":"Core Parameters"},{"location":"system/ad_service_state_monitor/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"system/bluetooth_monitor/","text":"bluetooth_monitor # Description # This node monitors a Bluetooth connection to a wireless device by using L2ping. L2ping generates PING echo command on Bluetooth L2CAP layer, and it is able to receive and check echo response from a wireless device. Block diagram # L2ping is only allowed for root by default, so this package provides the following approach to minimize security risks as much as possible: Provide a small program named l2ping_service which performs L2ping and provides wireless device information to bluetooth_monitor by using socket programming. bluetooth_monitor is able to know wireless device information and L2ping status as an unprivileged user since those information are sent by socket communication. Output # bluetooth_monitor: bluetooth_connection # [summary] level message OK OK WARN RTT warning ERROR Lost Function error [values] key value (example) Device [0-9]: Status OK / RTT warning / Verify error / Lost / Ping rejected / Function error Device [0-9]: Name Wireless Controller Device [0-9]: Manufacturer MediaTek, Inc. Device [0-9]: Address AA:BB:CC:DD:EE:FF Device [0-9]: RTT 0.00ms The following key will be added when bluetooth_monitor reports Function error . ex.) The connect system call failed. key (example) value (example) Device [0-9]: connect No such file or directory Parameters # Name Type Default Value Explanation port int 7640 Port number to connect to L2ping service. timeout int 5 Wait timeout seconds for the response. rtt_warn float 0.00 RTT(Round-Trip Time) to generate warn. addresses string * List of bluetooth address of wireless devices to monitor. rtt_warn 0.00(zero) : Disable checking RTT otherwise : Check RTT with specified seconds addresses * : All connected devices AA:BB:CC:DD:EE:FF : You can specify a device to monitor by setting a Bluetooth address Instructions before starting # You can skip this instructions if you run l2ping_service as root user. Assign capability to l2ping_service since L2ping requires cap_net_raw+eip capability. sudo setcap 'cap_net_raw+eip' ./build/bluetooth_monitor/l2ping_service Run l2ping_service and bluetooth_monitor . ./build/bluetooth_monitor/l2ping_service ros2 launch bluetooth_monitor bluetooth_monitor.launch.xml Known limitations and issues # None.","title":"bluetooth_monitor"},{"location":"system/bluetooth_monitor/#bluetooth_monitor","text":"","title":"bluetooth_monitor"},{"location":"system/bluetooth_monitor/#description","text":"This node monitors a Bluetooth connection to a wireless device by using L2ping. L2ping generates PING echo command on Bluetooth L2CAP layer, and it is able to receive and check echo response from a wireless device.","title":"Description"},{"location":"system/bluetooth_monitor/#block-diagram","text":"L2ping is only allowed for root by default, so this package provides the following approach to minimize security risks as much as possible: Provide a small program named l2ping_service which performs L2ping and provides wireless device information to bluetooth_monitor by using socket programming. bluetooth_monitor is able to know wireless device information and L2ping status as an unprivileged user since those information are sent by socket communication.","title":"Block diagram"},{"location":"system/bluetooth_monitor/#output","text":"","title":"Output"},{"location":"system/bluetooth_monitor/#bluetooth_monitor-bluetooth_connection","text":"[summary] level message OK OK WARN RTT warning ERROR Lost Function error [values] key value (example) Device [0-9]: Status OK / RTT warning / Verify error / Lost / Ping rejected / Function error Device [0-9]: Name Wireless Controller Device [0-9]: Manufacturer MediaTek, Inc. Device [0-9]: Address AA:BB:CC:DD:EE:FF Device [0-9]: RTT 0.00ms The following key will be added when bluetooth_monitor reports Function error . ex.) The connect system call failed. key (example) value (example) Device [0-9]: connect No such file or directory","title":"bluetooth_monitor: bluetooth_connection"},{"location":"system/bluetooth_monitor/#parameters","text":"Name Type Default Value Explanation port int 7640 Port number to connect to L2ping service. timeout int 5 Wait timeout seconds for the response. rtt_warn float 0.00 RTT(Round-Trip Time) to generate warn. addresses string * List of bluetooth address of wireless devices to monitor. rtt_warn 0.00(zero) : Disable checking RTT otherwise : Check RTT with specified seconds addresses * : All connected devices AA:BB:CC:DD:EE:FF : You can specify a device to monitor by setting a Bluetooth address","title":"Parameters"},{"location":"system/bluetooth_monitor/#instructions-before-starting","text":"You can skip this instructions if you run l2ping_service as root user. Assign capability to l2ping_service since L2ping requires cap_net_raw+eip capability. sudo setcap 'cap_net_raw+eip' ./build/bluetooth_monitor/l2ping_service Run l2ping_service and bluetooth_monitor . ./build/bluetooth_monitor/l2ping_service ros2 launch bluetooth_monitor bluetooth_monitor.launch.xml","title":"Instructions before starting"},{"location":"system/bluetooth_monitor/#known-limitations-and-issues","text":"None.","title":"Known limitations and issues"},{"location":"system/dummy_diag_publisher/","text":"dummy_diag_publisher # Purpose # This package outputs a dummy diagnostic data for debugging and developing. Inputs / Outputs # Outputs # Name Type Description /diagnostics diagnostic_msgs::msgs::DiagnosticArray Diagnostics outputs Parameters # Node Parameters # Name Type Default Value Explanation Reconfigurable update_rate int 10 Timer callback period [Hz] false diag_name string diag_name Diag_name set by dummy diag publisher false is_active bool true Force update or not true status DummyDiagPublisherNode::Status 0 (OK) diag status set by dummy diag publisher true Assumptions / Known limits # TBD. Usage # ros2 launch dummy_diag_publisher dummy_diag_publisher.launch.xml","title":"dummy_diag_publisher"},{"location":"system/dummy_diag_publisher/#dummy_diag_publisher","text":"","title":"dummy_diag_publisher"},{"location":"system/dummy_diag_publisher/#purpose","text":"This package outputs a dummy diagnostic data for debugging and developing.","title":"Purpose"},{"location":"system/dummy_diag_publisher/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"system/dummy_diag_publisher/#outputs","text":"Name Type Description /diagnostics diagnostic_msgs::msgs::DiagnosticArray Diagnostics outputs","title":"Outputs"},{"location":"system/dummy_diag_publisher/#parameters","text":"","title":"Parameters"},{"location":"system/dummy_diag_publisher/#node-parameters","text":"Name Type Default Value Explanation Reconfigurable update_rate int 10 Timer callback period [Hz] false diag_name string diag_name Diag_name set by dummy diag publisher false is_active bool true Force update or not true status DummyDiagPublisherNode::Status 0 (OK) diag status set by dummy diag publisher true","title":"Node Parameters"},{"location":"system/dummy_diag_publisher/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"system/dummy_diag_publisher/#usage","text":"ros2 launch dummy_diag_publisher dummy_diag_publisher.launch.xml","title":"Usage"},{"location":"system/dummy_infrastructure/","text":"dummy_infrastructure # This is a debug node for infrastructure communication. Usage # ros2 launch dummy_infrastructure dummy_infrastructure.launch.xml ros2 run rqt_reconfigure rqt_reconfigure Inputs / Outputs # Inputs # Name Type Description ~/input/command_array tier4_v2x_msgs::msg::InfrastructureCommandArray Infrastructure command Outputs # Name Type Description ~/output/state_array tier4_v2x_msgs::msg::VirtualTrafficLightStateArray Virtual traffic light array Parameters # Node Parameters # Name Type Default Value Explanation update_rate int 10 Timer callback period [Hz] use_first_command bool true Consider instrument id or not instrument_id string `` Used as command id approval bool false set approval filed to ros param is_finalized bool false Stop at stop_line if finalization isn't completed Assumptions / Known limits # TBD.","title":"dummy_infrastructure"},{"location":"system/dummy_infrastructure/#dummy_infrastructure","text":"This is a debug node for infrastructure communication.","title":"dummy_infrastructure"},{"location":"system/dummy_infrastructure/#usage","text":"ros2 launch dummy_infrastructure dummy_infrastructure.launch.xml ros2 run rqt_reconfigure rqt_reconfigure","title":"Usage"},{"location":"system/dummy_infrastructure/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"system/dummy_infrastructure/#inputs","text":"Name Type Description ~/input/command_array tier4_v2x_msgs::msg::InfrastructureCommandArray Infrastructure command","title":"Inputs"},{"location":"system/dummy_infrastructure/#outputs","text":"Name Type Description ~/output/state_array tier4_v2x_msgs::msg::VirtualTrafficLightStateArray Virtual traffic light array","title":"Outputs"},{"location":"system/dummy_infrastructure/#parameters","text":"","title":"Parameters"},{"location":"system/dummy_infrastructure/#node-parameters","text":"Name Type Default Value Explanation update_rate int 10 Timer callback period [Hz] use_first_command bool true Consider instrument id or not instrument_id string `` Used as command id approval bool false set approval filed to ros param is_finalized bool false Stop at stop_line if finalization isn't completed","title":"Node Parameters"},{"location":"system/dummy_infrastructure/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"system/emergency_handler/","text":"emergency_handler # Purpose # Emergency Handler is a node to select proper MRM from from system failure state contained in HazardStatus. Inner-workings / Algorithms # State Transitions # Inputs / Outputs # Input # Name Type Description /system/emergency/hazard_status autoware_auto_system_msgs::msg::HazardStatusStamped Used to select proper MRM from system failure state contained in HazardStatus /control/vehicle_cmd autoware_auto_control_msgs::msg::AckermannControlCommand Used as reference when generate Emergency Control Command /localization/kinematic_state nav_msgs::msg::Odometry Used to decide whether vehicle is stopped or not /vehicle/status/control_mode autoware_auto_vehicle_msgs::msg::ControlModeReport Used to check vehicle mode: autonomous or manual. Output # Name Type Description /system/emergency/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand Required to execute proper MRM /system/emergency/shift_cmd autoware_auto_vehicle_msgs::msg::GearCommand Required to execute proper MRM (send gear cmd) /system/emergency/hazard_cmd autoware_auto_vehicle_msgs::msg::HazardLightsCommand Required to execute proper MRM (send turn signal cmd) /system/emergency/emergency_state autoware_auto_system_msgs::msg::EmergencyStateStamped Used to inform the emergency situation of the vehicle Parameters # Node Parameters # Name Type Default Value Explanation update_rate int 10 Timer callback period. Core Parameters # Name Type Default Value Explanation timeout_hazard_status double 0.5 If the input hazard_status topic cannot be received for more than timeout_hazard_status , vehicle will make an emergency stop. use_parking_after_stopped bool false If this parameter is true, it will publish PARKING shift command. turning_hazard_on.emergency bool true If this parameter is true, hazard lamps will be turned on during emergency state. Assumptions / Known limits # TBD.","title":"emergency_handler"},{"location":"system/emergency_handler/#emergency_handler","text":"","title":"emergency_handler"},{"location":"system/emergency_handler/#purpose","text":"Emergency Handler is a node to select proper MRM from from system failure state contained in HazardStatus.","title":"Purpose"},{"location":"system/emergency_handler/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"system/emergency_handler/#state-transitions","text":"","title":"State Transitions"},{"location":"system/emergency_handler/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"system/emergency_handler/#input","text":"Name Type Description /system/emergency/hazard_status autoware_auto_system_msgs::msg::HazardStatusStamped Used to select proper MRM from system failure state contained in HazardStatus /control/vehicle_cmd autoware_auto_control_msgs::msg::AckermannControlCommand Used as reference when generate Emergency Control Command /localization/kinematic_state nav_msgs::msg::Odometry Used to decide whether vehicle is stopped or not /vehicle/status/control_mode autoware_auto_vehicle_msgs::msg::ControlModeReport Used to check vehicle mode: autonomous or manual.","title":"Input"},{"location":"system/emergency_handler/#output","text":"Name Type Description /system/emergency/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand Required to execute proper MRM /system/emergency/shift_cmd autoware_auto_vehicle_msgs::msg::GearCommand Required to execute proper MRM (send gear cmd) /system/emergency/hazard_cmd autoware_auto_vehicle_msgs::msg::HazardLightsCommand Required to execute proper MRM (send turn signal cmd) /system/emergency/emergency_state autoware_auto_system_msgs::msg::EmergencyStateStamped Used to inform the emergency situation of the vehicle","title":"Output"},{"location":"system/emergency_handler/#parameters","text":"","title":"Parameters"},{"location":"system/emergency_handler/#node-parameters","text":"Name Type Default Value Explanation update_rate int 10 Timer callback period.","title":"Node Parameters"},{"location":"system/emergency_handler/#core-parameters","text":"Name Type Default Value Explanation timeout_hazard_status double 0.5 If the input hazard_status topic cannot be received for more than timeout_hazard_status , vehicle will make an emergency stop. use_parking_after_stopped bool false If this parameter is true, it will publish PARKING shift command. turning_hazard_on.emergency bool true If this parameter is true, hazard lamps will be turned on during emergency state.","title":"Core Parameters"},{"location":"system/emergency_handler/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"system/system_error_monitor/","text":"system_error_monitor # Purpose # Autoware Error Monitor has two main functions. It is to judge the system hazard level from the aggregated diagnostic information of each module of Autoware. It enables automatic recovery from the emergency state. Inner-workings / Algorithms # State Transition # updateEmergencyHoldingCondition Flow Chart # Inputs / Outputs # Input # Name Type Description /diagnostics_agg diagnostic_msgs::msg::DiagnosticArray Diagnostic information aggregated based diagnostic_aggregator setting is used to /autoware/state autoware_auto_system_msgs::msg::AutowareState Required to ignore error during Route, Planning and Finalizing. /control/current_gate_mode tier4_control_msgs::msg::GateMode Required to select the appropriate module from autonomous_driving or external_control /vehicle/control_mode autoware_auto_vehicle_msgs::msg::ControlModeReport Required to not hold emergency during manual driving Output # Name Type Description /system/emergency/hazard_status autoware_auto_system_msgs::msg::HazardStatusStamped HazardStatus contains system hazard level, emergency hold status and failure details /diagnostics_err diagnostic_msgs::msg::DiagnosticArray This has the same contents as HazardStatus. This is used for visualization Parameters # Node Parameters # Name Type Default Value Explanation ignore_missing_diagnostics bool false If this parameter is turned off, it will be ignored if required modules have not been received. add_leaf_diagnostics bool true Required to use children diagnostics. diag_timeout_sec double 1.0 (sec) If required diagnostic is not received for a diag_timeout_sec , the diagnostic state become STALE state. data_ready_timeout double 30.0 If input topics required for system_error_monitor are not available for data_ready_timeout seconds, autoware_state will translate to emergency state. Core Parameters # Name Type Default Value Explanation hazard_recovery_timeout double 5.0 The vehicle can recovery to normal driving if emergencies disappear during hazard_recovery_timeout . use_emergency_hold bool false If it is false, the vehicle will return to normal as soon as emergencies disappear. use_emergency_hold_in_manual_driving bool false If this parameter is turned off, emergencies will be ignored during manual driving. emergency_hazard_level int 2 If hazard_level is more than emergency_hazard_level, autoware state will translate to emergency state YAML format for system_error_monitor # The parameter key should be filled with the hierarchical diagnostics output by diagnostic_aggregator. Parameters prefixed with required_modules.autonomous_driving are for autonomous driving. Parameters with the required_modules.remote_control prefix are for remote control. If the value is default , the default value will be set. Key Type Default Value Explanation required_modules.autonomous_driving.DIAGNOSTIC_NAME.sf_at string \"none\" Diagnostic level where it becomes Safe Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.autonomous_driving.DIAGNOSTIC_NAME.lf_at string \"warn\" Diagnostic level where it becomes Latent Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.autonomous_driving.DIAGNOSTIC_NAME.spf_at string \"error\" Diagnostic level where it becomes Single Point Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.autonomous_driving.DIAGNOSTIC_NAME.auto_recovery string \"true\" Determines whether the system will automatically recover when it recovers from an error. required_modules.remote_control.DIAGNOSTIC_NAME.sf_at string \"none\" Diagnostic level where it becomes Safe Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.remote_control.DIAGNOSTIC_NAME.lf_at string \"warn\" Diagnostic level where it becomes Latent Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.remote_control.DIAGNOSTIC_NAME.spf_at string \"error\" Diagnostic level where it becomes Single Point Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.remote_control.DIAGNOSTIC_NAME.auto_recovery string \"true\" Determines whether the system will automatically recover when it recovers from an error. Assumptions / Known limits # TBD.","title":"system_error_monitor"},{"location":"system/system_error_monitor/#system_error_monitor","text":"","title":"system_error_monitor"},{"location":"system/system_error_monitor/#purpose","text":"Autoware Error Monitor has two main functions. It is to judge the system hazard level from the aggregated diagnostic information of each module of Autoware. It enables automatic recovery from the emergency state.","title":"Purpose"},{"location":"system/system_error_monitor/#inner-workings-algorithms","text":"","title":"Inner-workings / Algorithms"},{"location":"system/system_error_monitor/#state-transition","text":"","title":"State Transition"},{"location":"system/system_error_monitor/#updateemergencyholdingcondition-flow-chart","text":"","title":"updateEmergencyHoldingCondition Flow Chart"},{"location":"system/system_error_monitor/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"system/system_error_monitor/#input","text":"Name Type Description /diagnostics_agg diagnostic_msgs::msg::DiagnosticArray Diagnostic information aggregated based diagnostic_aggregator setting is used to /autoware/state autoware_auto_system_msgs::msg::AutowareState Required to ignore error during Route, Planning and Finalizing. /control/current_gate_mode tier4_control_msgs::msg::GateMode Required to select the appropriate module from autonomous_driving or external_control /vehicle/control_mode autoware_auto_vehicle_msgs::msg::ControlModeReport Required to not hold emergency during manual driving","title":"Input"},{"location":"system/system_error_monitor/#output","text":"Name Type Description /system/emergency/hazard_status autoware_auto_system_msgs::msg::HazardStatusStamped HazardStatus contains system hazard level, emergency hold status and failure details /diagnostics_err diagnostic_msgs::msg::DiagnosticArray This has the same contents as HazardStatus. This is used for visualization","title":"Output"},{"location":"system/system_error_monitor/#parameters","text":"","title":"Parameters"},{"location":"system/system_error_monitor/#node-parameters","text":"Name Type Default Value Explanation ignore_missing_diagnostics bool false If this parameter is turned off, it will be ignored if required modules have not been received. add_leaf_diagnostics bool true Required to use children diagnostics. diag_timeout_sec double 1.0 (sec) If required diagnostic is not received for a diag_timeout_sec , the diagnostic state become STALE state. data_ready_timeout double 30.0 If input topics required for system_error_monitor are not available for data_ready_timeout seconds, autoware_state will translate to emergency state.","title":"Node Parameters"},{"location":"system/system_error_monitor/#core-parameters","text":"Name Type Default Value Explanation hazard_recovery_timeout double 5.0 The vehicle can recovery to normal driving if emergencies disappear during hazard_recovery_timeout . use_emergency_hold bool false If it is false, the vehicle will return to normal as soon as emergencies disappear. use_emergency_hold_in_manual_driving bool false If this parameter is turned off, emergencies will be ignored during manual driving. emergency_hazard_level int 2 If hazard_level is more than emergency_hazard_level, autoware state will translate to emergency state","title":"Core Parameters"},{"location":"system/system_error_monitor/#yaml-format-for-system_error_monitor","text":"The parameter key should be filled with the hierarchical diagnostics output by diagnostic_aggregator. Parameters prefixed with required_modules.autonomous_driving are for autonomous driving. Parameters with the required_modules.remote_control prefix are for remote control. If the value is default , the default value will be set. Key Type Default Value Explanation required_modules.autonomous_driving.DIAGNOSTIC_NAME.sf_at string \"none\" Diagnostic level where it becomes Safe Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.autonomous_driving.DIAGNOSTIC_NAME.lf_at string \"warn\" Diagnostic level where it becomes Latent Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.autonomous_driving.DIAGNOSTIC_NAME.spf_at string \"error\" Diagnostic level where it becomes Single Point Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.autonomous_driving.DIAGNOSTIC_NAME.auto_recovery string \"true\" Determines whether the system will automatically recover when it recovers from an error. required_modules.remote_control.DIAGNOSTIC_NAME.sf_at string \"none\" Diagnostic level where it becomes Safe Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.remote_control.DIAGNOSTIC_NAME.lf_at string \"warn\" Diagnostic level where it becomes Latent Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.remote_control.DIAGNOSTIC_NAME.spf_at string \"error\" Diagnostic level where it becomes Single Point Fault. Available options are \"none\" , \"warn\" , \"error\" . required_modules.remote_control.DIAGNOSTIC_NAME.auto_recovery string \"true\" Determines whether the system will automatically recover when it recovers from an error.","title":"YAML format for system_error_monitor"},{"location":"system/system_error_monitor/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"system/system_monitor/","text":"System Monitor for Autoware # Further improvement of system monitor functionality for Autoware. Description # This package provides the following nodes for monitoring system: CPU Monitor HDD Monitor Memory Monitor Network Monitor NTP Monitor Process Monitor GPU Monitor Supported architecture # x86_64 arm64v8/aarch64 Operation confirmed platform # PC system intel core i7 NVIDIA Jetson AGX Xavier Raspberry Pi4 Model B How to use # Use colcon build and launch in the same way as other packages. colcon build source install/setup.bash ros2 launch system_monitor system_monitor.launch.xml CPU and GPU monitoring method differs depending on platform. CMake automatically chooses source to be built according to build environment. If you build this package on intel platform, CPU monitor and GPU monitor which run on intel platform are built. ROS topics published by system monitor # Every topic is published in 1 minute interval. CPU Monitor HDD Monitor Mem Monitor Net Monitor NTP Monitor Process Monitor GPU Monitor [Usage] \u2713\uff1aSupported, -\uff1aNot supported Node Message Intel arm64(tegra) arm64(raspi) Notes CPU Monitor CPU Temperature \u2713 \u2713 \u2713 CPU Usage \u2713 \u2713 \u2713 CPU Load Average \u2713 \u2713 \u2713 CPU Thermal Throttling \u2713 - \u2713 CPU Frequency \u2713 \u2713 \u2713 Notification of frequency only, normally error not generated. HDD Monitor HDD Temperature \u2713 \u2713 \u2713 HDD PowerOnHours \u2713 \u2713 \u2713 HDD TotalDataWritten \u2713 \u2713 \u2713 HDD Usage \u2713 \u2713 \u2713 Memory Monitor Memory Usage \u2713 \u2713 \u2713 Net Monitor Network Usage \u2713 \u2713 \u2713 NTP Monitor NTP Offset \u2713 \u2713 \u2713 Process Monitor Tasks Summary \u2713 \u2713 \u2713 High-load Proc[0-9] \u2713 \u2713 \u2713 High-mem Proc[0-9] \u2713 \u2713 \u2713 GPU Monitor GPU Temperature \u2713 \u2713 - GPU Usage \u2713 \u2713 - GPU Memory Usage \u2713 - - GPU Thermal Throttling \u2713 - - GPU Frequency - \u2713 - ROS parameters # See ROS parameters . Notes # CPU monitor for intel platform # Thermal throttling event can be monitored by reading contents of MSR(Model Specific Register), and accessing MSR is only allowed for root by default, so this package provides the following approach to minimize security risks as much as possible: Provide a small program named 'msr_reader' which accesses MSR and sends thermal throttling status to CPU monitor by using socket programming. Run 'msr_reader' as a specific user instead of root. CPU monitor is able to know the status as an unprivileged user since thermal throttling status is sent by socket communication. Instructions before starting # Create a user to run 'msr_reader'. sudo adduser <username> Load kernel module 'msr' into your target system. The path '/dev/cpu/CPUNUM/msr' appears. sudo modprobe msr Allow user to access MSR with read-only privilege using the Access Control List (ACL). sudo setfacl -m u:<username>:r /dev/cpu/*/msr Assign capability to 'msr_reader' since msr kernel module requires rawio capability. sudo setcap cap_sys_rawio = ep install/system_monitor/lib/system_monitor/msr_reader Run 'msr_reader' as the user you created, and run system_monitor as a generic user. su <username> install/system_monitor/lib/system_monitor/msr_reader See also # msr_reader HDD Monitor # Generally, S.M.A.R.T. information is used to monitor HDD temperature and life of HDD, and normally accessing disk device node is allowed for root user or disk group. As with the CPU monitor, this package provides an approach to minimize security risks as much as possible: Provide a small program named 'hdd_reader' which accesses S.M.A.R.T. information and sends some items of it to HDD monitor by using socket programming. Run 'hdd_reader' as a specific user. HDD monitor is able to know some items of S.M.A.R.T. information as an unprivileged user since those are sent by socket communication. Instructions before starting # Create a user to run 'hdd_reader'. sudo adduser <username> Add user to the disk group. sudo usermod -a -G disk <username> Assign capabilities to 'hdd_reader' since SCSI kernel module requires rawio capability to send ATA PASS-THROUGH (12) command and NVMe kernel module requires admin capability to send Admin Command. sudo setcap 'cap_sys_rawio=ep cap_sys_admin=ep' install/system_monitor/lib/system_monitor/hdd_reader Run 'hdd_reader' as the user you created, and run system_monitor as a generic user. su <username> install/system_monitor/lib/system_monitor/hdd_reader See also # hdd_reader GPU Monitor for intel platform # Currently GPU monitor for intel platform only supports NVIDIA GPU whose information can be accessed by NVML API. Also you need to install CUDA libraries. For installation instructions for CUDA 10.0, see NVIDIA CUDA Installation Guide for Linux . UML diagrams # See Class diagrams . See Sequence diagrams .","title":"System Monitor for Autoware"},{"location":"system/system_monitor/#system-monitor-for-autoware","text":"Further improvement of system monitor functionality for Autoware.","title":"System Monitor for Autoware"},{"location":"system/system_monitor/#description","text":"This package provides the following nodes for monitoring system: CPU Monitor HDD Monitor Memory Monitor Network Monitor NTP Monitor Process Monitor GPU Monitor","title":"Description"},{"location":"system/system_monitor/#supported-architecture","text":"x86_64 arm64v8/aarch64","title":"Supported architecture"},{"location":"system/system_monitor/#operation-confirmed-platform","text":"PC system intel core i7 NVIDIA Jetson AGX Xavier Raspberry Pi4 Model B","title":"Operation confirmed platform"},{"location":"system/system_monitor/#how-to-use","text":"Use colcon build and launch in the same way as other packages. colcon build source install/setup.bash ros2 launch system_monitor system_monitor.launch.xml CPU and GPU monitoring method differs depending on platform. CMake automatically chooses source to be built according to build environment. If you build this package on intel platform, CPU monitor and GPU monitor which run on intel platform are built.","title":"How to use"},{"location":"system/system_monitor/#ros-topics-published-by-system-monitor","text":"Every topic is published in 1 minute interval. CPU Monitor HDD Monitor Mem Monitor Net Monitor NTP Monitor Process Monitor GPU Monitor [Usage] \u2713\uff1aSupported, -\uff1aNot supported Node Message Intel arm64(tegra) arm64(raspi) Notes CPU Monitor CPU Temperature \u2713 \u2713 \u2713 CPU Usage \u2713 \u2713 \u2713 CPU Load Average \u2713 \u2713 \u2713 CPU Thermal Throttling \u2713 - \u2713 CPU Frequency \u2713 \u2713 \u2713 Notification of frequency only, normally error not generated. HDD Monitor HDD Temperature \u2713 \u2713 \u2713 HDD PowerOnHours \u2713 \u2713 \u2713 HDD TotalDataWritten \u2713 \u2713 \u2713 HDD Usage \u2713 \u2713 \u2713 Memory Monitor Memory Usage \u2713 \u2713 \u2713 Net Monitor Network Usage \u2713 \u2713 \u2713 NTP Monitor NTP Offset \u2713 \u2713 \u2713 Process Monitor Tasks Summary \u2713 \u2713 \u2713 High-load Proc[0-9] \u2713 \u2713 \u2713 High-mem Proc[0-9] \u2713 \u2713 \u2713 GPU Monitor GPU Temperature \u2713 \u2713 - GPU Usage \u2713 \u2713 - GPU Memory Usage \u2713 - - GPU Thermal Throttling \u2713 - - GPU Frequency - \u2713 -","title":"ROS topics published by system monitor"},{"location":"system/system_monitor/#ros-parameters","text":"See ROS parameters .","title":"ROS parameters"},{"location":"system/system_monitor/#notes","text":"","title":"Notes"},{"location":"system/system_monitor/#cpu-monitor-for-intel-platform","text":"Thermal throttling event can be monitored by reading contents of MSR(Model Specific Register), and accessing MSR is only allowed for root by default, so this package provides the following approach to minimize security risks as much as possible: Provide a small program named 'msr_reader' which accesses MSR and sends thermal throttling status to CPU monitor by using socket programming. Run 'msr_reader' as a specific user instead of root. CPU monitor is able to know the status as an unprivileged user since thermal throttling status is sent by socket communication.","title":"CPU monitor for intel platform"},{"location":"system/system_monitor/#instructions-before-starting","text":"Create a user to run 'msr_reader'. sudo adduser <username> Load kernel module 'msr' into your target system. The path '/dev/cpu/CPUNUM/msr' appears. sudo modprobe msr Allow user to access MSR with read-only privilege using the Access Control List (ACL). sudo setfacl -m u:<username>:r /dev/cpu/*/msr Assign capability to 'msr_reader' since msr kernel module requires rawio capability. sudo setcap cap_sys_rawio = ep install/system_monitor/lib/system_monitor/msr_reader Run 'msr_reader' as the user you created, and run system_monitor as a generic user. su <username> install/system_monitor/lib/system_monitor/msr_reader","title":"Instructions before starting"},{"location":"system/system_monitor/#see-also","text":"msr_reader","title":"See also"},{"location":"system/system_monitor/#hdd-monitor","text":"Generally, S.M.A.R.T. information is used to monitor HDD temperature and life of HDD, and normally accessing disk device node is allowed for root user or disk group. As with the CPU monitor, this package provides an approach to minimize security risks as much as possible: Provide a small program named 'hdd_reader' which accesses S.M.A.R.T. information and sends some items of it to HDD monitor by using socket programming. Run 'hdd_reader' as a specific user. HDD monitor is able to know some items of S.M.A.R.T. information as an unprivileged user since those are sent by socket communication.","title":"HDD Monitor"},{"location":"system/system_monitor/#instructions-before-starting_1","text":"Create a user to run 'hdd_reader'. sudo adduser <username> Add user to the disk group. sudo usermod -a -G disk <username> Assign capabilities to 'hdd_reader' since SCSI kernel module requires rawio capability to send ATA PASS-THROUGH (12) command and NVMe kernel module requires admin capability to send Admin Command. sudo setcap 'cap_sys_rawio=ep cap_sys_admin=ep' install/system_monitor/lib/system_monitor/hdd_reader Run 'hdd_reader' as the user you created, and run system_monitor as a generic user. su <username> install/system_monitor/lib/system_monitor/hdd_reader","title":"Instructions before starting"},{"location":"system/system_monitor/#see-also_1","text":"hdd_reader","title":"See also"},{"location":"system/system_monitor/#gpu-monitor-for-intel-platform","text":"Currently GPU monitor for intel platform only supports NVIDIA GPU whose information can be accessed by NVML API. Also you need to install CUDA libraries. For installation instructions for CUDA 10.0, see NVIDIA CUDA Installation Guide for Linux .","title":"GPU Monitor for intel platform"},{"location":"system/system_monitor/#uml-diagrams","text":"See Class diagrams . See Sequence diagrams .","title":"UML diagrams"},{"location":"system/system_monitor/docs/class_diagrams/","text":"Class diagrams # CPU Monitor # HDD Monitor # Memory Monitor # Net Monitor # NTP Monitor # Process Monitor # GPU Monitor #","title":"Class diagrams"},{"location":"system/system_monitor/docs/class_diagrams/#class-diagrams","text":"","title":"Class diagrams"},{"location":"system/system_monitor/docs/class_diagrams/#cpu-monitor","text":"","title":"CPU Monitor"},{"location":"system/system_monitor/docs/class_diagrams/#hdd-monitor","text":"","title":"HDD Monitor"},{"location":"system/system_monitor/docs/class_diagrams/#memory-monitor","text":"","title":"Memory Monitor"},{"location":"system/system_monitor/docs/class_diagrams/#net-monitor","text":"","title":"Net Monitor"},{"location":"system/system_monitor/docs/class_diagrams/#ntp-monitor","text":"","title":"NTP Monitor"},{"location":"system/system_monitor/docs/class_diagrams/#process-monitor","text":"","title":"Process Monitor"},{"location":"system/system_monitor/docs/class_diagrams/#gpu-monitor","text":"","title":"GPU Monitor"},{"location":"system/system_monitor/docs/hdd_reader/","text":"hdd_reader # Name # hdd_reader - Read S.M.A.R.T. information for monitoring HDD temperature and life of HDD Synopsis # hdd_reader [OPTION] Description # Read S.M.A.R.T. information for monitoring HDD temperature and life of HDD. This runs as a daemon process and listens to a TCP/IP port (7635 by default). Options: -h, --help Display help -p, --port # Port number to listen to Exit status: Returns 0 if OK; non-zero otherwise. Notes # The 'hdd_reader' accesses minimal data enough to get Model number, Serial number, HDD temperature, and life of HDD. This is an approach to limit its functionality, however, the functionality can be expanded for further improvements and considerations in the future. [ATA] # Purpose Name Length Model number, Serial number IDENTIFY DEVICE data 256 words(512 bytes) HDD temperature, life of HDD SMART READ DATA 256 words(512 bytes) For details please see the documents below. ATA Command Set - 4 (ACS-4) ATA/ATAPI Command Set - 3 (ACS-3) SMART Attribute Overview SMART Attribute Annex [NVMe] # Purpose Name Length Model number, Serial number Identify Controller data structure 4096 bytes HDD temperature, life of HDD SMART / Health Information 36 Dword(144 bytes) For details please see the documents below. NVM Express 1.2b Operation confirmed drives # SAMSUNG MZVLB1T0HALR (SSD) Western Digital My Passport (Portable HDD)","title":"hdd_reader"},{"location":"system/system_monitor/docs/hdd_reader/#hdd_reader","text":"","title":"hdd_reader"},{"location":"system/system_monitor/docs/hdd_reader/#name","text":"hdd_reader - Read S.M.A.R.T. information for monitoring HDD temperature and life of HDD","title":"Name"},{"location":"system/system_monitor/docs/hdd_reader/#synopsis","text":"hdd_reader [OPTION]","title":"Synopsis"},{"location":"system/system_monitor/docs/hdd_reader/#description","text":"Read S.M.A.R.T. information for monitoring HDD temperature and life of HDD. This runs as a daemon process and listens to a TCP/IP port (7635 by default). Options: -h, --help Display help -p, --port # Port number to listen to Exit status: Returns 0 if OK; non-zero otherwise.","title":"Description"},{"location":"system/system_monitor/docs/hdd_reader/#notes","text":"The 'hdd_reader' accesses minimal data enough to get Model number, Serial number, HDD temperature, and life of HDD. This is an approach to limit its functionality, however, the functionality can be expanded for further improvements and considerations in the future.","title":"Notes"},{"location":"system/system_monitor/docs/hdd_reader/#ata","text":"Purpose Name Length Model number, Serial number IDENTIFY DEVICE data 256 words(512 bytes) HDD temperature, life of HDD SMART READ DATA 256 words(512 bytes) For details please see the documents below. ATA Command Set - 4 (ACS-4) ATA/ATAPI Command Set - 3 (ACS-3) SMART Attribute Overview SMART Attribute Annex","title":"[ATA]"},{"location":"system/system_monitor/docs/hdd_reader/#nvme","text":"Purpose Name Length Model number, Serial number Identify Controller data structure 4096 bytes HDD temperature, life of HDD SMART / Health Information 36 Dword(144 bytes) For details please see the documents below. NVM Express 1.2b","title":"[NVMe]"},{"location":"system/system_monitor/docs/hdd_reader/#operation-confirmed-drives","text":"SAMSUNG MZVLB1T0HALR (SSD) Western Digital My Passport (Portable HDD)","title":"Operation confirmed drives"},{"location":"system/system_monitor/docs/msr_reader/","text":"msr_reader # Name # msr_reader - Read MSR register for monitoring thermal throttling event Synopsis # msr_reader [OPTION] Description # Read MSR register for monitoring thermal throttling event. This runs as a daemon process and listens to a TCP/IP port (7634 by default). Options: -h, --help Display help -p, --port # Port number to listen to Exit status: Returns 0 if OK; non-zero otherwise. Notes # The 'msr_reader' accesses minimal data enough to get thermal throttling event. This is an approach to limit its functionality, however, the functionality can be expanded for further improvements and considerations in the future. Register Address Name Length 1B1H IA32_PACKAGE_THERM_STATUS 64bit For details please see the documents below. Intel\u00ae 64 and IA-32 ArchitecturesSoftware Developer\u2019s Manual Operation confirmed platform # PC system intel core i7","title":"msr_reader"},{"location":"system/system_monitor/docs/msr_reader/#msr_reader","text":"","title":"msr_reader"},{"location":"system/system_monitor/docs/msr_reader/#name","text":"msr_reader - Read MSR register for monitoring thermal throttling event","title":"Name"},{"location":"system/system_monitor/docs/msr_reader/#synopsis","text":"msr_reader [OPTION]","title":"Synopsis"},{"location":"system/system_monitor/docs/msr_reader/#description","text":"Read MSR register for monitoring thermal throttling event. This runs as a daemon process and listens to a TCP/IP port (7634 by default). Options: -h, --help Display help -p, --port # Port number to listen to Exit status: Returns 0 if OK; non-zero otherwise.","title":"Description"},{"location":"system/system_monitor/docs/msr_reader/#notes","text":"The 'msr_reader' accesses minimal data enough to get thermal throttling event. This is an approach to limit its functionality, however, the functionality can be expanded for further improvements and considerations in the future. Register Address Name Length 1B1H IA32_PACKAGE_THERM_STATUS 64bit For details please see the documents below. Intel\u00ae 64 and IA-32 ArchitecturesSoftware Developer\u2019s Manual","title":"Notes"},{"location":"system/system_monitor/docs/msr_reader/#operation-confirmed-platform","text":"PC system intel core i7","title":"Operation confirmed platform"},{"location":"system/system_monitor/docs/ros_parameters/","text":"ROS parameters # CPU Monitor # cpu_monitor: Name Type Unit Default Notes temp_warn float DegC 90.0 Generates warning when CPU temperature reaches a specified value or higher. temp_error float DegC 95.0 Generates error when CPU temperature reaches a specified value or higher. usage_warn float %(1e-2) 0.90 Generates warning when CPU usage reaches a specified value or higher and last for usage_warn_count counts. usage_error float %(1e-2) 1.00 Generates error when CPU usage reaches a specified value or higher and last for usage_error_count counts. usage_warn_count int n/a 2 Generates warning when CPU usage reaches usage_warn value or higher and last for a specified counts. usage_error_count int n/a 2 Generates error when CPU usage reaches usage_error value or higher and last for a specified counts. load1_warn float %(1e-2) 0.90 Generates warning when load average 1min reaches a specified value or higher. load5_warn float %(1e-2) 0.80 Generates warning when load average 5min reaches a specified value or higher. msr_reader_port int n/a 7634 Port number to connect to msr_reader. HDD Monitor # hdd_monitor: disks: Name Type Unit Default Notes name string n/a none The disk name to monitor temperature. (e.g. /dev/sda) temp_warn float DegC 55.0 Generates warning when HDD temperature reaches a specified value or higher. temp_error float DegC 70.0 Generates error when HDD temperature reaches a specified value or higher. power_on_hours_warn int Hour 3000000 Generates warning when HDD power-on hours reaches a specified value or higher. total_data_written_attribute_id int n/a 0xF1 S.M.A.R.T attribute ID of total data written. total_data_written_warn int depends on device 4915200 Generates warning when HDD total data written reaches a specified value or higher. total_data_written_safety_factor int %(1e-2) 0.05 Safety factor of HDD total data written. hdd_monitor: Name Type Unit Default Notes hdd_reader_port int n/a 7635 Port number to connect to hdd_reader. usage_warn float %(1e-2) 0.95 Generates warning when disk usage reaches a specified value or higher. usage_error float %(1e-2) 0.99 Generates error when disk usage reaches a specified value or higher. Memory Monitor # mem_monitor: Name Type Unit Default Notes usage_warn float %(1e-2) 0.95 Generates warning when physical memory usage reaches a specified value or higher. usage_error float %(1e-2) 0.99 Generates error when physical memory usage reaches a specified value or higher. Net Monitor # net_monitor: Name Type Unit Default Notes devices list[string] n/a none The name of network interface to monitor. (e.g. eth0, * for all network interfaces) usage_warn float %(1e-2) 0.95 Generates warning when network usage reaches a specified value or higher. NTP Monitor # ntp_monitor: Name Type Unit Default Notes server string n/a ntp.ubuntu.com The name of NTP server to synchronize date and time. (e.g. ntp.nict.jp for Japan) offset_warn float sec 0.1 Generates warning when NTP offset reaches a specified value or higher. (default is 100ms) offset_error float sec 5.0 Generates warning when NTP offset reaches a specified value or higher. (default is 5sec) Process Monitor # process_monitor: Name Type Unit Default Notes num_of_procs int n/a 5 The number of processes to generate High-load Proc[0-9] and High-mem Proc[0-9]. GPU Monitor # gpu_monitor: Name Type Unit Default Notes temp_warn float DegC 90.0 Generates warning when GPU temperature reaches a specified value or higher. temp_error float DegC 95.0 Generates error when GPU temperature reaches a specified value or higher. gpu_usage_warn float %(1e-2) 0.90 Generates warning when GPU usage reaches a specified value or higher. gpu_usage_error float %(1e-2) 1.00 Generates error when GPU usage reaches a specified value or higher. memory_usage_warn float %(1e-2) 0.90 Generates warning when GPU memory usage reaches a specified value or higher. memory_usage_error float %(1e-2) 1.00 Generates error when GPU memory usage reaches a specified value or higher.","title":"ROS parameters"},{"location":"system/system_monitor/docs/ros_parameters/#ros-parameters","text":"","title":"ROS parameters"},{"location":"system/system_monitor/docs/ros_parameters/#cpu-monitor","text":"cpu_monitor: Name Type Unit Default Notes temp_warn float DegC 90.0 Generates warning when CPU temperature reaches a specified value or higher. temp_error float DegC 95.0 Generates error when CPU temperature reaches a specified value or higher. usage_warn float %(1e-2) 0.90 Generates warning when CPU usage reaches a specified value or higher and last for usage_warn_count counts. usage_error float %(1e-2) 1.00 Generates error when CPU usage reaches a specified value or higher and last for usage_error_count counts. usage_warn_count int n/a 2 Generates warning when CPU usage reaches usage_warn value or higher and last for a specified counts. usage_error_count int n/a 2 Generates error when CPU usage reaches usage_error value or higher and last for a specified counts. load1_warn float %(1e-2) 0.90 Generates warning when load average 1min reaches a specified value or higher. load5_warn float %(1e-2) 0.80 Generates warning when load average 5min reaches a specified value or higher. msr_reader_port int n/a 7634 Port number to connect to msr_reader.","title":"CPU Monitor"},{"location":"system/system_monitor/docs/ros_parameters/#hdd-monitor","text":"hdd_monitor: disks: Name Type Unit Default Notes name string n/a none The disk name to monitor temperature. (e.g. /dev/sda) temp_warn float DegC 55.0 Generates warning when HDD temperature reaches a specified value or higher. temp_error float DegC 70.0 Generates error when HDD temperature reaches a specified value or higher. power_on_hours_warn int Hour 3000000 Generates warning when HDD power-on hours reaches a specified value or higher. total_data_written_attribute_id int n/a 0xF1 S.M.A.R.T attribute ID of total data written. total_data_written_warn int depends on device 4915200 Generates warning when HDD total data written reaches a specified value or higher. total_data_written_safety_factor int %(1e-2) 0.05 Safety factor of HDD total data written. hdd_monitor: Name Type Unit Default Notes hdd_reader_port int n/a 7635 Port number to connect to hdd_reader. usage_warn float %(1e-2) 0.95 Generates warning when disk usage reaches a specified value or higher. usage_error float %(1e-2) 0.99 Generates error when disk usage reaches a specified value or higher.","title":"HDD Monitor"},{"location":"system/system_monitor/docs/ros_parameters/#memory-monitor","text":"mem_monitor: Name Type Unit Default Notes usage_warn float %(1e-2) 0.95 Generates warning when physical memory usage reaches a specified value or higher. usage_error float %(1e-2) 0.99 Generates error when physical memory usage reaches a specified value or higher.","title":"Memory Monitor"},{"location":"system/system_monitor/docs/ros_parameters/#net-monitor","text":"net_monitor: Name Type Unit Default Notes devices list[string] n/a none The name of network interface to monitor. (e.g. eth0, * for all network interfaces) usage_warn float %(1e-2) 0.95 Generates warning when network usage reaches a specified value or higher.","title":"Net Monitor"},{"location":"system/system_monitor/docs/ros_parameters/#ntp-monitor","text":"ntp_monitor: Name Type Unit Default Notes server string n/a ntp.ubuntu.com The name of NTP server to synchronize date and time. (e.g. ntp.nict.jp for Japan) offset_warn float sec 0.1 Generates warning when NTP offset reaches a specified value or higher. (default is 100ms) offset_error float sec 5.0 Generates warning when NTP offset reaches a specified value or higher. (default is 5sec)","title":"NTP Monitor"},{"location":"system/system_monitor/docs/ros_parameters/#process-monitor","text":"process_monitor: Name Type Unit Default Notes num_of_procs int n/a 5 The number of processes to generate High-load Proc[0-9] and High-mem Proc[0-9].","title":"Process Monitor"},{"location":"system/system_monitor/docs/ros_parameters/#gpu-monitor","text":"gpu_monitor: Name Type Unit Default Notes temp_warn float DegC 90.0 Generates warning when GPU temperature reaches a specified value or higher. temp_error float DegC 95.0 Generates error when GPU temperature reaches a specified value or higher. gpu_usage_warn float %(1e-2) 0.90 Generates warning when GPU usage reaches a specified value or higher. gpu_usage_error float %(1e-2) 1.00 Generates error when GPU usage reaches a specified value or higher. memory_usage_warn float %(1e-2) 0.90 Generates warning when GPU memory usage reaches a specified value or higher. memory_usage_error float %(1e-2) 1.00 Generates error when GPU memory usage reaches a specified value or higher.","title":"GPU Monitor"},{"location":"system/system_monitor/docs/seq_diagrams/","text":"Sequence diagrams # CPU Monitor # HDD Monitor # Memory Monitor # Net Monitor # NTP Monitor # Process Monitor # GPU Monitor #","title":"Sequence diagrams"},{"location":"system/system_monitor/docs/seq_diagrams/#sequence-diagrams","text":"","title":"Sequence diagrams"},{"location":"system/system_monitor/docs/seq_diagrams/#cpu-monitor","text":"","title":"CPU Monitor"},{"location":"system/system_monitor/docs/seq_diagrams/#hdd-monitor","text":"","title":"HDD Monitor"},{"location":"system/system_monitor/docs/seq_diagrams/#memory-monitor","text":"","title":"Memory Monitor"},{"location":"system/system_monitor/docs/seq_diagrams/#net-monitor","text":"","title":"Net Monitor"},{"location":"system/system_monitor/docs/seq_diagrams/#ntp-monitor","text":"","title":"NTP Monitor"},{"location":"system/system_monitor/docs/seq_diagrams/#process-monitor","text":"","title":"Process Monitor"},{"location":"system/system_monitor/docs/seq_diagrams/#gpu-monitor","text":"","title":"GPU Monitor"},{"location":"system/system_monitor/docs/topics_cpu_monitor/","text":"ROS topics: CPU Monitor # CPU Temperature # /diagnostics/cpu_monitor: CPU Temperature [summary] level message OK OK [values] key (example) value (example) Package id 0, Core [0-9], thermal_zone[0-9] 50.0 DegC *key: thermal_zone[0-9] for ARM architecture. CPU Usage # /diagnostics/cpu_monitor: CPU Usage [summary] level message OK OK WARN high load ERROR very high Lload [values] key value (example) CPU [all,0-9]: status OK / high load / very high load CPU [all,0-9]: usr 2.00% CPU [all,0-9]: nice 0.00% CPU [all,0-9]: sys 1.00% CPU [all,0-9]: idle 97.00% CPU Load Average # /diagnostics/cpu_monitor: CPU Load Average [summary] level message OK OK WARN high load [values] key value (example) 1min 14.50% 5min 14.55% 15min 9.67% CPU Thermal Throttling # Intel and raspi platform only. Tegra platform not supported. /diagnostics/cpu_monitor: CPU Thermal Throttling [summary] level message OK OK ERROR throttling [values for intel platform] key value (example) CPU [0-9]: Pkg Thermal Status OK / throttling [values for raspi platform] key value (example) status All clear / Currently throttled / Soft temperature limit active CPU Frequency # /diagnostics/cpu_monitor: CPU Frequency [summary] level message OK OK [values] key value (example) CPU [0-9]: clock 2879MHz","title":"ROS topics: CPU Monitor"},{"location":"system/system_monitor/docs/topics_cpu_monitor/#ros-topics-cpu-monitor","text":"","title":"ROS topics: CPU Monitor"},{"location":"system/system_monitor/docs/topics_cpu_monitor/#cpu-temperature","text":"/diagnostics/cpu_monitor: CPU Temperature [summary] level message OK OK [values] key (example) value (example) Package id 0, Core [0-9], thermal_zone[0-9] 50.0 DegC *key: thermal_zone[0-9] for ARM architecture.","title":"CPU Temperature"},{"location":"system/system_monitor/docs/topics_cpu_monitor/#cpu-usage","text":"/diagnostics/cpu_monitor: CPU Usage [summary] level message OK OK WARN high load ERROR very high Lload [values] key value (example) CPU [all,0-9]: status OK / high load / very high load CPU [all,0-9]: usr 2.00% CPU [all,0-9]: nice 0.00% CPU [all,0-9]: sys 1.00% CPU [all,0-9]: idle 97.00%","title":"CPU Usage"},{"location":"system/system_monitor/docs/topics_cpu_monitor/#cpu-load-average","text":"/diagnostics/cpu_monitor: CPU Load Average [summary] level message OK OK WARN high load [values] key value (example) 1min 14.50% 5min 14.55% 15min 9.67%","title":"CPU Load Average"},{"location":"system/system_monitor/docs/topics_cpu_monitor/#cpu-thermal-throttling","text":"Intel and raspi platform only. Tegra platform not supported. /diagnostics/cpu_monitor: CPU Thermal Throttling [summary] level message OK OK ERROR throttling [values for intel platform] key value (example) CPU [0-9]: Pkg Thermal Status OK / throttling [values for raspi platform] key value (example) status All clear / Currently throttled / Soft temperature limit active","title":"CPU Thermal Throttling"},{"location":"system/system_monitor/docs/topics_cpu_monitor/#cpu-frequency","text":"/diagnostics/cpu_monitor: CPU Frequency [summary] level message OK OK [values] key value (example) CPU [0-9]: clock 2879MHz","title":"CPU Frequency"},{"location":"system/system_monitor/docs/topics_gpu_monitor/","text":"ROS topics: GPU Monitor # Intel and tegra platform only. Raspi platform not supported. GPU Temperature # /diagnostics/gpu_monitor: GPU Temperature [summary] level message OK OK WARN warm ERROR hot [values] key (example) value (example) GeForce GTX 1650, thermal_zone[0-9] 46.0 DegC *key: thermal_zone[0-9] for ARM architecture. GPU Usage # /diagnostics/gpu_monitor: GPU Usage [summary] level message OK OK WARN high load ERROR very high load [values] key value (example) GPU [0-9]: status OK / high load / very high load GPU [0-9]: name GeForce GTX 1650, gpu.[0-9] GPU [0-9]: usage 19.0% *key: gpu.[0-9] for ARM architecture. GPU Memory Usage # Intel platform only. There is no separate gpu memory in tegra. Both cpu and gpu uses cpu memory. /diagnostics/gpu_monitor: GPU Memory Usage [summary] level message OK OK WARN high load ERROR very high load [values] key value (example) GPU [0-9]: status OK / high load / very high load GPU [0-9]: name GeForce GTX 1650 GPU [0-9]: usage 13.0% GPU [0-9]: total 3G GPU [0-9]: used 1G GPU [0-9]: free 2G GPU Thermal Throttling # Intel platform only. Tegra platform not supported. /diagnostics/gpu_monitor: GPU Thermal Throttling [summary] level message OK OK ERROR throttling [values] key value (example) GPU [0-9]: status OK / throttling GPU [0-9]: name GeForce GTX 1650 GPU [0-9]: graphics clock 1020 MHz GPU [0-9]: reasons GpuIdle / SwThermalSlowdown etc. GPU Frequency # Tegra platform only. /diagnostics/gpu_monitor: GPU Frequency [summary] level message OK OK [values] key (example) value (example) GPU 17000000.gv11b: clock 318 MHz","title":"ROS topics: GPU Monitor"},{"location":"system/system_monitor/docs/topics_gpu_monitor/#ros-topics-gpu-monitor","text":"Intel and tegra platform only. Raspi platform not supported.","title":"ROS topics: GPU Monitor"},{"location":"system/system_monitor/docs/topics_gpu_monitor/#gpu-temperature","text":"/diagnostics/gpu_monitor: GPU Temperature [summary] level message OK OK WARN warm ERROR hot [values] key (example) value (example) GeForce GTX 1650, thermal_zone[0-9] 46.0 DegC *key: thermal_zone[0-9] for ARM architecture.","title":"GPU Temperature"},{"location":"system/system_monitor/docs/topics_gpu_monitor/#gpu-usage","text":"/diagnostics/gpu_monitor: GPU Usage [summary] level message OK OK WARN high load ERROR very high load [values] key value (example) GPU [0-9]: status OK / high load / very high load GPU [0-9]: name GeForce GTX 1650, gpu.[0-9] GPU [0-9]: usage 19.0% *key: gpu.[0-9] for ARM architecture.","title":"GPU Usage"},{"location":"system/system_monitor/docs/topics_gpu_monitor/#gpu-memory-usage","text":"Intel platform only. There is no separate gpu memory in tegra. Both cpu and gpu uses cpu memory. /diagnostics/gpu_monitor: GPU Memory Usage [summary] level message OK OK WARN high load ERROR very high load [values] key value (example) GPU [0-9]: status OK / high load / very high load GPU [0-9]: name GeForce GTX 1650 GPU [0-9]: usage 13.0% GPU [0-9]: total 3G GPU [0-9]: used 1G GPU [0-9]: free 2G","title":"GPU Memory Usage"},{"location":"system/system_monitor/docs/topics_gpu_monitor/#gpu-thermal-throttling","text":"Intel platform only. Tegra platform not supported. /diagnostics/gpu_monitor: GPU Thermal Throttling [summary] level message OK OK ERROR throttling [values] key value (example) GPU [0-9]: status OK / throttling GPU [0-9]: name GeForce GTX 1650 GPU [0-9]: graphics clock 1020 MHz GPU [0-9]: reasons GpuIdle / SwThermalSlowdown etc.","title":"GPU Thermal Throttling"},{"location":"system/system_monitor/docs/topics_gpu_monitor/#gpu-frequency","text":"Tegra platform only. /diagnostics/gpu_monitor: GPU Frequency [summary] level message OK OK [values] key (example) value (example) GPU 17000000.gv11b: clock 318 MHz","title":"GPU Frequency"},{"location":"system/system_monitor/docs/topics_hdd_monitor/","text":"ROS topics: HDD Monitor # HDD Temperature # /diagnostics/hdd_monitor: HDD Temperature [summary] level message OK OK WARN hot ERROR critical hot [values] key value (example) HDD [0-9]: status OK / hot / critical hot HDD [0-9]: name /dev/nvme0 HDD [0-9]: model SAMSUNG MZVLB1T0HBLR-000L7 HDD [0-9]: serial S4EMNF0M820682 HDD [0-9]: temperature 37.0 DegC HDD PowerOnHours # /diagnostics/hdd_monitor: HDD PowerOnHours [summary] level message OK OK WARN lifetime limit [values] key value (example) HDD [0-9]: status OK / lifetime limit HDD [0-9]: name /dev/nvme0 HDD [0-9]: model PHISON PS5012-E12S-512G HDD [0-9]: serial FB590709182505050767 HDD [0-9]: power on hours 4834 Hours HDD TotalDataWritten # /diagnostics/hdd_monitor: HDD TotalDataWritten [summary] level message OK OK WARN warranty period [values] key value (example) HDD [0-9]: status OK / warranty period HDD [0-9]: name /dev/nvme0 HDD [0-9]: model PHISON PS5012-E12S-512G HDD [0-9]: serial FB590709182505050767 HDD [0-9]: total data written 146295330 not available HDD Usage # /diagnostics/hdd_monitor: HDD Usage [summary] level message OK OK WARN low disk space ERROR very low disk space [values] key value (example) HDD [0-9]: status OK / low disk space / very low disk space HDD [0-9]: filesystem /dev/nvme0n1p4 HDD [0-9]: size 264G HDD [0-9]: used 172G HDD [0-9]: avail 749G HDD [0-9]: use 69% HDD [0-9]: mounted on /","title":"ROS topics: HDD Monitor"},{"location":"system/system_monitor/docs/topics_hdd_monitor/#ros-topics-hdd-monitor","text":"","title":"ROS topics: HDD Monitor"},{"location":"system/system_monitor/docs/topics_hdd_monitor/#hdd-temperature","text":"/diagnostics/hdd_monitor: HDD Temperature [summary] level message OK OK WARN hot ERROR critical hot [values] key value (example) HDD [0-9]: status OK / hot / critical hot HDD [0-9]: name /dev/nvme0 HDD [0-9]: model SAMSUNG MZVLB1T0HBLR-000L7 HDD [0-9]: serial S4EMNF0M820682 HDD [0-9]: temperature 37.0 DegC","title":"HDD Temperature"},{"location":"system/system_monitor/docs/topics_hdd_monitor/#hdd-poweronhours","text":"/diagnostics/hdd_monitor: HDD PowerOnHours [summary] level message OK OK WARN lifetime limit [values] key value (example) HDD [0-9]: status OK / lifetime limit HDD [0-9]: name /dev/nvme0 HDD [0-9]: model PHISON PS5012-E12S-512G HDD [0-9]: serial FB590709182505050767 HDD [0-9]: power on hours 4834 Hours","title":"HDD PowerOnHours"},{"location":"system/system_monitor/docs/topics_hdd_monitor/#hdd-totaldatawritten","text":"/diagnostics/hdd_monitor: HDD TotalDataWritten [summary] level message OK OK WARN warranty period [values] key value (example) HDD [0-9]: status OK / warranty period HDD [0-9]: name /dev/nvme0 HDD [0-9]: model PHISON PS5012-E12S-512G HDD [0-9]: serial FB590709182505050767 HDD [0-9]: total data written 146295330 not available","title":"HDD TotalDataWritten"},{"location":"system/system_monitor/docs/topics_hdd_monitor/#hdd-usage","text":"/diagnostics/hdd_monitor: HDD Usage [summary] level message OK OK WARN low disk space ERROR very low disk space [values] key value (example) HDD [0-9]: status OK / low disk space / very low disk space HDD [0-9]: filesystem /dev/nvme0n1p4 HDD [0-9]: size 264G HDD [0-9]: used 172G HDD [0-9]: avail 749G HDD [0-9]: use 69% HDD [0-9]: mounted on /","title":"HDD Usage"},{"location":"system/system_monitor/docs/topics_mem_monitor/","text":"ROS topics: Memory Monitor # Memory Usage # /diagnostics/mem_monitor: Memory Usage [summary] level message OK OK WARN high load ERROR very high load [values] key value (example) Mem: usage 29.72% Mem: total 31.2G Mem: used 6.0G Mem: free 20.7G Mem: shared 2.9G Mem: buff/cache 4.5G Mem: available 21.9G Swap: total 2.0G Swap: used 218M Swap: free 1.8G Total: total 33.2G Total: used 6.2G Total: free 22.5G Total: used+ 9.1G","title":"ROS topics: Memory Monitor"},{"location":"system/system_monitor/docs/topics_mem_monitor/#ros-topics-memory-monitor","text":"","title":"ROS topics: Memory Monitor"},{"location":"system/system_monitor/docs/topics_mem_monitor/#memory-usage","text":"/diagnostics/mem_monitor: Memory Usage [summary] level message OK OK WARN high load ERROR very high load [values] key value (example) Mem: usage 29.72% Mem: total 31.2G Mem: used 6.0G Mem: free 20.7G Mem: shared 2.9G Mem: buff/cache 4.5G Mem: available 21.9G Swap: total 2.0G Swap: used 218M Swap: free 1.8G Total: total 33.2G Total: used 6.2G Total: free 22.5G Total: used+ 9.1G","title":"Memory Usage"},{"location":"system/system_monitor/docs/topics_net_monitor/","text":"ROS topics: Net Monitor # Network Usage # /diagnostics/net_monitor: Network Usage [summary] level message OK OK [values] key value (example) Network [0-9]: status OK Network [0-9]: interface name wlp82s0 Network [0-9]: rx_usage 0.00% Network [0-9]: tx_usage 0.00% Network [0-9]: rx_traffic 0.00 MB/s Network [0-9]: tx_traffic 0.00 MB/s Network [0-9]: capacity 400.0 MB/s Network [0-9]: mtu 1500 Network [0-9]: rx_bytes 58455228 Network [0-9]: rx_errors 0 Network [0-9]: tx_bytes 11069136 Network [0-9]: tx_errors 0 Network [0-9]: collisions 0 Network Traffic # /diagnostics/net_monitor: Network Traffic [summary] level message OK OK [values] program key value (example) nethogs [0-9]: PROGRAM /lambda/greengrassSystemComponents/1384/999 nethogs [0-9]: SENT (KB/Sec) 1.13574 nethogs [0-9]: RECEIVED (KB/Sec) 0.261914 [values] all key value (example) nethogs: all (KB/Sec) python3.7/1520/999 0.274414 0.354883 /lambda/greengrassSystemComponents/1299/999 0.487305 0.0966797 sshd: muser@pts/5/15917/1002 0.396094 0.0585938 /usr/bin/python3.7/2371/999 0 0 /greengrass/ggc/packages/1.10.0/bin/daemon/906/0 0 0 python3.7/4362/999 0 0 unknown TCP/0/0 0 0 [values] error key value (example) error [nethogs -t] execve failed: No such file or directory","title":"ROS topics: Net Monitor"},{"location":"system/system_monitor/docs/topics_net_monitor/#ros-topics-net-monitor","text":"","title":"ROS topics: Net Monitor"},{"location":"system/system_monitor/docs/topics_net_monitor/#network-usage","text":"/diagnostics/net_monitor: Network Usage [summary] level message OK OK [values] key value (example) Network [0-9]: status OK Network [0-9]: interface name wlp82s0 Network [0-9]: rx_usage 0.00% Network [0-9]: tx_usage 0.00% Network [0-9]: rx_traffic 0.00 MB/s Network [0-9]: tx_traffic 0.00 MB/s Network [0-9]: capacity 400.0 MB/s Network [0-9]: mtu 1500 Network [0-9]: rx_bytes 58455228 Network [0-9]: rx_errors 0 Network [0-9]: tx_bytes 11069136 Network [0-9]: tx_errors 0 Network [0-9]: collisions 0","title":"Network Usage"},{"location":"system/system_monitor/docs/topics_net_monitor/#network-traffic","text":"/diagnostics/net_monitor: Network Traffic [summary] level message OK OK [values] program key value (example) nethogs [0-9]: PROGRAM /lambda/greengrassSystemComponents/1384/999 nethogs [0-9]: SENT (KB/Sec) 1.13574 nethogs [0-9]: RECEIVED (KB/Sec) 0.261914 [values] all key value (example) nethogs: all (KB/Sec) python3.7/1520/999 0.274414 0.354883 /lambda/greengrassSystemComponents/1299/999 0.487305 0.0966797 sshd: muser@pts/5/15917/1002 0.396094 0.0585938 /usr/bin/python3.7/2371/999 0 0 /greengrass/ggc/packages/1.10.0/bin/daemon/906/0 0 0 python3.7/4362/999 0 0 unknown TCP/0/0 0 0 [values] error key value (example) error [nethogs -t] execve failed: No such file or directory","title":"Network Traffic"},{"location":"system/system_monitor/docs/topics_ntp_monitor/","text":"ROS topics: NTP Monitor # NTP Offset # /diagnostics/ntp_monitor: NTP Offset [summary] level message OK OK WARN high ERROR too high [values] key value (example) NTP Offset -0.013181 sec NTP Delay 0.053880 sec","title":"ROS topics: NTP Monitor"},{"location":"system/system_monitor/docs/topics_ntp_monitor/#ros-topics-ntp-monitor","text":"","title":"ROS topics: NTP Monitor"},{"location":"system/system_monitor/docs/topics_ntp_monitor/#ntp-offset","text":"/diagnostics/ntp_monitor: NTP Offset [summary] level message OK OK WARN high ERROR too high [values] key value (example) NTP Offset -0.013181 sec NTP Delay 0.053880 sec","title":"NTP Offset"},{"location":"system/system_monitor/docs/topics_process_monitor/","text":"ROS topics: Process Monitor # Tasks Summary # /diagnostics/process_monitor: Tasks Summary [summary] level message OK OK [values] key value (example) total 409 running 2 sleeping 321 stopped 0 zombie 0 High-load Proc[0-9] # /diagnostics/process_monitor: High-load Proc[0-9] [summary] level message OK OK [values] key value (example) COMMAND /usr/lib/firefox/firefox %CPU 37.5 %MEM 2.1 PID 14062 USER autoware PR 20 NI 0 VIRT 3461152 RES 669052 SHR 481208 S S TIME+ 23:57.49 High-mem Proc[0-9] # /diagnostics/process_monitor: High-mem Proc[0-9] [summary] level message OK OK [values] key value (example) COMMAND /snap/multipass/1784/usr/bin/qemu-system-x86_64 %CPU 0 %MEM 2.5 PID 1565 USER root PR 20 NI 0 VIRT 3722320 RES 812432 SHR 20340 S S TIME+ 0:22.84","title":"ROS topics: Process Monitor"},{"location":"system/system_monitor/docs/topics_process_monitor/#ros-topics-process-monitor","text":"","title":"ROS topics: Process Monitor"},{"location":"system/system_monitor/docs/topics_process_monitor/#tasks-summary","text":"/diagnostics/process_monitor: Tasks Summary [summary] level message OK OK [values] key value (example) total 409 running 2 sleeping 321 stopped 0 zombie 0","title":"Tasks Summary"},{"location":"system/system_monitor/docs/topics_process_monitor/#high-load-proc0-9","text":"/diagnostics/process_monitor: High-load Proc[0-9] [summary] level message OK OK [values] key value (example) COMMAND /usr/lib/firefox/firefox %CPU 37.5 %MEM 2.1 PID 14062 USER autoware PR 20 NI 0 VIRT 3461152 RES 669052 SHR 481208 S S TIME+ 23:57.49","title":"High-load Proc[0-9]"},{"location":"system/system_monitor/docs/topics_process_monitor/#high-mem-proc0-9","text":"/diagnostics/process_monitor: High-mem Proc[0-9] [summary] level message OK OK [values] key value (example) COMMAND /snap/multipass/1784/usr/bin/qemu-system-x86_64 %CPU 0 %MEM 2.5 PID 1565 USER root PR 20 NI 0 VIRT 3722320 RES 812432 SHR 20340 S S TIME+ 0:22.84","title":"High-mem Proc[0-9]"},{"location":"system/system_monitor/docs/traffic_reader/","text":"traffic_reader # Name # traffic_reader - monitoring netwok traffic by process Synopsis # traffic_reader [OPTION] Description # Monitoring netwok traffic by process. This runs as a daemon process and listens to a TCP/IP port (7636 by default). Options: -h, --help Display help -p, --port # Port number to listen to Exit status: Returns 0 if OK; non-zero otherwise. Notes # The 'traffic_reader' requires nethogs command. Operation confirmed platform # Ubuntu 20.04.3 LTS (GNU/Linux 5.11.0-40-generic x86_64)","title":"traffic_reader"},{"location":"system/system_monitor/docs/traffic_reader/#traffic_reader","text":"","title":"traffic_reader"},{"location":"system/system_monitor/docs/traffic_reader/#name","text":"traffic_reader - monitoring netwok traffic by process","title":"Name"},{"location":"system/system_monitor/docs/traffic_reader/#synopsis","text":"traffic_reader [OPTION]","title":"Synopsis"},{"location":"system/system_monitor/docs/traffic_reader/#description","text":"Monitoring netwok traffic by process. This runs as a daemon process and listens to a TCP/IP port (7636 by default). Options: -h, --help Display help -p, --port # Port number to listen to Exit status: Returns 0 if OK; non-zero otherwise.","title":"Description"},{"location":"system/system_monitor/docs/traffic_reader/#notes","text":"The 'traffic_reader' requires nethogs command.","title":"Notes"},{"location":"system/system_monitor/docs/traffic_reader/#operation-confirmed-platform","text":"Ubuntu 20.04.3 LTS (GNU/Linux 5.11.0-40-generic x86_64)","title":"Operation confirmed platform"},{"location":"system/topic_state_monitor/Readme/","text":"topic_state_monitor # Purpose # This node monitors input topic for abnormalities such as timeout and low frequency. The result of topic status is published as diagnostics. Inner-workings / Algorithms # The types of topic status and corresponding diagnostic status are following. Topic status Diagnostic status Description OK OK The topic has no abnormalities NotReceived ERROR The topic has not been received yet WarnRate WARN The frequency of the topic is dropped ErrorRate ERROR The frequency of the topic is significantly dropped Timeout ERROR The topic subscription is stopped for a certain time Inputs / Outputs # Input # Name Type Description any name any type Subscribe target topic to monitor Output # Name Type Description /diagnostics diagnostic_msgs/DiagnosticArray Diagnostics outputs Parameters # Node Parameters # Name Type Default Value Description update_rate double 10.0 Timer callback period [Hz] window_size int 10 Window size of target topic for calculating frequency Core Parameters # Name Type Default Value Description topic string - Name of target topic topic_type string - Type of target topic transient_local bool false QoS policy of topic subscription (Transient Local/Volatile) best_effort bool false QoS policy of topic subscription (Best Effort/Reliable) diag_name string - Name used for the diagnostics to publish warn_rate double 0.5 If the topic rate is lower than this value, the topic status becomes WarnRate error_rate double 0.1 If the topic rate is lower than this value, the topic status becomes ErrorRate timeout double 1.0 If the topic subscription is stopped for more than this time [s], the topic status becomes Timeout Assumptions / Known limits # TBD.","title":"topic_state_monitor"},{"location":"system/topic_state_monitor/Readme/#topic_state_monitor","text":"","title":"topic_state_monitor"},{"location":"system/topic_state_monitor/Readme/#purpose","text":"This node monitors input topic for abnormalities such as timeout and low frequency. The result of topic status is published as diagnostics.","title":"Purpose"},{"location":"system/topic_state_monitor/Readme/#inner-workings-algorithms","text":"The types of topic status and corresponding diagnostic status are following. Topic status Diagnostic status Description OK OK The topic has no abnormalities NotReceived ERROR The topic has not been received yet WarnRate WARN The frequency of the topic is dropped ErrorRate ERROR The frequency of the topic is significantly dropped Timeout ERROR The topic subscription is stopped for a certain time","title":"Inner-workings / Algorithms"},{"location":"system/topic_state_monitor/Readme/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"system/topic_state_monitor/Readme/#input","text":"Name Type Description any name any type Subscribe target topic to monitor","title":"Input"},{"location":"system/topic_state_monitor/Readme/#output","text":"Name Type Description /diagnostics diagnostic_msgs/DiagnosticArray Diagnostics outputs","title":"Output"},{"location":"system/topic_state_monitor/Readme/#parameters","text":"","title":"Parameters"},{"location":"system/topic_state_monitor/Readme/#node-parameters","text":"Name Type Default Value Description update_rate double 10.0 Timer callback period [Hz] window_size int 10 Window size of target topic for calculating frequency","title":"Node Parameters"},{"location":"system/topic_state_monitor/Readme/#core-parameters","text":"Name Type Default Value Description topic string - Name of target topic topic_type string - Type of target topic transient_local bool false QoS policy of topic subscription (Transient Local/Volatile) best_effort bool false QoS policy of topic subscription (Best Effort/Reliable) diag_name string - Name used for the diagnostics to publish warn_rate double 0.5 If the topic rate is lower than this value, the topic status becomes WarnRate error_rate double 0.1 If the topic rate is lower than this value, the topic status becomes ErrorRate timeout double 1.0 If the topic subscription is stopped for more than this time [s], the topic status becomes Timeout","title":"Core Parameters"},{"location":"system/topic_state_monitor/Readme/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"system/velodyne_monitor/Readme/","text":"velodyne_monitor # Purpose # This node monitors the status of Velodyne LiDARs. The result of the status is published as diagnostics. Inner-workings / Algorithms # The status of Velodyne LiDAR can be retrieved from http://[ip_address]/cgi/{info, settings, status, diag}.json . The types of abnormal status and corresponding diagnostics status are following. Abnormal status Diagnostic status No abnormality OK Top board temperature is too cold ERROR Top board temperature is cold WARN Top board temperature is too hot ERROR Top board temperature is hot WARN Bottom board temperature is too cold ERROR Bottom board temperature is cold WARN Bottom board temperature is too hot ERROR Bottom board temperature is hot WARN Rpm(Rotations per minute) of the motor is too low ERROR Rpm(Rotations per minute) of the motor is low WARN Connection error (cannot get Velodyne LiDAR status) ERROR Inputs / Outputs # Input # None Output # Name Type Description /diagnostics diagnostic_msgs/DiagnosticArray Diagnostics outputs Parameters # Node Parameters # Name Type Default Value Description timeout double 0.5 Timeout for HTTP request to get Velodyne LiDAR status [s] Core Parameters # Name Type Default Value Description ip_address string \"192.168.1.201\" IP address of target Velodyne LiDAR temp_cold_warn double -5.0 If the temperature of Velodyne LiDAR is lower than this value, the diagnostics status becomes WARN [\u00b0C] temp_cold_error double -10.0 If the temperature of Velodyne LiDAR is lower than this value, the diagnostics status becomes ERROR [\u00b0C] temp_hot_warn double 75.0 If the temperature of Velodyne LiDAR is higher than this value, the diagnostics status becomes WARN [\u00b0C] temp_hot_error double 80.0 If the temperature of Velodyne LiDAR is higher than this value, the diagnostics status becomes ERROR [\u00b0C] rpm_ratio_warn double 0.80 If the rpm rate of the motor (= current rpm / default rpm) is lower than this value, the diagnostics status becomes WARN rpm_ratio_error double 0.70 If the rpm rate of the motor (= current rpm / default rpm) is lower than this value, the diagnostics status becomes ERROR Assumptions / Known limits # TBD.","title":"velodyne_monitor"},{"location":"system/velodyne_monitor/Readme/#velodyne_monitor","text":"","title":"velodyne_monitor"},{"location":"system/velodyne_monitor/Readme/#purpose","text":"This node monitors the status of Velodyne LiDARs. The result of the status is published as diagnostics.","title":"Purpose"},{"location":"system/velodyne_monitor/Readme/#inner-workings-algorithms","text":"The status of Velodyne LiDAR can be retrieved from http://[ip_address]/cgi/{info, settings, status, diag}.json . The types of abnormal status and corresponding diagnostics status are following. Abnormal status Diagnostic status No abnormality OK Top board temperature is too cold ERROR Top board temperature is cold WARN Top board temperature is too hot ERROR Top board temperature is hot WARN Bottom board temperature is too cold ERROR Bottom board temperature is cold WARN Bottom board temperature is too hot ERROR Bottom board temperature is hot WARN Rpm(Rotations per minute) of the motor is too low ERROR Rpm(Rotations per minute) of the motor is low WARN Connection error (cannot get Velodyne LiDAR status) ERROR","title":"Inner-workings / Algorithms"},{"location":"system/velodyne_monitor/Readme/#inputs-outputs","text":"","title":"Inputs / Outputs"},{"location":"system/velodyne_monitor/Readme/#input","text":"None","title":"Input"},{"location":"system/velodyne_monitor/Readme/#output","text":"Name Type Description /diagnostics diagnostic_msgs/DiagnosticArray Diagnostics outputs","title":"Output"},{"location":"system/velodyne_monitor/Readme/#parameters","text":"","title":"Parameters"},{"location":"system/velodyne_monitor/Readme/#node-parameters","text":"Name Type Default Value Description timeout double 0.5 Timeout for HTTP request to get Velodyne LiDAR status [s]","title":"Node Parameters"},{"location":"system/velodyne_monitor/Readme/#core-parameters","text":"Name Type Default Value Description ip_address string \"192.168.1.201\" IP address of target Velodyne LiDAR temp_cold_warn double -5.0 If the temperature of Velodyne LiDAR is lower than this value, the diagnostics status becomes WARN [\u00b0C] temp_cold_error double -10.0 If the temperature of Velodyne LiDAR is lower than this value, the diagnostics status becomes ERROR [\u00b0C] temp_hot_warn double 75.0 If the temperature of Velodyne LiDAR is higher than this value, the diagnostics status becomes WARN [\u00b0C] temp_hot_error double 80.0 If the temperature of Velodyne LiDAR is higher than this value, the diagnostics status becomes ERROR [\u00b0C] rpm_ratio_warn double 0.80 If the rpm rate of the motor (= current rpm / default rpm) is lower than this value, the diagnostics status becomes WARN rpm_ratio_error double 0.70 If the rpm rate of the motor (= current rpm / default rpm) is lower than this value, the diagnostics status becomes ERROR","title":"Core Parameters"},{"location":"system/velodyne_monitor/Readme/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/","text":"accel_brake_map_calibrator # The role of this node is to automatically calibrate accel_map.csv / brake_map.csv used in the raw_vehicle_cmd_converter node. The base map, which is lexus's one by default, is updated iteratively with the loaded driving data. How to calibrate # Launch Calibrator # After launching Autoware, run the accel_brake_map_calibrator by the following command and then perform autonomous driving. Note: You can collect data with manual driving if it is possible to use the same vehicle interface as during autonomous driving (e.g. using a joystick). ros2 launch accel_brake_map_calibrator accel_brake_map_calibrator.launch.xml rviz: = true Or if you want to use rosbag files, run the following commands. ros2 launch accel_brake_map_calibrator accel_brake_map_calibrator.launch.xml rviz: = true use_sim_time: = true ros2 bag play <rosbag_file> --clock During the calibration with setting the parameter progress_file_output to true, the log file is output in [directory of accel_brake_map_calibrator ]/config/ . You can also see accel and brake maps in [directory of accel_brake_map_calibrator ]/config/accel_map.csv and [directory of accel_brake_map_calibrator ]/config/brake_map.csv after calibration. Calibration plugin # The rviz:=true option displays the RViz with a calibration plugin as below. The current status (velocity and pedal) is shown in the plugin. The color on the current cell varies green/red depending on the current data is valid/invalid. The data that doesn't satisfy the following conditions are considered invalid and will not be used for estimation. The velocity and pedal conditions are within certain ranges from the index values. The steer value, pedal speed, pitch value, etc. are less than corresponding thresholds. The velocity is higher than a threshold. Note: You don't need to worry about whether the current state is red or green during calibration. Just keep getting data until all the cells turn red. The value of each cell in the map is gray at first, and it changes from blue to red as the number of valid data in the cell accumulates. It is preferable to continue the calibration until each cell of the map becomes close to red. In particular, the performance near the stop depends strongly on the velocity of 0 ~ 6m/s range and the pedal value of +0.2 ~ -0.4, range so it is desirable to focus on those areas. Diagnostics # The accel brake map_calibrator publishes diagnostics message depending on the calibration status. Diagnostic type WARN indicates that the current accel/brake map is estimated to be inaccurate. In this situation, it is strongly recommended to perform a re-calibration of the accel/brake map. Status Diagnostics Type Diagnostics message Description No calibration required OK \"OK\" Calibration Required WARN \"Accel/brake map Calibration is required.\" The accuracy of current accel/brake map may be low. This diagnostics status can be also checked on the following ROS topic. ros2 topic echo /accel_brake_map_calibrator/output/update_suggest When the diagnostics type is WARN , True is published on this topic and the update of the accel/brake map is suggested. Evaluation of the accel / brake map accuracy # The accuracy of map is evaluated by the Root Mean Squared Error (RMSE) between the observed acceleration and predicted acceleration. TERMS: Observed acceleration : the current vehicle acceleration which is calculated as a derivative value of the wheel speed. Predicted acceleration : the output of the original accel/brake map, which the Autoware is expecting. The value is calculated using the current pedal and velocity. You can check additional error information with the following topics. /accel_brake_map_calibrator/output/current_map_error : The error of the original map set in the csv_path_accel/brake_map path. The original map is not accurate if this value is large. /accel_brake_map_calibrator/output/updated_map_error : The error of the map calibrated in this node. The calibration quality is low if this value is large. /accel_brake_map_calibrator/output/map_error_ratio : The error ratio between the original map and updated map (ratio = updated / current). If this value is less than 1, it is desirable to update the map. How to visualize calibration data # The process of calibration can be visualized as below. Since these scripts need the log output of the calibration, the pedal_accel_graph_output parameter must be set to true while the calibration is running for the visualization. Visualize plot of relation between acceleration and pedal # The following command shows the plot of used data in the calibration. In each plot of velocity ranges, you can see the distribution of the relationship between pedal and acceleration, and raw data points with colors according to their pitch angles. ros2 run accel_brake_map_calibrator view_plot.py Visualize statistics about acceleration/velocity/pedal data # The following command shows the statistics of the calibration: mean value standard deviation number of data of all data in each map cell. ros2 run accel_brake_map_calibrator view_statistics.py How to save the calibrated accel / brake map anytime you want # You can save accel and brake map anytime with the following command. ros2 service call /accel_brake_map_calibrator/update_map_dir tier4_vehicle_msgs/srv/UpdateAccelBrakeMap \"path: '<accel/brake map directory>'\" You can also save accel and brake map in the default directory where Autoware reads accel_map.csv/brake_map.csv using the RViz plugin (AccelBrakeMapCalibratorButtonPanel) as following. Click Panels tab, and select AccelBrakeMapCalibratorButtonPanel. Select the panel, and the button will appear at the bottom of RViz. Press the button, and the accel / brake map will be saved. (The button cannot be pressed in certain situations, such as when the calibrator node is not running.) Parameters # Name Type Description Default value update_method string you can select map calibration method. \"update_offset_each_cell\" calculates offsets for each grid cells on the map. \"update_offset_total\" calculates the total offset of the map. \"update_offset_each_cell\" get_pitch_method string \"tf\": get pitch from tf, \"none\": unable to perform pitch validation and pitch compensation \"tf\" pedal_accel_graph_output bool if true, it will output a log of the pedal accel graph. true progress_file_output bool if true, it will output a log and csv file of the update process. false default_map_dir str directory of default map [directory of raw_vehicle_cmd_converter ]/data/default/ calibrated_map_dir str directory of calibrated map [directory of accel_brake_map_calibrator ]/config/","title":"accel_brake_map_calibrator"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#accel_brake_map_calibrator","text":"The role of this node is to automatically calibrate accel_map.csv / brake_map.csv used in the raw_vehicle_cmd_converter node. The base map, which is lexus's one by default, is updated iteratively with the loaded driving data.","title":"accel_brake_map_calibrator"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#how-to-calibrate","text":"","title":"How to calibrate"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#launch-calibrator","text":"After launching Autoware, run the accel_brake_map_calibrator by the following command and then perform autonomous driving. Note: You can collect data with manual driving if it is possible to use the same vehicle interface as during autonomous driving (e.g. using a joystick). ros2 launch accel_brake_map_calibrator accel_brake_map_calibrator.launch.xml rviz: = true Or if you want to use rosbag files, run the following commands. ros2 launch accel_brake_map_calibrator accel_brake_map_calibrator.launch.xml rviz: = true use_sim_time: = true ros2 bag play <rosbag_file> --clock During the calibration with setting the parameter progress_file_output to true, the log file is output in [directory of accel_brake_map_calibrator ]/config/ . You can also see accel and brake maps in [directory of accel_brake_map_calibrator ]/config/accel_map.csv and [directory of accel_brake_map_calibrator ]/config/brake_map.csv after calibration.","title":"Launch Calibrator"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#calibration-plugin","text":"The rviz:=true option displays the RViz with a calibration plugin as below. The current status (velocity and pedal) is shown in the plugin. The color on the current cell varies green/red depending on the current data is valid/invalid. The data that doesn't satisfy the following conditions are considered invalid and will not be used for estimation. The velocity and pedal conditions are within certain ranges from the index values. The steer value, pedal speed, pitch value, etc. are less than corresponding thresholds. The velocity is higher than a threshold. Note: You don't need to worry about whether the current state is red or green during calibration. Just keep getting data until all the cells turn red. The value of each cell in the map is gray at first, and it changes from blue to red as the number of valid data in the cell accumulates. It is preferable to continue the calibration until each cell of the map becomes close to red. In particular, the performance near the stop depends strongly on the velocity of 0 ~ 6m/s range and the pedal value of +0.2 ~ -0.4, range so it is desirable to focus on those areas.","title":"Calibration plugin"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#diagnostics","text":"The accel brake map_calibrator publishes diagnostics message depending on the calibration status. Diagnostic type WARN indicates that the current accel/brake map is estimated to be inaccurate. In this situation, it is strongly recommended to perform a re-calibration of the accel/brake map. Status Diagnostics Type Diagnostics message Description No calibration required OK \"OK\" Calibration Required WARN \"Accel/brake map Calibration is required.\" The accuracy of current accel/brake map may be low. This diagnostics status can be also checked on the following ROS topic. ros2 topic echo /accel_brake_map_calibrator/output/update_suggest When the diagnostics type is WARN , True is published on this topic and the update of the accel/brake map is suggested.","title":"Diagnostics"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#evaluation-of-the-accel-brake-map-accuracy","text":"The accuracy of map is evaluated by the Root Mean Squared Error (RMSE) between the observed acceleration and predicted acceleration. TERMS: Observed acceleration : the current vehicle acceleration which is calculated as a derivative value of the wheel speed. Predicted acceleration : the output of the original accel/brake map, which the Autoware is expecting. The value is calculated using the current pedal and velocity. You can check additional error information with the following topics. /accel_brake_map_calibrator/output/current_map_error : The error of the original map set in the csv_path_accel/brake_map path. The original map is not accurate if this value is large. /accel_brake_map_calibrator/output/updated_map_error : The error of the map calibrated in this node. The calibration quality is low if this value is large. /accel_brake_map_calibrator/output/map_error_ratio : The error ratio between the original map and updated map (ratio = updated / current). If this value is less than 1, it is desirable to update the map.","title":"Evaluation of the accel / brake map accuracy"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#how-to-visualize-calibration-data","text":"The process of calibration can be visualized as below. Since these scripts need the log output of the calibration, the pedal_accel_graph_output parameter must be set to true while the calibration is running for the visualization.","title":"How to visualize calibration data"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#visualize-plot-of-relation-between-acceleration-and-pedal","text":"The following command shows the plot of used data in the calibration. In each plot of velocity ranges, you can see the distribution of the relationship between pedal and acceleration, and raw data points with colors according to their pitch angles. ros2 run accel_brake_map_calibrator view_plot.py","title":"Visualize plot of relation between acceleration and pedal"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#visualize-statistics-about-accelerationvelocitypedal-data","text":"The following command shows the statistics of the calibration: mean value standard deviation number of data of all data in each map cell. ros2 run accel_brake_map_calibrator view_statistics.py","title":"Visualize statistics about acceleration/velocity/pedal data"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#how-to-save-the-calibrated-accel-brake-map-anytime-you-want","text":"You can save accel and brake map anytime with the following command. ros2 service call /accel_brake_map_calibrator/update_map_dir tier4_vehicle_msgs/srv/UpdateAccelBrakeMap \"path: '<accel/brake map directory>'\" You can also save accel and brake map in the default directory where Autoware reads accel_map.csv/brake_map.csv using the RViz plugin (AccelBrakeMapCalibratorButtonPanel) as following. Click Panels tab, and select AccelBrakeMapCalibratorButtonPanel. Select the panel, and the button will appear at the bottom of RViz. Press the button, and the accel / brake map will be saved. (The button cannot be pressed in certain situations, such as when the calibrator node is not running.)","title":"How to save the calibrated accel / brake map anytime you want"},{"location":"vehicle/accel_brake_map_calibrator/accel_brake_map_calibrator/#parameters","text":"Name Type Description Default value update_method string you can select map calibration method. \"update_offset_each_cell\" calculates offsets for each grid cells on the map. \"update_offset_total\" calculates the total offset of the map. \"update_offset_each_cell\" get_pitch_method string \"tf\": get pitch from tf, \"none\": unable to perform pitch validation and pitch compensation \"tf\" pedal_accel_graph_output bool if true, it will output a log of the pedal accel graph. true progress_file_output bool if true, it will output a log and csv file of the update process. false default_map_dir str directory of default map [directory of raw_vehicle_cmd_converter ]/data/default/ calibrated_map_dir str directory of calibrated map [directory of accel_brake_map_calibrator ]/config/","title":"Parameters"},{"location":"vehicle/external_cmd_converter/","text":"external_cmd_converter # external_cmd_converter is a node that converts desired mechanical input to acceleration and velocity by using accel/brake map. Input topics # Name Type Description ~/in/external_control_cmd tier4_external_api_msgs::msg::ControlCommand target throttle/brake/steering_angle/steering_angle_velocity is necessary to calculate desired control command. ~/input/shift_cmd\" autoware_auto_vehicle_msgs::GearCommand current command of gear. ~/input/emergency_stop tier4_external_api_msgs::msg::Heartbeat emergency heart beat for external command. ~/input/current_gate_mode tier4_control_msgs::msg::GateMode topic for gate mode. ~/input/odometry navigation_msgs::Odometry twist topic in odometry is used. Output topics # Name Type Description ~/out/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand ackermann control command converted from selected external command Parameters # Parameter Type Description timer_rate double timer's update rate wait_for_first_topic double if time out check is done after receiving first topic control_command_timeout double time out check for control command emergency_stop_timeout double time out check for emergency stop command Limitation # tbd.","title":"external_cmd_converter"},{"location":"vehicle/external_cmd_converter/#external_cmd_converter","text":"external_cmd_converter is a node that converts desired mechanical input to acceleration and velocity by using accel/brake map.","title":"external_cmd_converter"},{"location":"vehicle/external_cmd_converter/#input-topics","text":"Name Type Description ~/in/external_control_cmd tier4_external_api_msgs::msg::ControlCommand target throttle/brake/steering_angle/steering_angle_velocity is necessary to calculate desired control command. ~/input/shift_cmd\" autoware_auto_vehicle_msgs::GearCommand current command of gear. ~/input/emergency_stop tier4_external_api_msgs::msg::Heartbeat emergency heart beat for external command. ~/input/current_gate_mode tier4_control_msgs::msg::GateMode topic for gate mode. ~/input/odometry navigation_msgs::Odometry twist topic in odometry is used.","title":"Input topics"},{"location":"vehicle/external_cmd_converter/#output-topics","text":"Name Type Description ~/out/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand ackermann control command converted from selected external command","title":"Output topics"},{"location":"vehicle/external_cmd_converter/#parameters","text":"Parameter Type Description timer_rate double timer's update rate wait_for_first_topic double if time out check is done after receiving first topic control_command_timeout double time out check for control command emergency_stop_timeout double time out check for emergency stop command","title":"Parameters"},{"location":"vehicle/external_cmd_converter/#limitation","text":"tbd.","title":"Limitation"},{"location":"vehicle/raw_vehicle_cmd_converter/","text":"raw_vehicle_cmd_converter # raw_vehicle_command_converter is a node that converts desired acceleration and velocity to mechanical input by using feed forward + feed back control (optional). Input topics # Name Type Description ~/input/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand target velocity/acceleration/steering_angle/steering_angle_velocity is necessary to calculate actuation command. ~/input/steering\" autoware_auto_vehicle_msgs::SteeringReport current status of steering used for steering feed back control ~/input/twist navigation_msgs::Odometry twist topic in odometry is used. Output topics # Name Type Description ~/output/actuation_cmd tier4_vehicle_msgs::msg::ActuationCommandStamped actuation command for vehicle to apply mechanical input Parameters # Parameter Type Description update_rate double timer's update rate th_max_message_delay_sec double threshold time of input messages' maximum delay th_arrived_distance_m double threshold distance to check if vehicle has arrived at the trajectory's endpoint th_stopped_time_sec double threshold time to check if vehicle is stopped th_stopped_velocity_mps double threshold velocity to check if vehicle is stopped Limitation # The current feed back implementation is only applied to steering control.","title":"raw_vehicle_cmd_converter"},{"location":"vehicle/raw_vehicle_cmd_converter/#raw_vehicle_cmd_converter","text":"raw_vehicle_command_converter is a node that converts desired acceleration and velocity to mechanical input by using feed forward + feed back control (optional).","title":"raw_vehicle_cmd_converter"},{"location":"vehicle/raw_vehicle_cmd_converter/#input-topics","text":"Name Type Description ~/input/control_cmd autoware_auto_control_msgs::msg::AckermannControlCommand target velocity/acceleration/steering_angle/steering_angle_velocity is necessary to calculate actuation command. ~/input/steering\" autoware_auto_vehicle_msgs::SteeringReport current status of steering used for steering feed back control ~/input/twist navigation_msgs::Odometry twist topic in odometry is used.","title":"Input topics"},{"location":"vehicle/raw_vehicle_cmd_converter/#output-topics","text":"Name Type Description ~/output/actuation_cmd tier4_vehicle_msgs::msg::ActuationCommandStamped actuation command for vehicle to apply mechanical input","title":"Output topics"},{"location":"vehicle/raw_vehicle_cmd_converter/#parameters","text":"Parameter Type Description update_rate double timer's update rate th_max_message_delay_sec double threshold time of input messages' maximum delay th_arrived_distance_m double threshold distance to check if vehicle has arrived at the trajectory's endpoint th_stopped_time_sec double threshold time to check if vehicle is stopped th_stopped_velocity_mps double threshold velocity to check if vehicle is stopped","title":"Parameters"},{"location":"vehicle/raw_vehicle_cmd_converter/#limitation","text":"The current feed back implementation is only applied to steering control.","title":"Limitation"},{"location":"vehicle/vehicle_info_util/Readme/","text":"Vehicle Info Util # Purpose # This package is to get vehicle info parameters. Description # Assumptions / Known limits # TBD.","title":"Vehicle Info Util"},{"location":"vehicle/vehicle_info_util/Readme/#vehicle-info-util","text":"","title":"Vehicle Info Util"},{"location":"vehicle/vehicle_info_util/Readme/#purpose","text":"This package is to get vehicle info parameters.","title":"Purpose"},{"location":"vehicle/vehicle_info_util/Readme/#description","text":"","title":"Description"},{"location":"vehicle/vehicle_info_util/Readme/#assumptions-known-limits","text":"TBD.","title":"Assumptions / Known limits"}]}